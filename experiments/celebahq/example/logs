/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="./data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="./data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    print(x.shape, label.shape)
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="./data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="./data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    print(x.shape, label.shape)
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="./data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="./data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    print(x.shape, label.shape)
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    print(x.shape, label.shape)
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x, label = x.cuda(), label.cuda()
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x, label = x.cuda(), label.cuda()
                    print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x, label = x.cuda(), label.cuda()
                    print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    # device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x, label = x.cuda(), label.cuda()
                    print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            tens = torch.from_numpy(nump_array)
            if (nump_array.ndim < 3):
                nump_array = np.expand_dims(nump_array, axis=-1)
            nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch))
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch))
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch), type(batch[0]))
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch), len(batch[0]))
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch), len(batch[0]), batch[0][0].shape, batch[0][1].shape)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        print(type(imgs[0]), imgs.shape)
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch), len(batch[0]), batch[0][0].shape, batch[0][1].shape)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        print(type(imgs[0]), imgs[0].shape)
        targets = torch.tensor([target[1] for target in batch])
        print(type(batch), len(batch), len(batch[0]), batch[0][0].shape, batch[0][1].shape)
        w = imgs[0].size[0]
        h = imgs[0].size[1]

        tensor = torch.zeros((len(imgs), im_dim, im_size, im_size), dtype=torch.uint8)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.uint8)
            #tens = torch.from_numpy(nump_array)
            #if (nump_array.ndim < 3):
            #   nump_array = np.expand_dims(nump_array, axis=-1)
            #nump_array = np.rollaxis(nump_array, 2)
            tensor[i] += torch.from_numpy(nump_array)

        return tensor, targets

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    print(type(out))
                    print(type(label))
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    print(type(out), out.size())
                    print(type(label), label.size())
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block), device=device)
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block), device=device)
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):

        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    print("x ",type(x), x.size())
                    print("label ",type(label), label.size())
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        print(batch[0][0].shape, batch[0][1].shape)
        dsfag-=abkj
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    print("x ",type(x), x.size())
                    print("label ",type(label), label.size())
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        print(batch[0][0].shape, batch[0][1].shape)
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        print("hello ",target_tensors.size())
        dsfag-=abkj
        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = x.to(device)
                    print("x ",type(x), x.size())
                    print("label ",type(label), label.size())
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        print(batch[0][0].shape, batch[0][1].shape)
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    print("number of blocks: ", args.num_blocks)
    print("hidden_dims: ", hidden_dims)
    print("div_samples", args.div_samples)
    print("strides ", strides)
    print("squeeze_first ", args.squeeze_first)
    print("non linearity ", args.nonlinearity)
    print("layer_type ", args.layer_type)
    print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    print("x ",type(x), x.size())
                    print("label ",type(label), label.size())
                    x = x.to(device)
                    label = x.to(device)
                    print("x ",type(x), x.size())
                    print("label ",type(label), label.size())
                    fdhhaet
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=False, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 1313
Iters per test: 563
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank)
        torch.cuda.set_device(args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank)
        torch.cuda.set_device(1)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank)
        torch.cuda.set_device(1)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    print(torch.cuda.current_device())    
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    print(torch.cuda.current_device()) 
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank)
        torch.cuda.set_device(1)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

      
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    print("device count ", torch.cuda.device_count())
    print("current device ",torch.cuda.current_device()) 
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank)
        torch.cuda.set_device(1)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

      
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    bdfs     
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    bdfs     
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(args.local_rank, torch.cuda.current_device())
        torch.cuda.set_device("cuda:%d"%args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    bdfs     
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        print(type(args.local_rank), type(torch.cuda.current_device()))
        torch.cuda.set_device("cuda:%d"%args.local_rank)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    bdfs     
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    bdfs     
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    total_gpus=1
                    _, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        print("check this: ",dist_utils.env_world_size(), distributed.get_world_size())
        gfdsgti
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        print(args.dist_backend)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        print("check this: ",dist_utils.env_world_size(), distributed.get_world_size())
        gfdsgti
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='nccl', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        print(args.dist_backend)
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        print("check this: ",dist_utils.env_world_size(), distributed.get_world_size())
        gfdsgti
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/cnf")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 6.850e-01(0.69) | Time/Itr 0.69(0.69) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
Itr 000001 | Wall 1.189e+00(0.59) | Time/Itr 0.50(0.68) | BPD 11.78(11.79) | Loss 11.78(11.79) | FE 128(128) | Grad Norm 5.529e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 8.19e-13
Itr 000002 | Wall 1.721e+00(0.57) | Time/Itr 0.53(0.68) | BPD 11.75(11.79) | Loss 11.75(11.79) | FE 128(128) | Grad Norm 5.521e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.37e-03 | jacobian_norm2: 6.74e-12
Itr 000003 | Wall 2.118e+00(0.53) | Time/Itr 0.40(0.67) | BPD 11.72(11.79) | Loss 11.72(11.79) | FE 128(128) | Grad Norm 5.514e+01(5.527e+01) | TT 4.00(4.00) | kinetic_energy: 1.37e-03 | jacobian_norm2: 3.15e-11
Itr 000004 | Wall 2.638e+00(0.53) | Time/Itr 0.52(0.66) | BPD 11.70(11.78) | Loss 11.70(11.78) | FE 128(128) | Grad Norm 5.517e+01(5.527e+01) | TT 4.00(4.00) | kinetic_energy: 1.39e-03 | jacobian_norm2: 8.00e-11
Itr 000005 | Wall 3.075e+00(0.51) | Time/Itr 0.44(0.66) | BPD 11.66(11.78) | Loss 11.66(11.78) | FE 128(128) | Grad Norm 5.515e+01(5.527e+01) | TT 4.00(4.00) | kinetic_energy: 1.42e-03 | jacobian_norm2: 1.75e-10
Itr 000006 | Wall 3.521e+00(0.50) | Time/Itr 0.45(0.65) | BPD 11.58(11.77) | Loss 11.58(11.77) | FE 128(128) | Grad Norm 5.498e+01(5.526e+01) | TT 4.00(4.00) | kinetic_energy: 1.46e-03 | jacobian_norm2: 3.61e-10
Itr 000007 | Wall 4.083e+00(0.51) | Time/Itr 0.56(0.65) | BPD 11.52(11.77) | Loss 11.52(11.77) | FE 128(128) | Grad Norm 5.489e+01(5.525e+01) | TT 4.00(4.00) | kinetic_energy: 1.54e-03 | jacobian_norm2: 7.11e-10
Itr 000008 | Wall 4.601e+00(0.51) | Time/Itr 0.52(0.64) | BPD 11.43(11.76) | Loss 11.43(11.76) | FE 128(128) | Grad Norm 5.478e+01(5.523e+01) | TT 4.00(4.00) | kinetic_energy: 1.66e-03 | jacobian_norm2: 9.47e-10
Itr 000009 | Wall 5.106e+00(0.51) | Time/Itr 0.51(0.64) | BPD 11.36(11.74) | Loss 11.36(11.74) | FE 128(128) | Grad Norm 5.471e+01(5.522e+01) | TT 4.00(4.00) | kinetic_energy: 1.83e-03 | jacobian_norm2: 1.51e-09
Itr 000010 | Wall 5.635e+00(0.51) | Time/Itr 0.53(0.64) | BPD 11.26(11.73) | Loss 11.26(11.73) | FE 128(128) | Grad Norm 5.459e+01(5.520e+01) | TT 4.00(4.00) | kinetic_energy: 2.07e-03 | jacobian_norm2: 2.63e-09
Itr 000011 | Wall 6.150e+00(0.51) | Time/Itr 0.51(0.63) | BPD 11.14(11.71) | Loss 11.14(11.71) | FE 128(128) | Grad Norm 5.442e+01(5.518e+01) | TT 4.00(4.00) | kinetic_energy: 2.38e-03 | jacobian_norm2: 3.69e-09
Itr 000012 | Wall 6.688e+00(0.51) | Time/Itr 0.54(0.63) | BPD 11.02(11.69) | Loss 11.02(11.69) | FE 128(128) | Grad Norm 5.426e+01(5.515e+01) | TT 4.00(4.00) | kinetic_energy: 2.79e-03 | jacobian_norm2: 5.08e-09
Itr 000013 | Wall 7.194e+00(0.51) | Time/Itr 0.51(0.63) | BPD 10.92(11.67) | Loss 10.92(11.67) | FE 128(128) | Grad Norm 5.421e+01(5.512e+01) | TT 4.00(4.00) | kinetic_energy: 3.32e-03 | jacobian_norm2: 8.01e-09
Itr 000014 | Wall 7.711e+00(0.51) | Time/Itr 0.52(0.62) | BPD 10.79(11.64) | Loss 10.79(11.64) | FE 128(128) | Grad Norm 5.404e+01(5.509e+01) | TT 4.00(4.00) | kinetic_energy: 3.99e-03 | jacobian_norm2: 1.02e-08
Itr 000015 | Wall 8.237e+00(0.51) | Time/Itr 0.53(0.62) | BPD 10.64(11.61) | Loss 10.64(11.61) | FE 128(128) | Grad Norm 5.383e+01(5.505e+01) | TT 4.00(4.00) | kinetic_energy: 4.83e-03 | jacobian_norm2: 1.27e-08
Itr 000016 | Wall 8.734e+00(0.51) | Time/Itr 0.50(0.62) | BPD 10.50(11.58) | Loss 10.50(11.58) | FE 128(128) | Grad Norm 5.371e+01(5.501e+01) | TT 4.00(4.00) | kinetic_energy: 5.86e-03 | jacobian_norm2: 1.89e-08
Itr 000017 | Wall 9.250e+00(0.51) | Time/Itr 0.52(0.61) | BPD 10.32(11.54) | Loss 10.32(11.54) | FE 128(128) | Grad Norm 5.343e+01(5.496e+01) | TT 4.00(4.00) | kinetic_energy: 7.12e-03 | jacobian_norm2: 2.42e-08
Itr 000018 | Wall 9.757e+00(0.51) | Time/Itr 0.51(0.61) | BPD 10.14(11.50) | Loss 10.14(11.50) | FE 128(128) | Grad Norm 5.319e+01(5.491e+01) | TT 4.00(4.00) | kinetic_energy: 8.64e-03 | jacobian_norm2: 3.06e-08
Itr 000019 | Wall 1.030e+01(0.51) | Time/Itr 0.54(0.61) | BPD 9.96(11.45) | Loss 9.96(11.45) | FE 128(128) | Grad Norm 5.294e+01(5.485e+01) | TT 4.00(4.00) | kinetic_energy: 1.05e-02 | jacobian_norm2: 3.70e-08
Itr 000020 | Wall 1.078e+01(0.51) | Time/Itr 0.48(0.60) | BPD 9.78(11.40) | Loss 9.78(11.40) | FE 128(128) | Grad Norm 5.274e+01(5.479e+01) | TT 4.00(4.00) | kinetic_energy: 1.26e-02 | jacobian_norm2: 4.55e-08
Itr 000021 | Wall 1.132e+01(0.51) | Time/Itr 0.54(0.60) | BPD 9.57(11.35) | Loss 9.57(11.35) | FE 128(128) | Grad Norm 5.240e+01(5.471e+01) | TT 4.00(4.00) | kinetic_energy: 1.52e-02 | jacobian_norm2: 6.77e-08
Itr 000022 | Wall 1.185e+01(0.52) | Time/Itr 0.54(0.60) | BPD 9.37(11.29) | Loss 9.37(11.29) | FE 128(128) | Grad Norm 5.215e+01(5.464e+01) | TT 4.00(4.00) | kinetic_energy: 1.82e-02 | jacobian_norm2: 7.41e-08
Itr 000023 | Wall 1.239e+01(0.52) | Time/Itr 0.54(0.60) | BPD 9.12(11.22) | Loss 9.12(11.22) | FE 128(128) | Grad Norm 5.168e+01(5.455e+01) | TT 4.00(4.00) | kinetic_energy: 2.18e-02 | jacobian_norm2: 8.87e-08
Itr 000024 | Wall 1.288e+01(0.52) | Time/Itr 0.49(0.59) | BPD 8.89(11.15) | Loss 8.89(11.15) | FE 128(128) | Grad Norm 5.131e+01(5.445e+01) | TT 4.00(4.00) | kinetic_energy: 2.59e-02 | jacobian_norm2: 1.05e-07
Itr 000025 | Wall 1.339e+01(0.51) | Time/Itr 0.51(0.59) | BPD 8.66(11.08) | Loss 8.66(11.08) | FE 128(128) | Grad Norm 5.089e+01(5.435e+01) | TT 4.00(4.00) | kinetic_energy: 3.07e-02 | jacobian_norm2: 1.51e-07
Itr 000026 | Wall 1.393e+01(0.52) | Time/Itr 0.54(0.59) | BPD 8.41(11.00) | Loss 8.41(11.00) | FE 128(128) | Grad Norm 5.046e+01(5.423e+01) | TT 4.00(4.00) | kinetic_energy: 3.62e-02 | jacobian_norm2: 1.80e-07
Itr 000027 | Wall 1.445e+01(0.52) | Time/Itr 0.52(0.59) | BPD 8.16(10.91) | Loss 8.16(10.91) | FE 128(128) | Grad Norm 4.997e+01(5.410e+01) | TT 4.00(4.00) | kinetic_energy: 4.26e-02 | jacobian_norm2: 2.05e-07
Itr 000028 | Wall 1.496e+01(0.52) | Time/Itr 0.51(0.59) | BPD 7.88(10.82) | Loss 7.88(10.82) | FE 128(128) | Grad Norm 4.931e+01(5.396e+01) | TT 4.00(4.00) | kinetic_energy: 4.99e-02 | jacobian_norm2: 3.80e-07
Itr 000029 | Wall 1.545e+01(0.52) | Time/Itr 0.50(0.58) | BPD 7.60(10.73) | Loss 7.60(10.73) | FE 128(128) | Grad Norm 4.870e+01(5.380e+01) | TT 4.00(4.00) | kinetic_energy: 5.84e-02 | jacobian_norm2: 3.72e-07
Itr 000030 | Wall 1.596e+01(0.51) | Time/Itr 0.51(0.58) | BPD 7.31(10.62) | Loss 7.31(10.62) | FE 128(128) | Grad Norm 4.799e+01(5.363e+01) | TT 4.00(4.00) | kinetic_energy: 6.81e-02 | jacobian_norm2: 4.86e-07
Itr 000031 | Wall 1.648e+01(0.51) | Time/Itr 0.51(0.58) | BPD 7.02(10.51) | Loss 7.02(10.51) | FE 128(128) | Grad Norm 4.725e+01(5.343e+01) | TT 4.00(4.00) | kinetic_energy: 7.91e-02 | jacobian_norm2: 5.80e-07
Itr 000032 | Wall 1.702e+01(0.52) | Time/Itr 0.55(0.58) | BPD 6.72(10.40) | Loss 6.72(10.40) | FE 128(128) | Grad Norm 4.634e+01(5.322e+01) | TT 4.00(4.00) | kinetic_energy: 9.17e-02 | jacobian_norm2: 6.76e-07
Itr 000033 | Wall 1.755e+01(0.52) | Time/Itr 0.53(0.58) | BPD 6.41(10.28) | Loss 6.41(10.28) | FE 128(128) | Grad Norm 4.533e+01(5.298e+01) | TT 4.00(4.00) | kinetic_energy: 1.06e-01 | jacobian_norm2: 8.92e-07
Itr 000034 | Wall 1.806e+01(0.52) | Time/Itr 0.51(0.57) | BPD 6.10(10.16) | Loss 6.10(10.16) | FE 128(128) | Grad Norm 4.428e+01(5.272e+01) | TT 4.00(4.00) | kinetic_energy: 1.22e-01 | jacobian_norm2: 1.42e-06
Itr 000035 | Wall 1.860e+01(0.52) | Time/Itr 0.54(0.57) | BPD 5.79(10.02) | Loss 5.79(10.03) | FE 128(128) | Grad Norm 4.312e+01(5.244e+01) | TT 4.00(4.00) | kinetic_energy: 1.41e-01 | jacobian_norm2: 1.47e-06
Itr 000036 | Wall 1.913e+01(0.52) | Time/Itr 0.53(0.57) | BPD 5.46(9.89) | Loss 5.47(9.89) | FE 128(128) | Grad Norm 4.171e+01(5.211e+01) | TT 4.00(4.00) | kinetic_energy: 1.62e-01 | jacobian_norm2: 2.01e-06
Itr 000037 | Wall 1.966e+01(0.52) | Time/Itr 0.53(0.57) | BPD 5.15(9.75) | Loss 5.16(9.75) | FE 128(128) | Grad Norm 4.025e+01(5.176e+01) | TT 4.00(4.00) | kinetic_energy: 1.86e-01 | jacobian_norm2: 2.11e-06
Itr 000038 | Wall 2.017e+01(0.52) | Time/Itr 0.51(0.57) | BPD 4.83(9.60) | Loss 4.84(9.60) | FE 128(128) | Grad Norm 3.853e+01(5.136e+01) | TT 4.00(4.00) | kinetic_energy: 2.13e-01 | jacobian_norm2: 3.16e-06
Itr 000039 | Wall 2.070e+01(0.52) | Time/Itr 0.53(0.57) | BPD 4.53(9.45) | Loss 4.53(9.45) | FE 128(128) | Grad Norm 3.680e+01(5.092e+01) | TT 4.00(4.00) | kinetic_energy: 2.43e-01 | jacobian_norm2: 3.76e-06
Itr 000040 | Wall 2.121e+01(0.52) | Time/Itr 0.50(0.57) | BPD 4.22(9.29) | Loss 4.23(9.29) | FE 128(128) | Grad Norm 3.466e+01(5.044e+01) | TT 4.00(4.00) | kinetic_energy: 2.77e-01 | jacobian_norm2: 4.76e-06
Itr 000041 | Wall 2.175e+01(0.52) | Time/Itr 0.55(0.57) | BPD 3.93(9.13) | Loss 3.93(9.13) | FE 128(128) | Grad Norm 3.241e+01(4.990e+01) | TT 4.00(4.00) | kinetic_energy: 3.16e-01 | jacobian_norm2: 6.51e-06
Itr 000042 | Wall 2.229e+01(0.52) | Time/Itr 0.54(0.56) | BPD 3.65(8.96) | Loss 3.65(8.97) | FE 128(128) | Grad Norm 2.984e+01(4.929e+01) | TT 4.00(4.00) | kinetic_energy: 3.58e-01 | jacobian_norm2: 9.76e-06
Itr 000043 | Wall 2.281e+01(0.52) | Time/Itr 0.51(0.56) | BPD 3.39(8.80) | Loss 3.39(8.80) | FE 128(128) | Grad Norm 2.714e+01(4.863e+01) | TT 4.00(4.00) | kinetic_energy: 4.06e-01 | jacobian_norm2: 8.79e-06
Itr 000044 | Wall 2.332e+01(0.52) | Time/Itr 0.51(0.56) | BPD 3.15(8.63) | Loss 3.15(8.63) | FE 128(128) | Grad Norm 2.405e+01(4.789e+01) | TT 4.00(4.00) | kinetic_energy: 4.59e-01 | jacobian_norm2: 1.15e-05
Itr 000045 | Wall 2.384e+01(0.52) | Time/Itr 0.52(0.56) | BPD 2.94(8.46) | Loss 2.95(8.46) | FE 128(128) | Grad Norm 2.089e+01(4.708e+01) | TT 4.00(4.00) | kinetic_energy: 5.17e-01 | jacobian_norm2: 1.37e-05
Itr 000046 | Wall 2.435e+01(0.52) | Time/Itr 0.51(0.56) | BPD 2.76(8.29) | Loss 2.76(8.29) | FE 128(128) | Grad Norm 1.730e+01(4.619e+01) | TT 4.00(4.00) | kinetic_energy: 5.81e-01 | jacobian_norm2: 1.75e-05
Itr 000047 | Wall 2.486e+01(0.52) | Time/Itr 0.50(0.56) | BPD 2.62(8.12) | Loss 2.63(8.12) | FE 128(128) | Grad Norm 1.354e+01(4.521e+01) | TT 4.00(4.00) | kinetic_energy: 6.50e-01 | jacobian_norm2: 2.27e-05
Itr 000048 | Wall 2.535e+01(0.52) | Time/Itr 0.49(0.56) | BPD 2.52(7.95) | Loss 2.53(7.95) | FE 128(128) | Grad Norm 9.764e+00(4.415e+01) | TT 4.00(4.00) | kinetic_energy: 7.24e-01 | jacobian_norm2: 2.52e-05
Itr 000049 | Wall 2.587e+01(0.52) | Time/Itr 0.52(0.55) | BPD 2.46(7.78) | Loss 2.46(7.79) | FE 128(128) | Grad Norm 5.839e+00(4.300e+01) | TT 4.00(4.00) | kinetic_energy: 8.01e-01 | jacobian_norm2: 2.78e-05
Itr 000050 | Wall 2.636e+01(0.52) | Time/Itr 0.49(0.55) | BPD 2.43(7.62) | Loss 2.44(7.62) | FE 128(128) | Grad Norm 3.087e+00(4.180e+01) | TT 4.00(4.00) | kinetic_energy: 8.81e-01 | jacobian_norm2: 4.72e-05
Itr 000051 | Wall 2.687e+01(0.52) | Time/Itr 0.51(0.55) | BPD 2.45(7.47) | Loss 2.46(7.47) | FE 128(128) | Grad Norm 4.316e+00(4.067e+01) | TT 4.00(4.00) | kinetic_energy: 9.60e-01 | jacobian_norm2: 4.90e-05
Itr 000052 | Wall 2.740e+01(0.52) | Time/Itr 0.53(0.55) | BPD 2.50(7.32) | Loss 2.51(7.32) | FE 128(128) | Grad Norm 7.636e+00(3.968e+01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 5.83e-05
Itr 000053 | Wall 2.793e+01(0.52) | Time/Itr 0.53(0.55) | BPD 2.55(7.18) | Loss 2.56(7.18) | FE 128(128) | Grad Norm 1.098e+01(3.882e+01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.97e-05
Itr 000054 | Wall 2.842e+01(0.52) | Time/Itr 0.48(0.55) | BPD 2.62(7.04) | Loss 2.63(7.04) | FE 128(128) | Grad Norm 1.371e+01(3.807e+01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 5.38e-05
Itr 000055 | Wall 2.896e+01(0.52) | Time/Itr 0.54(0.55) | BPD 2.67(6.91) | Loss 2.69(6.91) | FE 128(128) | Grad Norm 1.584e+01(3.740e+01) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 5.47e-05
Itr 000056 | Wall 2.946e+01(0.52) | Time/Itr 0.51(0.55) | BPD 2.71(6.78) | Loss 2.73(6.79) | FE 128(128) | Grad Norm 1.704e+01(3.679e+01) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 7.76e-05
Itr 000057 | Wall 3.000e+01(0.52) | Time/Itr 0.54(0.55) | BPD 2.73(6.66) | Loss 2.74(6.66) | FE 128(128) | Grad Norm 1.762e+01(3.622e+01) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 6.60e-05
Itr 000058 | Wall 3.052e+01(0.52) | Time/Itr 0.52(0.55) | BPD 2.73(6.54) | Loss 2.74(6.55) | FE 128(128) | Grad Norm 1.739e+01(3.565e+01) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 7.00e-05
Itr 000059 | Wall 3.103e+01(0.52) | Time/Itr 0.51(0.54) | BPD 2.70(6.43) | Loss 2.71(6.43) | FE 128(128) | Grad Norm 1.650e+01(3.508e+01) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 4.93e-05
Itr 000060 | Wall 3.153e+01(0.52) | Time/Itr 0.50(0.54) | BPD 2.65(6.31) | Loss 2.67(6.32) | FE 128(128) | Grad Norm 1.517e+01(3.448e+01) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 5.62e-05
Itr 000061 | Wall 3.202e+01(0.52) | Time/Itr 0.49(0.54) | BPD 2.60(6.20) | Loss 2.61(6.21) | FE 128(128) | Grad Norm 1.340e+01(3.385e+01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 5.96e-05
Itr 000062 | Wall 3.255e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.55(6.09) | Loss 2.57(6.10) | FE 128(128) | Grad Norm 1.161e+01(3.318e+01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 4.58e-05
Itr 000063 | Wall 3.308e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.49(5.99) | Loss 2.50(5.99) | FE 128(128) | Grad Norm 9.408e+00(3.247e+01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.07e-05
Itr 000064 | Wall 3.360e+01(0.52) | Time/Itr 0.52(0.54) | BPD 2.45(5.88) | Loss 2.46(5.88) | FE 128(128) | Grad Norm 7.411e+00(3.172e+01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 4.23e-05
Itr 000065 | Wall 3.413e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.41(5.78) | Loss 2.42(5.78) | FE 128(128) | Grad Norm 5.419e+00(3.093e+01) | TT 4.00(4.00) | kinetic_energy: 1.00e+00 | jacobian_norm2: 3.67e-05
Itr 000066 | Wall 3.466e+01(0.52) | Time/Itr 0.52(0.54) | BPD 2.38(5.67) | Loss 2.39(5.68) | FE 128(128) | Grad Norm 3.723e+00(3.011e+01) | TT 4.00(4.00) | kinetic_energy: 9.62e-01 | jacobian_norm2: 3.44e-05
Itr 000067 | Wall 3.516e+01(0.52) | Time/Itr 0.50(0.54) | BPD 2.36(5.57) | Loss 2.37(5.58) | FE 128(128) | Grad Norm 2.510e+00(2.928e+01) | TT 4.00(4.00) | kinetic_energy: 9.23e-01 | jacobian_norm2: 3.39e-05
Itr 000068 | Wall 3.570e+01(0.52) | Time/Itr 0.55(0.54) | BPD 2.36(5.48) | Loss 2.37(5.48) | FE 128(128) | Grad Norm 2.212e+00(2.847e+01) | TT 4.00(4.00) | kinetic_energy: 8.88e-01 | jacobian_norm2: 2.49e-05
Itr 000069 | Wall 3.627e+01(0.52) | Time/Itr 0.57(0.54) | BPD 2.35(5.38) | Loss 2.36(5.39) | FE 128(128) | Grad Norm 2.923e+00(2.770e+01) | TT 4.00(4.00) | kinetic_energy: 8.57e-01 | jacobian_norm2: 2.12e-05
Itr 000070 | Wall 3.679e+01(0.52) | Time/Itr 0.52(0.54) | BPD 2.35(5.29) | Loss 2.36(5.30) | FE 128(128) | Grad Norm 3.883e+00(2.699e+01) | TT 4.00(4.00) | kinetic_energy: 8.30e-01 | jacobian_norm2: 2.13e-05
Itr 000071 | Wall 3.728e+01(0.52) | Time/Itr 0.49(0.54) | BPD 2.36(5.20) | Loss 2.37(5.21) | FE 128(128) | Grad Norm 4.735e+00(2.632e+01) | TT 4.00(4.00) | kinetic_energy: 8.08e-01 | jacobian_norm2: 2.12e-05
Itr 000072 | Wall 3.784e+01(0.52) | Time/Itr 0.56(0.54) | BPD 2.36(5.12) | Loss 2.37(5.13) | FE 128(128) | Grad Norm 5.505e+00(2.570e+01) | TT 4.00(4.00) | kinetic_energy: 7.89e-01 | jacobian_norm2: 1.97e-05
Itr 000073 | Wall 3.836e+01(0.52) | Time/Itr 0.52(0.54) | BPD 2.37(5.04) | Loss 2.38(5.04) | FE 128(128) | Grad Norm 6.061e+00(2.511e+01) | TT 4.00(4.00) | kinetic_energy: 7.75e-01 | jacobian_norm2: 1.60e-05
Itr 000074 | Wall 3.887e+01(0.52) | Time/Itr 0.52(0.54) | BPD 2.37(4.96) | Loss 2.38(4.96) | FE 128(128) | Grad Norm 6.541e+00(2.455e+01) | TT 4.00(4.00) | kinetic_energy: 7.64e-01 | jacobian_norm2: 1.96e-05
Itr 000075 | Wall 3.938e+01(0.52) | Time/Itr 0.51(0.54) | BPD 2.37(4.88) | Loss 2.38(4.89) | FE 128(128) | Grad Norm 6.737e+00(2.402e+01) | TT 4.00(4.00) | kinetic_energy: 7.57e-01 | jacobian_norm2: 1.92e-05
Itr 000076 | Wall 3.991e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.37(4.80) | Loss 2.38(4.81) | FE 128(128) | Grad Norm 6.924e+00(2.350e+01) | TT 4.00(4.00) | kinetic_energy: 7.54e-01 | jacobian_norm2: 1.86e-05
Itr 000077 | Wall 4.044e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.37(4.73) | Loss 2.37(4.74) | FE 128(128) | Grad Norm 6.892e+00(2.301e+01) | TT 4.00(4.00) | kinetic_energy: 7.54e-01 | jacobian_norm2: 1.47e-05
Itr 000078 | Wall 4.097e+01(0.52) | Time/Itr 0.53(0.54) | BPD 2.36(4.66) | Loss 2.37(4.67) | FE 128(128) | Grad Norm 6.705e+00(2.252e+01) | TT 4.00(4.00) | kinetic_energy: 7.57e-01 | jacobian_norm2: 1.68e-05
Itr 000079 | Wall 4.147e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.35(4.59) | Loss 2.36(4.60) | FE 128(128) | Grad Norm 6.405e+00(2.203e+01) | TT 4.00(4.00) | kinetic_energy: 7.63e-01 | jacobian_norm2: 1.94e-05
Itr 000080 | Wall 4.200e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.35(4.52) | Loss 2.35(4.53) | FE 128(128) | Grad Norm 5.932e+00(2.155e+01) | TT 4.00(4.00) | kinetic_energy: 7.71e-01 | jacobian_norm2: 1.79e-05
Itr 000081 | Wall 4.253e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.33(4.46) | Loss 2.34(4.46) | FE 128(128) | Grad Norm 5.569e+00(2.107e+01) | TT 4.00(4.00) | kinetic_energy: 7.82e-01 | jacobian_norm2: 1.70e-05
Itr 000082 | Wall 4.306e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.32(4.39) | Loss 2.33(4.40) | FE 128(128) | Grad Norm 5.003e+00(2.059e+01) | TT 4.00(4.00) | kinetic_energy: 7.94e-01 | jacobian_norm2: 1.67e-05
Itr 000083 | Wall 4.359e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.31(4.33) | Loss 2.32(4.34) | FE 128(128) | Grad Norm 4.450e+00(2.010e+01) | TT 4.00(4.00) | kinetic_energy: 8.08e-01 | jacobian_norm2: 2.08e-05
Itr 000084 | Wall 4.412e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.30(4.27) | Loss 2.31(4.28) | FE 128(128) | Grad Norm 3.825e+00(1.962e+01) | TT 4.00(4.00) | kinetic_energy: 8.22e-01 | jacobian_norm2: 1.99e-05
Itr 000085 | Wall 4.464e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.29(4.21) | Loss 2.30(4.22) | FE 128(128) | Grad Norm 3.247e+00(1.912e+01) | TT 4.00(4.00) | kinetic_energy: 8.38e-01 | jacobian_norm2: 1.48e-05
Itr 000086 | Wall 4.518e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.29(4.15) | Loss 2.30(4.16) | FE 128(128) | Grad Norm 2.770e+00(1.863e+01) | TT 4.00(4.00) | kinetic_energy: 8.53e-01 | jacobian_norm2: 2.33e-05
Itr 000087 | Wall 4.569e+01(0.52) | Time/Itr 0.51(0.53) | BPD 2.28(4.10) | Loss 2.29(4.10) | FE 128(128) | Grad Norm 2.301e+00(1.814e+01) | TT 4.00(4.00) | kinetic_energy: 8.68e-01 | jacobian_norm2: 1.93e-05
Itr 000088 | Wall 4.618e+01(0.52) | Time/Itr 0.49(0.53) | BPD 2.28(4.04) | Loss 2.28(4.05) | FE 128(128) | Grad Norm 2.006e+00(1.766e+01) | TT 4.00(4.00) | kinetic_energy: 8.82e-01 | jacobian_norm2: 2.05e-05
Itr 000089 | Wall 4.672e+01(0.52) | Time/Itr 0.54(0.53) | BPD 2.28(3.99) | Loss 2.29(4.00) | FE 128(128) | Grad Norm 1.888e+00(1.719e+01) | TT 4.00(4.00) | kinetic_energy: 8.95e-01 | jacobian_norm2: 2.07e-05
Itr 000090 | Wall 4.723e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.27(3.94) | Loss 2.28(3.94) | FE 128(128) | Grad Norm 1.907e+00(1.673e+01) | TT 4.00(4.00) | kinetic_energy: 9.07e-01 | jacobian_norm2: 2.30e-05
Itr 000091 | Wall 4.773e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.27(3.89) | Loss 2.28(3.89) | FE 128(128) | Grad Norm 2.042e+00(1.629e+01) | TT 4.00(4.00) | kinetic_energy: 9.16e-01 | jacobian_norm2: 1.93e-05
Itr 000092 | Wall 4.825e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.27(3.84) | Loss 2.28(3.85) | FE 128(128) | Grad Norm 2.118e+00(1.586e+01) | TT 4.00(4.00) | kinetic_energy: 9.24e-01 | jacobian_norm2: 2.43e-05
Itr 000093 | Wall 4.881e+01(0.52) | Time/Itr 0.56(0.53) | BPD 2.26(3.79) | Loss 2.27(3.80) | FE 128(128) | Grad Norm 2.245e+00(1.545e+01) | TT 4.00(4.00) | kinetic_energy: 9.29e-01 | jacobian_norm2: 2.44e-05
Itr 000094 | Wall 4.933e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.26(3.75) | Loss 2.27(3.75) | FE 128(128) | Grad Norm 2.262e+00(1.506e+01) | TT 4.00(4.00) | kinetic_energy: 9.31e-01 | jacobian_norm2: 2.49e-05
Itr 000095 | Wall 4.988e+01(0.52) | Time/Itr 0.55(0.53) | BPD 2.26(3.70) | Loss 2.26(3.71) | FE 128(128) | Grad Norm 2.208e+00(1.467e+01) | TT 4.00(4.00) | kinetic_energy: 9.32e-01 | jacobian_norm2: 2.65e-05
Itr 000096 | Wall 5.038e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.25(3.66) | Loss 2.26(3.67) | FE 128(128) | Grad Norm 2.210e+00(1.430e+01) | TT 4.00(4.00) | kinetic_energy: 9.31e-01 | jacobian_norm2: 2.41e-05
Itr 000097 | Wall 5.090e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.25(3.62) | Loss 2.26(3.62) | FE 128(128) | Grad Norm 2.119e+00(1.393e+01) | TT 4.00(4.00) | kinetic_energy: 9.27e-01 | jacobian_norm2: 1.73e-05
Itr 000098 | Wall 5.140e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.24(3.57) | Loss 2.25(3.58) | FE 128(128) | Grad Norm 1.969e+00(1.357e+01) | TT 4.00(4.00) | kinetic_energy: 9.23e-01 | jacobian_norm2: 1.82e-05
Itr 000099 | Wall 5.193e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.24(3.53) | Loss 2.25(3.54) | FE 128(128) | Grad Norm 1.864e+00(1.322e+01) | TT 4.00(4.00) | kinetic_energy: 9.17e-01 | jacobian_norm2: 2.51e-05
Itr 000100 | Wall 5.244e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.23(3.50) | Loss 2.24(3.50) | FE 128(128) | Grad Norm 1.788e+00(1.288e+01) | TT 4.00(4.00) | kinetic_energy: 9.10e-01 | jacobian_norm2: 2.14e-05
Itr 000101 | Wall 5.294e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.23(3.46) | Loss 2.24(3.46) | FE 128(128) | Grad Norm 1.756e+00(1.255e+01) | TT 4.00(4.00) | kinetic_energy: 9.03e-01 | jacobian_norm2: 2.00e-05
Itr 000102 | Wall 5.347e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.22(3.42) | Loss 2.23(3.43) | FE 128(128) | Grad Norm 1.798e+00(1.222e+01) | TT 4.00(4.00) | kinetic_energy: 8.96e-01 | jacobian_norm2: 1.60e-05
Itr 000103 | Wall 5.397e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.22(3.38) | Loss 2.23(3.39) | FE 128(128) | Grad Norm 1.918e+00(1.192e+01) | TT 4.00(4.00) | kinetic_energy: 8.90e-01 | jacobian_norm2: 1.96e-05
Itr 000104 | Wall 5.448e+01(0.52) | Time/Itr 0.51(0.53) | BPD 2.22(3.35) | Loss 2.23(3.36) | FE 128(128) | Grad Norm 2.069e+00(1.162e+01) | TT 4.00(4.00) | kinetic_energy: 8.83e-01 | jacobian_norm2: 2.07e-05
Itr 000105 | Wall 5.500e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.21(3.32) | Loss 2.22(3.32) | FE 128(128) | Grad Norm 2.188e+00(1.134e+01) | TT 4.00(4.00) | kinetic_energy: 8.78e-01 | jacobian_norm2: 1.40e-05
Itr 000106 | Wall 5.552e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.21(3.28) | Loss 2.22(3.29) | FE 128(128) | Grad Norm 2.333e+00(1.107e+01) | TT 4.00(4.00) | kinetic_energy: 8.74e-01 | jacobian_norm2: 1.50e-05
Itr 000107 | Wall 5.600e+01(0.52) | Time/Itr 0.47(0.52) | BPD 2.21(3.25) | Loss 2.21(3.26) | FE 128(128) | Grad Norm 2.442e+00(1.081e+01) | TT 4.00(4.00) | kinetic_energy: 8.70e-01 | jacobian_norm2: 2.24e-05
Itr 000108 | Wall 5.650e+01(0.52) | Time/Itr 0.50(0.52) | BPD 2.20(3.22) | Loss 2.21(3.23) | FE 128(128) | Grad Norm 2.605e+00(1.056e+01) | TT 4.00(4.00) | kinetic_energy: 8.69e-01 | jacobian_norm2: 1.94e-05
Itr 000109 | Wall 5.701e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.20(3.19) | Loss 2.21(3.20) | FE 128(128) | Grad Norm 2.612e+00(1.032e+01) | TT 4.00(4.00) | kinetic_energy: 8.68e-01 | jacobian_norm2: 1.74e-05
Itr 000110 | Wall 5.754e+01(0.52) | Time/Itr 0.53(0.52) | BPD 2.20(3.16) | Loss 2.21(3.17) | FE 128(128) | Grad Norm 2.617e+00(1.009e+01) | TT 4.00(4.00) | kinetic_energy: 8.68e-01 | jacobian_norm2: 1.61e-05
Itr 000111 | Wall 5.808e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.19(3.13) | Loss 2.20(3.14) | FE 128(128) | Grad Norm 2.601e+00(9.867e+00) | TT 4.00(4.00) | kinetic_energy: 8.70e-01 | jacobian_norm2: 1.53e-05
Itr 000112 | Wall 5.858e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.19(3.10) | Loss 2.20(3.11) | FE 128(128) | Grad Norm 2.476e+00(9.646e+00) | TT 4.00(4.00) | kinetic_energy: 8.72e-01 | jacobian_norm2: 1.71e-05
Itr 000113 | Wall 5.909e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.18(3.07) | Loss 2.19(3.08) | FE 128(128) | Grad Norm 2.392e+00(9.428e+00) | TT 4.00(4.00) | kinetic_energy: 8.76e-01 | jacobian_norm2: 1.26e-05
Itr 000114 | Wall 5.964e+01(0.52) | Time/Itr 0.55(0.52) | BPD 2.18(3.05) | Loss 2.19(3.05) | FE 128(128) | Grad Norm 2.248e+00(9.213e+00) | TT 4.00(4.00) | kinetic_energy: 8.80e-01 | jacobian_norm2: 1.55e-05
Itr 000115 | Wall 6.018e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.18(3.02) | Loss 2.18(3.03) | FE 128(128) | Grad Norm 2.144e+00(9.001e+00) | TT 4.00(4.00) | kinetic_energy: 8.85e-01 | jacobian_norm2: 1.84e-05
Itr 000116 | Wall 6.074e+01(0.52) | Time/Itr 0.56(0.53) | BPD 2.17(2.99) | Loss 2.18(3.00) | FE 128(128) | Grad Norm 2.050e+00(8.792e+00) | TT 4.00(4.00) | kinetic_energy: 8.90e-01 | jacobian_norm2: 1.73e-05
Itr 000117 | Wall 6.129e+01(0.52) | Time/Itr 0.55(0.53) | BPD 2.17(2.97) | Loss 2.17(2.98) | FE 128(128) | Grad Norm 1.916e+00(8.586e+00) | TT 4.00(4.00) | kinetic_energy: 8.95e-01 | jacobian_norm2: 1.51e-05
Itr 000118 | Wall 6.181e+01(0.52) | Time/Itr 0.52(0.53) | BPD 2.16(2.95) | Loss 2.17(2.95) | FE 128(128) | Grad Norm 1.847e+00(8.384e+00) | TT 4.00(4.00) | kinetic_energy: 9.00e-01 | jacobian_norm2: 1.25e-05
Itr 000119 | Wall 6.235e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.16(2.92) | Loss 2.17(2.93) | FE 128(128) | Grad Norm 1.736e+00(8.184e+00) | TT 4.00(4.00) | kinetic_energy: 9.05e-01 | jacobian_norm2: 1.54e-05
Itr 000120 | Wall 6.289e+01(0.52) | Time/Itr 0.54(0.53) | BPD 2.15(2.90) | Loss 2.16(2.91) | FE 128(128) | Grad Norm 1.699e+00(7.990e+00) | TT 4.00(4.00) | kinetic_energy: 9.09e-01 | jacobian_norm2: 1.68e-05
Itr 000121 | Wall 6.342e+01(0.52) | Time/Itr 0.53(0.53) | BPD 2.15(2.88) | Loss 2.16(2.88) | FE 128(128) | Grad Norm 1.638e+00(7.799e+00) | TT 4.00(4.00) | kinetic_energy: 9.13e-01 | jacobian_norm2: 1.73e-05
Itr 000122 | Wall 6.396e+01(0.52) | Time/Itr 0.55(0.53) | BPD 2.14(2.85) | Loss 2.15(2.86) | FE 128(128) | Grad Norm 1.593e+00(7.613e+00) | TT 4.00(4.00) | kinetic_energy: 9.16e-01 | jacobian_norm2: 1.55e-05
Itr 000123 | Wall 6.446e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.14(2.83) | Loss 2.15(2.84) | FE 128(128) | Grad Norm 1.566e+00(7.431e+00) | TT 4.00(4.00) | kinetic_energy: 9.18e-01 | jacobian_norm2: 1.26e-05
Itr 000124 | Wall 6.496e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.13(2.81) | Loss 2.14(2.82) | FE 128(128) | Grad Norm 1.551e+00(7.255e+00) | TT 4.00(4.00) | kinetic_energy: 9.20e-01 | jacobian_norm2: 1.55e-05
Itr 000125 | Wall 6.547e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.13(2.79) | Loss 2.14(2.80) | FE 128(128) | Grad Norm 1.540e+00(7.084e+00) | TT 4.00(4.00) | kinetic_energy: 9.21e-01 | jacobian_norm2: 1.29e-05
Itr 000126 | Wall 6.596e+01(0.52) | Time/Itr 0.49(0.52) | BPD 2.13(2.77) | Loss 2.14(2.78) | FE 128(128) | Grad Norm 1.563e+00(6.918e+00) | TT 4.00(4.00) | kinetic_energy: 9.22e-01 | jacobian_norm2: 2.15e-05
Itr 000127 | Wall 6.647e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.13(2.75) | Loss 2.14(2.76) | FE 128(128) | Grad Norm 1.544e+00(6.757e+00) | TT 4.00(4.00) | kinetic_energy: 9.22e-01 | jacobian_norm2: 1.65e-05
Itr 000128 | Wall 6.696e+01(0.52) | Time/Itr 0.49(0.52) | BPD 2.12(2.73) | Loss 2.13(2.74) | FE 128(128) | Grad Norm 1.560e+00(6.601e+00) | TT 4.00(4.00) | kinetic_energy: 9.22e-01 | jacobian_norm2: 1.55e-05
Itr 000129 | Wall 6.747e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.11(2.71) | Loss 2.12(2.72) | FE 128(128) | Grad Norm 1.591e+00(6.451e+00) | TT 4.00(4.00) | kinetic_energy: 9.21e-01 | jacobian_norm2: 1.28e-05
Itr 000130 | Wall 6.798e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.11(2.70) | Loss 2.12(2.70) | FE 128(128) | Grad Norm 1.599e+00(6.305e+00) | TT 4.00(4.00) | kinetic_energy: 9.20e-01 | jacobian_norm2: 1.50e-05
Itr 000131 | Wall 6.847e+01(0.52) | Time/Itr 0.49(0.52) | BPD 2.11(2.68) | Loss 2.12(2.69) | FE 128(128) | Grad Norm 1.637e+00(6.165e+00) | TT 4.00(4.00) | kinetic_energy: 9.19e-01 | jacobian_norm2: 1.34e-05
Itr 000132 | Wall 6.903e+01(0.52) | Time/Itr 0.56(0.52) | BPD 2.10(2.66) | Loss 2.11(2.67) | FE 128(128) | Grad Norm 1.657e+00(6.030e+00) | TT 4.00(4.00) | kinetic_energy: 9.19e-01 | jacobian_norm2: 1.41e-05
Itr 000133 | Wall 6.953e+01(0.52) | Time/Itr 0.50(0.52) | BPD 2.10(2.64) | Loss 2.11(2.65) | FE 128(128) | Grad Norm 1.732e+00(5.901e+00) | TT 4.00(4.00) | kinetic_energy: 9.19e-01 | jacobian_norm2: 1.14e-05
Itr 000134 | Wall 7.006e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.10(2.63) | Loss 2.11(2.64) | FE 128(128) | Grad Norm 1.749e+00(5.776e+00) | TT 4.00(4.00) | kinetic_energy: 9.19e-01 | jacobian_norm2: 1.40e-05
Itr 000135 | Wall 7.058e+01(0.52) | Time/Itr 0.52(0.52) | BPD 2.09(2.61) | Loss 2.10(2.62) | FE 128(128) | Grad Norm 1.735e+00(5.655e+00) | TT 4.00(4.00) | kinetic_energy: 9.20e-01 | jacobian_norm2: 1.32e-05
Itr 000136 | Wall 7.112e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.09(2.60) | Loss 2.10(2.61) | FE 128(128) | Grad Norm 1.727e+00(5.537e+00) | TT 4.00(4.00) | kinetic_energy: 9.21e-01 | jacobian_norm2: 1.34e-05
Itr 000137 | Wall 7.167e+01(0.52) | Time/Itr 0.55(0.52) | BPD 2.08(2.58) | Loss 2.09(2.59) | FE 128(128) | Grad Norm 1.713e+00(5.422e+00) | TT 4.00(4.00) | kinetic_energy: 9.22e-01 | jacobian_norm2: 1.32e-05
Itr 000138 | Wall 7.218e+01(0.52) | Time/Itr 0.52(0.52) | BPD 2.08(2.57) | Loss 2.09(2.57) | FE 128(128) | Grad Norm 1.672e+00(5.310e+00) | TT 4.00(4.00) | kinetic_energy: 9.24e-01 | jacobian_norm2: 1.33e-05
Itr 000139 | Wall 7.272e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.08(2.55) | Loss 2.09(2.56) | FE 128(128) | Grad Norm 1.630e+00(5.200e+00) | TT 4.00(4.00) | kinetic_energy: 9.27e-01 | jacobian_norm2: 1.03e-05
Itr 000140 | Wall 7.326e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.07(2.54) | Loss 2.08(2.55) | FE 128(128) | Grad Norm 1.665e+00(5.094e+00) | TT 4.00(4.00) | kinetic_energy: 9.30e-01 | jacobian_norm2: 1.13e-05
Itr 000141 | Wall 7.376e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.07(2.52) | Loss 2.07(2.53) | FE 128(128) | Grad Norm 1.604e+00(4.989e+00) | TT 4.00(4.00) | kinetic_energy: 9.33e-01 | jacobian_norm2: 1.36e-05
Itr 000142 | Wall 7.434e+01(0.52) | Time/Itr 0.57(0.52) | BPD 2.06(2.51) | Loss 2.07(2.52) | FE 128(128) | Grad Norm 1.581e+00(4.887e+00) | TT 4.00(4.00) | kinetic_energy: 9.36e-01 | jacobian_norm2: 1.14e-05
Itr 000143 | Wall 7.488e+01(0.52) | Time/Itr 0.54(0.53) | BPD 2.06(2.50) | Loss 2.07(2.50) | FE 128(128) | Grad Norm 1.533e+00(4.786e+00) | TT 4.00(4.00) | kinetic_energy: 9.40e-01 | jacobian_norm2: 1.16e-05
Itr 000144 | Wall 7.539e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.05(2.48) | Loss 2.06(2.49) | FE 128(128) | Grad Norm 1.495e+00(4.687e+00) | TT 4.00(4.00) | kinetic_energy: 9.43e-01 | jacobian_norm2: 1.52e-05
Itr 000145 | Wall 7.590e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.05(2.47) | Loss 2.06(2.48) | FE 128(128) | Grad Norm 1.469e+00(4.591e+00) | TT 4.00(4.00) | kinetic_energy: 9.47e-01 | jacobian_norm2: 1.43e-05
Itr 000146 | Wall 7.642e+01(0.52) | Time/Itr 0.53(0.52) | BPD 2.04(2.46) | Loss 2.05(2.47) | FE 128(128) | Grad Norm 1.475e+00(4.497e+00) | TT 4.00(4.00) | kinetic_energy: 9.50e-01 | jacobian_norm2: 1.28e-05
Itr 000147 | Wall 7.696e+01(0.52) | Time/Itr 0.54(0.52) | BPD 2.04(2.44) | Loss 2.05(2.45) | FE 128(128) | Grad Norm 1.407e+00(4.405e+00) | TT 4.00(4.00) | kinetic_energy: 9.53e-01 | jacobian_norm2: 1.54e-05
Itr 000148 | Wall 7.747e+01(0.52) | Time/Itr 0.50(0.52) | BPD 2.04(2.43) | Loss 2.05(2.44) | FE 128(128) | Grad Norm 1.395e+00(4.314e+00) | TT 4.00(4.00) | kinetic_energy: 9.56e-01 | jacobian_norm2: 1.27e-05
Itr 000149 | Wall 7.802e+01(0.52) | Time/Itr 0.55(0.52) | BPD 2.03(2.42) | Loss 2.04(2.43) | FE 128(128) | Grad Norm 1.380e+00(4.226e+00) | TT 4.00(4.00) | kinetic_energy: 9.59e-01 | jacobian_norm2: 1.46e-05
Itr 000150 | Wall 7.858e+01(0.52) | Time/Itr 0.57(0.53) | BPD 2.03(2.41) | Loss 2.04(2.42) | FE 128(128) | Grad Norm 1.355e+00(4.140e+00) | TT 4.00(4.00) | kinetic_energy: 9.61e-01 | jacobian_norm2: 1.31e-05
Itr 000151 | Wall 7.909e+01(0.52) | Time/Itr 0.51(0.53) | BPD 2.03(2.40) | Loss 2.04(2.41) | FE 128(128) | Grad Norm 1.344e+00(4.056e+00) | TT 4.00(4.00) | kinetic_energy: 9.63e-01 | jacobian_norm2: 1.42e-05
Itr 000152 | Wall 7.966e+01(0.52) | Time/Itr 0.57(0.53) | BPD 2.02(2.39) | Loss 2.03(2.39) | FE 128(128) | Grad Norm 1.357e+00(3.975e+00) | TT 4.00(4.00) | kinetic_energy: 9.65e-01 | jacobian_norm2: 1.37e-05
Itr 000153 | Wall 8.015e+01(0.52) | Time/Itr 0.50(0.53) | BPD 2.01(2.37) | Loss 2.02(2.38) | FE 128(128) | Grad Norm 1.370e+00(3.897e+00) | TT 4.00(4.00) | kinetic_energy: 9.67e-01 | jacobian_norm2: 1.27e-05
Itr 000154 | Wall 8.066e+01(0.52) | Time/Itr 0.51(0.53) | BPD 2.01(2.36) | Loss 2.02(2.37) | FE 128(128) | Grad Norm 1.372e+00(3.821e+00) | TT 4.00(4.00) | kinetic_energy: 9.68e-01 | jacobian_norm2: 1.23e-05
Itr 000155 | Wall 8.117e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.01(2.35) | Loss 2.02(2.36) | FE 128(128) | Grad Norm 1.320e+00(3.746e+00) | TT 4.00(4.00) | kinetic_energy: 9.70e-01 | jacobian_norm2: 1.53e-05
Itr 000156 | Wall 8.171e+01(0.52) | Time/Itr 0.54(0.53) | BPD 2.00(2.34) | Loss 2.01(2.35) | FE 128(128) | Grad Norm 1.346e+00(3.674e+00) | TT 4.00(4.00) | kinetic_energy: 9.71e-01 | jacobian_norm2: 1.34e-05
Itr 000157 | Wall 8.223e+01(0.52) | Time/Itr 0.51(0.52) | BPD 2.00(2.33) | Loss 2.01(2.34) | FE 128(128) | Grad Norm 1.345e+00(3.604e+00) | TT 4.00(4.00) | kinetic_energy: 9.73e-01 | jacobian_norm2: 1.57e-05
Itr 000158 | Wall 8.275e+01(0.52) | Time/Itr 0.52(0.52) | BPD 1.99(2.32) | Loss 2.00(2.33) | FE 128(128) | Grad Norm 1.349e+00(3.537e+00) | TT 4.00(4.00) | kinetic_energy: 9.75e-01 | jacobian_norm2: 1.35e-05
Itr 000159 | Wall 8.327e+01(0.52) | Time/Itr 0.52(0.52) | BPD 1.99(2.31) | Loss 2.00(2.32) | FE 128(128) | Grad Norm 1.350e+00(3.471e+00) | TT 4.00(4.00) | kinetic_energy: 9.77e-01 | jacobian_norm2: 1.53e-05
Itr 000160 | Wall 8.383e+01(0.52) | Time/Itr 0.56(0.53) | BPD 1.99(2.30) | Loss 2.00(2.31) | FE 128(128) | Grad Norm 1.320e+00(3.407e+00) | TT 4.00(4.00) | kinetic_energy: 9.80e-01 | jacobian_norm2: 1.52e-05
Itr 000161 | Wall 8.435e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.98(2.29) | Loss 1.99(2.30) | FE 128(128) | Grad Norm 1.308e+00(3.344e+00) | TT 4.00(4.00) | kinetic_energy: 9.83e-01 | jacobian_norm2: 1.62e-05
Itr 000162 | Wall 8.489e+01(0.52) | Time/Itr 0.54(0.53) | BPD 1.98(2.28) | Loss 1.99(2.29) | FE 128(128) | Grad Norm 1.302e+00(3.282e+00) | TT 4.00(4.00) | kinetic_energy: 9.86e-01 | jacobian_norm2: 1.71e-05
Itr 000163 | Wall 8.541e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.97(2.27) | Loss 1.98(2.28) | FE 128(128) | Grad Norm 1.247e+00(3.221e+00) | TT 4.00(4.00) | kinetic_energy: 9.90e-01 | jacobian_norm2: 1.75e-05
Itr 000164 | Wall 8.595e+01(0.52) | Time/Itr 0.54(0.53) | BPD 1.97(2.26) | Loss 1.98(2.27) | FE 128(128) | Grad Norm 1.256e+00(3.162e+00) | TT 4.00(4.00) | kinetic_energy: 9.93e-01 | jacobian_norm2: 1.84e-05
Itr 000165 | Wall 8.646e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.96(2.26) | Loss 1.97(2.26) | FE 128(128) | Grad Norm 1.232e+00(3.104e+00) | TT 4.00(4.00) | kinetic_energy: 9.97e-01 | jacobian_norm2: 1.82e-05
Itr 000166 | Wall 8.698e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.96(2.25) | Loss 1.97(2.26) | FE 128(128) | Grad Norm 1.216e+00(3.048e+00) | TT 4.00(4.00) | kinetic_energy: 1.00e+00 | jacobian_norm2: 1.66e-05
Itr 000167 | Wall 8.750e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.95(2.24) | Loss 1.96(2.25) | FE 128(128) | Grad Norm 1.184e+00(2.992e+00) | TT 4.00(4.00) | kinetic_energy: 1.00e+00 | jacobian_norm2: 2.10e-05
Itr 000168 | Wall 8.806e+01(0.52) | Time/Itr 0.56(0.53) | BPD 1.95(2.23) | Loss 1.96(2.24) | FE 128(128) | Grad Norm 1.165e+00(2.937e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.11e-05
Itr 000169 | Wall 8.859e+01(0.52) | Time/Itr 0.53(0.53) | BPD 1.95(2.22) | Loss 1.96(2.23) | FE 128(128) | Grad Norm 1.131e+00(2.883e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.52e-05
Itr 000170 | Wall 8.913e+01(0.52) | Time/Itr 0.55(0.53) | BPD 1.94(2.21) | Loss 1.95(2.22) | FE 128(128) | Grad Norm 1.141e+00(2.831e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.17e-05
Itr 000171 | Wall 8.968e+01(0.52) | Time/Itr 0.55(0.53) | BPD 1.93(2.20) | Loss 1.94(2.21) | FE 128(128) | Grad Norm 1.143e+00(2.780e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.18e-05
Itr 000172 | Wall 9.019e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.93(2.20) | Loss 1.94(2.20) | FE 128(128) | Grad Norm 1.125e+00(2.730e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.49e-05
Itr 000173 | Wall 9.071e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.93(2.19) | Loss 1.94(2.20) | FE 128(128) | Grad Norm 1.085e+00(2.681e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.54e-05
Itr 000174 | Wall 9.125e+01(0.52) | Time/Itr 0.54(0.53) | BPD 1.92(2.18) | Loss 1.93(2.19) | FE 128(128) | Grad Norm 1.101e+00(2.634e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.86e-05
Itr 000175 | Wall 9.175e+01(0.52) | Time/Itr 0.50(0.53) | BPD 1.92(2.17) | Loss 1.93(2.18) | FE 128(128) | Grad Norm 1.103e+00(2.588e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.39e-05
Itr 000176 | Wall 9.229e+01(0.52) | Time/Itr 0.55(0.53) | BPD 1.91(2.16) | Loss 1.92(2.17) | FE 128(128) | Grad Norm 1.067e+00(2.542e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.58e-05
Itr 000177 | Wall 9.284e+01(0.52) | Time/Itr 0.55(0.53) | BPD 1.91(2.16) | Loss 1.92(2.17) | FE 128(128) | Grad Norm 1.085e+00(2.498e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 3.25e-05
Itr 000178 | Wall 9.335e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.91(2.15) | Loss 1.92(2.16) | FE 128(128) | Grad Norm 1.047e+00(2.455e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.09e-05
Itr 000179 | Wall 9.387e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.90(2.14) | Loss 1.91(2.15) | FE 128(128) | Grad Norm 1.024e+00(2.412e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.24e-05
Itr 000180 | Wall 9.440e+01(0.52) | Time/Itr 0.53(0.53) | BPD 1.90(2.13) | Loss 1.91(2.14) | FE 128(128) | Grad Norm 1.052e+00(2.371e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.40e-05
Itr 000181 | Wall 9.491e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.89(2.13) | Loss 1.90(2.14) | FE 128(128) | Grad Norm 9.812e-01(2.329e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 4.55e-05
Itr 000182 | Wall 9.544e+01(0.52) | Time/Itr 0.53(0.53) | BPD 1.89(2.12) | Loss 1.90(2.13) | FE 128(128) | Grad Norm 9.470e-01(2.288e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 4.04e-05
Itr 000183 | Wall 9.599e+01(0.52) | Time/Itr 0.55(0.53) | BPD 1.89(2.11) | Loss 1.90(2.12) | FE 128(128) | Grad Norm 9.356e-01(2.247e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.17e-05
Itr 000184 | Wall 9.653e+01(0.52) | Time/Itr 0.54(0.53) | BPD 1.88(2.11) | Loss 1.89(2.12) | FE 128(128) | Grad Norm 9.063e-01(2.207e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.68e-05
Itr 000185 | Wall 9.702e+01(0.52) | Time/Itr 0.49(0.53) | BPD 1.87(2.10) | Loss 1.88(2.11) | FE 128(128) | Grad Norm 9.093e-01(2.168e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.41e-05
Itr 000186 | Wall 9.751e+01(0.52) | Time/Itr 0.50(0.53) | BPD 1.87(2.09) | Loss 1.88(2.10) | FE 128(128) | Grad Norm 9.448e-01(2.131e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 5.29e-05
Itr 000187 | Wall 9.804e+01(0.52) | Time/Itr 0.53(0.53) | BPD 1.87(2.09) | Loss 1.88(2.09) | FE 128(128) | Grad Norm 9.204e-01(2.095e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 6.26e-05
Itr 000188 | Wall 9.858e+01(0.52) | Time/Itr 0.54(0.53) | BPD 1.86(2.08) | Loss 1.87(2.09) | FE 128(128) | Grad Norm 8.964e-01(2.059e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 5.81e-05
Itr 000189 | Wall 9.909e+01(0.52) | Time/Itr 0.51(0.53) | BPD 1.86(2.07) | Loss 1.87(2.08) | FE 128(128) | Grad Norm 8.493e-01(2.023e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.81e-05
Itr 000190 | Wall 9.961e+01(0.52) | Time/Itr 0.52(0.53) | BPD 1.85(2.07) | Loss 1.86(2.08) | FE 128(128) | Grad Norm 8.762e-01(1.988e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.65e-05
Itr 000191 | Wall 1.001e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.85(2.06) | Loss 1.86(2.07) | FE 128(128) | Grad Norm 7.773e-01(1.952e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.94e-05
Itr 000192 | Wall 1.007e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.85(2.05) | Loss 1.86(2.06) | FE 128(128) | Grad Norm 7.603e-01(1.916e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.55e-05
Itr 000193 | Wall 1.012e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.84(2.05) | Loss 1.85(2.06) | FE 128(128) | Grad Norm 7.599e-01(1.882e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 7.27e-05
Itr 000194 | Wall 1.018e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.84(2.04) | Loss 1.85(2.05) | FE 128(128) | Grad Norm 7.683e-01(1.848e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 9.88e-05
Itr 000195 | Wall 1.023e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.84(2.03) | Loss 1.85(2.04) | FE 128(128) | Grad Norm 7.410e-01(1.815e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 8.93e-05
Itr 000196 | Wall 1.028e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.84(2.03) | Loss 1.85(2.04) | FE 128(128) | Grad Norm 7.574e-01(1.783e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 9.24e-05
Itr 000197 | Wall 1.033e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.83(2.02) | Loss 1.84(2.03) | FE 128(128) | Grad Norm 7.345e-01(1.752e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.15e-04
Itr 000198 | Wall 1.038e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.83(2.02) | Loss 1.84(2.03) | FE 128(128) | Grad Norm 6.922e-01(1.720e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 9.03e-05
Itr 000199 | Wall 1.044e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.83(2.01) | Loss 1.84(2.02) | FE 128(128) | Grad Norm 6.466e-01(1.688e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.11e-04
Itr 000200 | Wall 1.049e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.82(2.00) | Loss 1.83(2.02) | FE 128(128) | Grad Norm 6.313e-01(1.656e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 9.97e-05
Itr 000201 | Wall 1.054e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.82(2.00) | Loss 1.83(2.01) | FE 128(128) | Grad Norm 6.081e-01(1.625e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 9.32e-05
Itr 000202 | Wall 1.060e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.82(1.99) | Loss 1.83(2.00) | FE 128(128) | Grad Norm 5.888e-01(1.594e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 1.41e-04
Itr 000203 | Wall 1.065e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.81(1.99) | Loss 1.82(2.00) | FE 128(128) | Grad Norm 5.884e-01(1.564e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 1.33e-04
Itr 000204 | Wall 1.070e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.81(1.98) | Loss 1.82(1.99) | FE 128(128) | Grad Norm 5.950e-01(1.534e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 1.34e-04
Itr 000205 | Wall 1.075e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.81(1.98) | Loss 1.82(1.99) | FE 128(128) | Grad Norm 5.550e-01(1.505e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 1.63e-04
Itr 000206 | Wall 1.081e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.81(1.97) | Loss 1.82(1.98) | FE 128(128) | Grad Norm 4.799e-01(1.474e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 1.78e-04
Itr 000207 | Wall 1.086e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.80(1.97) | Loss 1.81(1.98) | FE 128(128) | Grad Norm 4.670e-01(1.444e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.55e-04
Itr 000208 | Wall 1.092e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.80(1.96) | Loss 1.81(1.97) | FE 128(128) | Grad Norm 4.318e-01(1.414e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.34e-04
Itr 000209 | Wall 1.097e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.80(1.96) | Loss 1.81(1.97) | FE 128(128) | Grad Norm 4.672e-01(1.385e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.70e-04
Itr 000210 | Wall 1.102e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.80(1.95) | Loss 1.81(1.96) | FE 128(128) | Grad Norm 4.124e-01(1.356e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.21e-04
Itr 000211 | Wall 1.107e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.80(1.95) | Loss 1.81(1.96) | FE 128(128) | Grad Norm 4.059e-01(1.328e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.22e-04
Itr 000212 | Wall 1.113e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.79(1.94) | Loss 1.81(1.95) | FE 128(128) | Grad Norm 4.381e-01(1.301e+00) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 1.96e-04
Itr 000213 | Wall 1.118e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.94) | Loss 1.80(1.95) | FE 128(128) | Grad Norm 3.321e-01(1.272e+00) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.33e-04
Itr 000214 | Wall 1.123e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.79(1.93) | Loss 1.80(1.95) | FE 128(128) | Grad Norm 3.275e-01(1.244e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.41e-04
Itr 000215 | Wall 1.129e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.79(1.93) | Loss 1.80(1.94) | FE 128(128) | Grad Norm 2.957e-01(1.215e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.07e-04
Itr 000216 | Wall 1.134e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.93) | Loss 1.80(1.94) | FE 128(128) | Grad Norm 3.493e-01(1.189e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.53e-04
Itr 000217 | Wall 1.139e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.92) | Loss 1.80(1.93) | FE 128(128) | Grad Norm 3.633e-01(1.164e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.20e-04
Itr 000218 | Wall 1.144e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.79(1.92) | Loss 1.80(1.93) | FE 128(128) | Grad Norm 3.849e-01(1.141e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.42e-04
Itr 000219 | Wall 1.150e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.79(1.91) | Loss 1.80(1.93) | FE 128(128) | Grad Norm 2.514e-01(1.114e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.26e-04
Itr 000220 | Wall 1.155e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.79(1.91) | Loss 1.80(1.92) | FE 128(128) | Grad Norm 2.106e-01(1.087e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.40e-04
Itr 000221 | Wall 1.160e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.91) | Loss 1.80(1.92) | FE 128(128) | Grad Norm 2.245e-01(1.061e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.74e-04
Itr 000222 | Wall 1.165e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.90) | Loss 1.80(1.91) | FE 128(128) | Grad Norm 3.116e-01(1.039e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.81e-04
Itr 000223 | Wall 1.170e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.78(1.90) | Loss 1.80(1.91) | FE 128(128) | Grad Norm 5.145e-01(1.023e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 3.51e-04
Itr 000224 | Wall 1.175e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.90) | Loss 1.80(1.91) | FE 128(128) | Grad Norm 2.067e-01(9.986e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 3.07e-04
Itr 000225 | Wall 1.181e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.89) | Loss 1.80(1.90) | FE 128(128) | Grad Norm 2.578e-01(9.764e-01) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 3.08e-04
Itr 000226 | Wall 1.186e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.79(1.89) | Loss 1.80(1.90) | FE 128(128) | Grad Norm 2.096e-01(9.534e-01) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 2.87e-04
Itr 000227 | Wall 1.191e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.89) | Loss 1.80(1.90) | FE 128(128) | Grad Norm 3.622e-01(9.356e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.93e-04
Itr 000228 | Wall 1.197e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.79(1.88) | Loss 1.80(1.89) | FE 128(128) | Grad Norm 3.963e-01(9.195e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 3.19e-04
Itr 000229 | Wall 1.202e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.79(1.88) | Loss 1.80(1.89) | FE 128(128) | Grad Norm 3.979e-01(9.038e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 3.36e-04
Itr 000230 | Wall 1.207e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.78(1.88) | Loss 1.80(1.89) | FE 128(128) | Grad Norm 2.601e-01(8.845e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.10e-04
Itr 000231 | Wall 1.212e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.79(1.88) | Loss 1.80(1.89) | FE 128(128) | Grad Norm 2.555e-01(8.656e-01) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 3.06e-04
Itr 000232 | Wall 1.217e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.87) | Loss 1.80(1.88) | FE 128(128) | Grad Norm 2.117e-01(8.460e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.24e-04
Itr 000233 | Wall 1.222e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.87) | Loss 1.79(1.88) | FE 128(128) | Grad Norm 3.901e-01(8.323e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.67e-04
Itr 000234 | Wall 1.227e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.87) | Loss 1.80(1.88) | FE 128(128) | Grad Norm 3.336e-01(8.174e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.77e-04
Itr 000235 | Wall 1.233e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.78(1.86) | Loss 1.80(1.88) | FE 128(128) | Grad Norm 2.784e-01(8.012e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 3.02e-04
Itr 000236 | Wall 1.238e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.79(1.86) | Loss 1.80(1.87) | FE 128(128) | Grad Norm 2.116e-01(7.835e-01) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 3.37e-04
Itr 000237 | Wall 1.244e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.86) | Loss 1.79(1.87) | FE 128(128) | Grad Norm 2.549e-01(7.677e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.68e-04
Itr 000238 | Wall 1.249e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.86) | Loss 1.79(1.87) | FE 128(128) | Grad Norm 2.721e-01(7.528e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.87e-04
Itr 000239 | Wall 1.254e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.78(1.86) | Loss 1.80(1.87) | FE 128(128) | Grad Norm 2.438e-01(7.375e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.47e-04
Itr 000240 | Wall 1.259e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.85) | Loss 1.80(1.86) | FE 128(128) | Grad Norm 2.818e-01(7.238e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.41e-04
Itr 000241 | Wall 1.264e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.78(1.85) | Loss 1.80(1.86) | FE 128(128) | Grad Norm 2.304e-01(7.090e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.64e-04
Itr 000242 | Wall 1.270e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.78(1.85) | Loss 1.80(1.86) | FE 128(128) | Grad Norm 2.046e-01(6.939e-01) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.54e-04
Itr 000243 | Wall 1.275e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.85) | Loss 1.79(1.86) | FE 128(128) | Grad Norm 3.000e-01(6.821e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.18e-04
Itr 000244 | Wall 1.280e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.85) | Loss 1.80(1.86) | FE 128(128) | Grad Norm 2.160e-01(6.681e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.47e-04
Itr 000245 | Wall 1.286e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.84) | Loss 1.79(1.85) | FE 128(128) | Grad Norm 2.146e-01(6.545e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.36e-04
Itr 000246 | Wall 1.291e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.84) | Loss 1.80(1.85) | FE 128(128) | Grad Norm 2.094e-01(6.412e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.29e-04
Itr 000247 | Wall 1.296e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.84) | Loss 1.80(1.85) | FE 128(128) | Grad Norm 1.945e-01(6.278e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.21e-04
Itr 000248 | Wall 1.301e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.78(1.84) | Loss 1.79(1.85) | FE 128(128) | Grad Norm 2.494e-01(6.164e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.12e-04
Itr 000249 | Wall 1.306e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.84) | Loss 1.79(1.85) | FE 128(128) | Grad Norm 2.360e-01(6.050e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.18e-04
Itr 000250 | Wall 1.311e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.83) | Loss 1.79(1.85) | FE 128(128) | Grad Norm 2.416e-01(5.941e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.37e-04
Itr 000251 | Wall 1.317e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.83) | Loss 1.79(1.84) | FE 128(128) | Grad Norm 1.836e-01(5.818e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.36e-04
Itr 000252 | Wall 1.322e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.79(1.83) | Loss 1.80(1.84) | FE 128(128) | Grad Norm 2.638e-01(5.722e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.16e-04
Itr 000253 | Wall 1.327e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.83) | Loss 1.79(1.84) | FE 128(128) | Grad Norm 2.217e-01(5.617e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.00e-04
Itr 000254 | Wall 1.332e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.83) | Loss 1.79(1.84) | FE 128(128) | Grad Norm 1.980e-01(5.508e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.11e-04
Itr 000255 | Wall 1.337e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.83) | Loss 1.79(1.84) | FE 128(128) | Grad Norm 2.657e-01(5.423e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.13e-04
Itr 000256 | Wall 1.343e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.83) | Loss 1.80(1.84) | FE 128(128) | Grad Norm 1.980e-01(5.319e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.24e-04
Itr 000257 | Wall 1.348e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.82) | Loss 1.79(1.84) | FE 128(128) | Grad Norm 1.756e-01(5.212e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 1.75e-04
Itr 000258 | Wall 1.353e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 2.226e-01(5.123e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.01e-04
Itr 000259 | Wall 1.358e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 1.480e-01(5.013e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.00e-04
Itr 000260 | Wall 1.363e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 2.152e-01(4.928e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.17e-04
Itr 000261 | Wall 1.368e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 1.879e-01(4.836e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.58e-04
Itr 000262 | Wall 1.374e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 2.185e-01(4.757e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 1.95e-04
Itr 000263 | Wall 1.379e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 2.103e-01(4.677e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.28e-04
Itr 000264 | Wall 1.384e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 1.953e-01(4.595e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.27e-04
Itr 000265 | Wall 1.389e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.82) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 2.135e-01(4.521e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.34e-04
Itr 000266 | Wall 1.395e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.81) | Loss 1.79(1.83) | FE 128(128) | Grad Norm 1.732e-01(4.438e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.34e-04
Itr 000267 | Wall 1.400e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.104e-01(4.368e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.06e-04
Itr 000268 | Wall 1.406e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.010e-01(4.297e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.10e-04
Itr 000269 | Wall 1.411e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.254e-01(4.236e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.29e-04
Itr 000270 | Wall 1.416e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.462e-01(4.183e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.28e-04
Itr 000271 | Wall 1.422e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.198e-01(4.123e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.92e-04
Itr 000272 | Wall 1.427e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.199e-01(4.065e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.48e-04
Itr 000273 | Wall 1.433e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 1.882e-01(4.000e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.48e-04
Itr 000274 | Wall 1.438e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 1.921e-01(3.937e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.45e-04
Itr 000275 | Wall 1.443e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.78(1.81) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 1.467e-01(3.863e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 1.98e-04
Itr 000276 | Wall 1.448e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.80) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 1.833e-01(3.802e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.78e-04
Itr 000277 | Wall 1.453e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.80) | Loss 1.79(1.82) | FE 128(128) | Grad Norm 2.527e-01(3.764e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.55e-04
Itr 000278 | Wall 1.458e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 3.236e-01(3.748e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.19e-04
Itr 000279 | Wall 1.463e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.883e-01(3.692e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.91e-04
Itr 000280 | Wall 1.469e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.585e-01(3.629e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.56e-04
Itr 000281 | Wall 1.474e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.304e-01(3.589e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.69e-04
Itr 000282 | Wall 1.480e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.924e-01(3.539e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.19e-04
Itr 000283 | Wall 1.485e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.289e-01(3.502e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.43e-04
Itr 000284 | Wall 1.490e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.799e-01(3.451e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.63e-04
Itr 000285 | Wall 1.495e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.745e-01(3.400e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.60e-04
Itr 000286 | Wall 1.501e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.287e-01(3.366e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.52e-04
Itr 000287 | Wall 1.506e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.430e-01(3.338e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.25e-04
Itr 000288 | Wall 1.512e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.860e-01(3.294e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.44e-04
Itr 000289 | Wall 1.517e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.096e-01(3.258e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.22e-04
Itr 000290 | Wall 1.522e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.554e-01(3.237e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.56e-04
Itr 000291 | Wall 1.528e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.80) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.923e-01(3.197e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.55e-04
Itr 000292 | Wall 1.533e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.79) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.689e-01(3.152e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.15e-04
Itr 000293 | Wall 1.538e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.78(1.79) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 1.860e-01(3.113e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.81e-04
Itr 000294 | Wall 1.543e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.78(1.79) | Loss 1.79(1.81) | FE 128(128) | Grad Norm 2.072e-01(3.082e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.40e-04
Itr 000295 | Wall 1.548e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.058e-01(3.051e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.78e-04
Itr 000296 | Wall 1.554e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.588e-01(3.007e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.63e-04
Itr 000297 | Wall 1.559e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.616e-01(2.966e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.06e-04
Itr 000298 | Wall 1.564e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.737e-01(2.929e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.71e-04
Itr 000299 | Wall 1.569e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.79) | Loss 1.78(1.80) | FE 128(128) | Grad Norm 2.721e-01(2.923e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.77e-04
Itr 000300 | Wall 1.574e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.865e-01(2.891e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.79e-04
Itr 000301 | Wall 1.579e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.780e-01(2.858e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.44e-04
Itr 000302 | Wall 1.584e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.954e-01(2.830e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.60e-04
Itr 000303 | Wall 1.590e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.975e-01(2.805e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.48e-04
Itr 000304 | Wall 1.595e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.810e-01(2.775e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.53e-04
Itr 000305 | Wall 1.601e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.485e-01(2.736e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.03e-04
Itr 000306 | Wall 1.606e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.588e-01(2.732e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.36e-04
Itr 000307 | Wall 1.611e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.165e-01(2.715e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.35e-04
Itr 000308 | Wall 1.616e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.457e-01(2.707e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.20e-04
Itr 000309 | Wall 1.621e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.878e-01(2.682e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.66e-04
Itr 000310 | Wall 1.627e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.577e-01(2.649e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.89e-04
Itr 000311 | Wall 1.632e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.554e-01(2.616e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.39e-04
Itr 000312 | Wall 1.637e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.79) | Loss 1.78(1.80) | FE 128(128) | Grad Norm 2.381e-01(2.609e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.76e-04
Itr 000313 | Wall 1.643e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.844e-01(2.586e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.07e-04
Itr 000314 | Wall 1.648e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.822e-01(2.563e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.38e-04
Itr 000315 | Wall 1.653e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.756e-01(2.539e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.34e-04
Itr 000316 | Wall 1.659e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.988e-01(2.523e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 4.05e-04
Itr 000317 | Wall 1.664e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.874e-01(2.503e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.01e-04
Itr 000318 | Wall 1.669e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.78(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.788e-01(2.482e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.88e-04
Itr 000319 | Wall 1.674e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.80) | FE 128(128) | Grad Norm 2.219e-01(2.474e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.37e-04
Itr 000320 | Wall 1.679e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.438e-01(2.443e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.80e-04
Itr 000321 | Wall 1.685e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 2.014e-01(2.430e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.78e-04
Itr 000322 | Wall 1.690e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.627e-01(2.406e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.61e-04
Itr 000323 | Wall 1.696e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.864e-01(2.419e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.68e-04
Itr 000324 | Wall 1.701e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.915e-01(2.404e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.67e-04
Itr 000325 | Wall 1.706e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.857e-01(2.388e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.71e-04
Itr 000326 | Wall 1.712e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.668e-01(2.366e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.53e-04
Itr 000327 | Wall 1.717e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.778e-01(2.349e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.54e-04
Itr 000328 | Wall 1.722e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 2.110e-01(2.341e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 4.41e-04
Itr 000329 | Wall 1.728e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 2.187e-01(2.337e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.50e-04
Itr 000330 | Wall 1.733e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.516e-01(2.312e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.25e-04
Itr 000331 | Wall 1.738e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.928e-01(2.301e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.54e-04
Itr 000332 | Wall 1.743e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.735e-01(2.284e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.78e-04
Itr 000333 | Wall 1.749e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.761e-01(2.268e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.41e-04
Itr 000334 | Wall 1.754e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.361e-01(2.241e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.85e-04
Itr 000335 | Wall 1.759e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.985e-01(2.233e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.18e-04
Itr 000336 | Wall 1.765e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.105e-01(2.229e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.17e-04
Itr 000337 | Wall 1.770e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.541e-01(2.239e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.05e-04
Itr 000338 | Wall 1.775e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.285e-01(2.240e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.66e-04
Itr 000339 | Wall 1.780e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.619e-01(2.221e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.02e-04
Itr 000340 | Wall 1.785e+02(0.52) | Time/Itr 0.48(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.029e-01(2.216e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.74e-04
Itr 000341 | Wall 1.790e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.387e-01(2.221e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.77e-04
Itr 000342 | Wall 1.795e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.717e-01(2.206e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.48e-04
Itr 000343 | Wall 1.801e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.538e-01(2.246e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.99e-04
Itr 000344 | Wall 1.806e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.089e-01(2.271e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.30e-04
Itr 000345 | Wall 1.811e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.083e-01(2.265e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.38e-04
Itr 000346 | Wall 1.816e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 4.793e-01(2.341e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 3.76e-04
Itr 000347 | Wall 1.822e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.814e-01(2.325e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.70e-04
Itr 000348 | Wall 1.827e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.477e-01(2.330e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.73e-04
Itr 000349 | Wall 1.832e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.124e-01(2.354e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.53e-04
Itr 000350 | Wall 1.837e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.340e-01(2.353e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.44e-04
Itr 000351 | Wall 1.843e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.975e-01(2.342e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.65e-04
Itr 000352 | Wall 1.848e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.432e-01(2.345e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 3.94e-04
Itr 000353 | Wall 1.853e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.967e-01(2.333e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.51e-04
Itr 000354 | Wall 1.858e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.516e-01(2.339e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.65e-04
Itr 000355 | Wall 1.864e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.116e-01(2.362e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.75e-04
Itr 000356 | Wall 1.869e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.850e-01(2.347e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.53e-04
Itr 000357 | Wall 1.875e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.294e-01(2.375e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.02e-04
Itr 000358 | Wall 1.880e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.176e-01(2.369e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.10e-04
Itr 000359 | Wall 1.885e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.320e-01(2.368e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.28e-04
Itr 000360 | Wall 1.890e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.997e-01(2.387e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.09e-04
Itr 000361 | Wall 1.895e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.473e-01(2.389e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.64e-04
Itr 000362 | Wall 1.901e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.420e-01(2.390e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.24e-04
Itr 000363 | Wall 1.906e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.309e-01(2.388e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.32e-04
Itr 000364 | Wall 1.912e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.770e-01(2.369e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.45e-04
Itr 000365 | Wall 1.917e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.788e-01(2.382e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.50e-04
Itr 000366 | Wall 1.922e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.664e-01(2.390e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.75e-04
Itr 000367 | Wall 1.927e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.163e-01(2.383e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.70e-04
Itr 000368 | Wall 1.933e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.691e-01(2.363e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.96e-04
Itr 000369 | Wall 1.938e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.731e-01(2.374e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 6.51e-04
Itr 000370 | Wall 1.943e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 2.268e-01(2.371e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.68e-04
Itr 000371 | Wall 1.948e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.991e-01(2.359e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.48e-04
Itr 000372 | Wall 1.953e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 3.980e-01(2.408e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.01e-04
Itr 000373 | Wall 1.958e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.412e-01(2.438e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.13e-04
Itr 000374 | Wall 1.963e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.185e-01(2.460e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 6.69e-04
Itr 000375 | Wall 1.969e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 5.517e-01(2.552e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.57e-04
Itr 000376 | Wall 1.974e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.174e-01(2.541e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.92e-04
Itr 000377 | Wall 1.979e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 5.172e-01(2.620e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 4.72e-04
Itr 000378 | Wall 1.984e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.037e-01(2.632e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.16e-04
Itr 000379 | Wall 1.989e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.960e-01(2.612e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.81e-04
Itr 000380 | Wall 1.994e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 4.020e-01(2.654e-01) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 5.28e-04
Itr 000381 | Wall 2.000e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.261e-01(2.642e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.69e-04
Itr 000382 | Wall 2.005e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.191e-01(2.629e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.98e-04
Itr 000383 | Wall 2.010e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.209e-01(2.646e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.63e-04
Itr 000384 | Wall 2.015e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.740e-01(2.649e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.87e-04
Itr 000385 | Wall 2.020e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.849e-01(2.625e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.38e-04
Itr 000386 | Wall 2.026e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.560e-01(2.653e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 6.06e-04
Itr 000387 | Wall 2.031e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.756e-01(2.626e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 4.62e-04
Itr 000388 | Wall 2.036e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.210e-01(2.644e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.28e-04
Itr 000389 | Wall 2.041e+02(0.52) | Time/Itr 0.57(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.441e-01(2.608e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.91e-04
Itr 000390 | Wall 2.047e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.836e-01(2.585e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.29e-04
Itr 000391 | Wall 2.052e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.096e-01(2.600e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.64e-04
Itr 000392 | Wall 2.057e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.744e-01(2.574e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.83e-04
Itr 000393 | Wall 2.062e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.697e-01(2.608e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.46e-04
Itr 000394 | Wall 2.067e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.962e-01(2.588e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.67e-04
Itr 000395 | Wall 2.073e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.856e-01(2.567e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.63e-04
Itr 000396 | Wall 2.078e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.199e-01(2.555e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.10e-04
Itr 000397 | Wall 2.083e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.052e-01(2.540e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.13e-04
Itr 000398 | Wall 2.088e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.818e-01(2.519e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.48e-04
Itr 000399 | Wall 2.093e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.720e-01(2.495e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.63e-04
Itr 000400 | Wall 2.098e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.304e-01(2.489e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.55e-04
Itr 000401 | Wall 2.104e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.606e-01(2.493e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.29e-04
Itr 000402 | Wall 2.109e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.543e-01(2.494e-01) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.78e-04
Itr 000403 | Wall 2.115e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.022e-01(2.480e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.84e-04
Itr 000404 | Wall 2.120e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.265e-01(2.473e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.91e-04
Itr 000405 | Wall 2.125e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.010e-01(2.460e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.83e-04
Itr 000406 | Wall 2.131e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.466e-01(2.430e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.98e-04
Itr 000407 | Wall 2.136e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.676e-01(2.407e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.79e-04
Itr 000408 | Wall 2.141e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.632e-01(2.384e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 8.00e-04
Itr 000409 | Wall 2.146e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.942e-01(2.371e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.31e-04
Itr 000410 | Wall 2.151e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.867e-01(2.356e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.14e-04
Itr 000411 | Wall 2.157e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.045e-01(2.346e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.59e-04
Itr 000412 | Wall 2.162e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.151e-01(2.340e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 8.27e-04
Itr 000413 | Wall 2.167e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.612e-01(2.319e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.03e-04
Itr 000414 | Wall 2.172e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.210e-01(2.315e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 7.11e-04
Itr 000415 | Wall 2.178e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.897e-01(2.303e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.03e-04
Itr 000416 | Wall 2.183e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.989e-01(2.293e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 6.30e-04
Itr 000417 | Wall 2.188e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.928e-01(2.282e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 6.63e-04
Itr 000418 | Wall 2.193e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.684e-01(2.264e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.91e-04
Itr 000419 | Wall 2.198e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.493e-01(2.271e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 7.59e-04
Itr 000420 | Wall 2.204e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.426e-01(2.276e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.16e-04
Itr 000421 | Wall 2.210e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.519e-01(2.253e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 5.06e-04
Itr 000422 | Wall 2.215e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.128e-01(2.249e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.70e-04
Itr 000423 | Wall 2.219e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.679e-01(2.232e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 8.77e-04
Itr 000424 | Wall 2.224e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.307e-01(2.235e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 9.65e-04
Itr 000425 | Wall 2.230e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.208e-01(2.234e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.43e-04
Itr 000426 | Wall 2.235e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.500e-01(2.242e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.08e-03
Itr 000427 | Wall 2.240e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.236e-01(2.242e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 7.74e-04
Itr 000428 | Wall 2.246e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.500e-01(2.219e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 6.25e-04
Itr 000429 | Wall 2.251e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.545e-01(2.199e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.21e-04
Itr 000430 | Wall 2.256e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.870e-01(2.189e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.71e-04
Itr 000431 | Wall 2.261e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.667e-01(2.204e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.46e-04
Itr 000432 | Wall 2.266e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.571e-01(2.215e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.01e-03
Itr 000433 | Wall 2.271e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.944e-01(2.206e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.65e-04
Itr 000434 | Wall 2.276e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.680e-01(2.191e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.47e-04
Itr 000435 | Wall 2.281e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.000e-01(2.185e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.69e-04
Itr 000436 | Wall 2.286e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.770e-01(2.173e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.10e-04
Itr 000437 | Wall 2.292e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.632e-01(2.156e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.67e-04
Itr 000438 | Wall 2.297e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.827e-01(2.146e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.06e-04
Itr 000439 | Wall 2.302e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.809e-01(2.136e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.81e-04
Itr 000440 | Wall 2.307e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.498e-01(2.117e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.86e-04
Itr 000441 | Wall 2.313e+02(0.52) | Time/Itr 0.59(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.765e-01(2.137e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.35e-04
Itr 000442 | Wall 2.318e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 3.001e-01(2.162e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.23e-03
Itr 000443 | Wall 2.323e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.442e-01(2.171e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.17e-03
Itr 000444 | Wall 2.328e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.931e-01(2.164e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.84e-04
Itr 000445 | Wall 2.333e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.104e-01(2.192e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.00e-03
Itr 000446 | Wall 2.339e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.139e-01(2.190e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.40e-03
Itr 000447 | Wall 2.344e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 5.435e-01(2.288e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.14e-03
Itr 000448 | Wall 2.349e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 4.350e-01(2.350e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 9.13e-04
Itr 000449 | Wall 2.354e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.806e-01(2.333e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 9.63e-04
Itr 000450 | Wall 2.359e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.158e-01(2.328e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.19e-03
Itr 000451 | Wall 2.365e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.725e-01(2.310e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.28e-03
Itr 000452 | Wall 2.370e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.046e-01(2.302e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.03e-03
Itr 000453 | Wall 2.375e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.903e-01(2.350e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.49e-04
Itr 000454 | Wall 2.380e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 4.055e-01(2.401e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.03e-03
Itr 000455 | Wall 2.385e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.971e-01(2.388e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.00e-03
Itr 000456 | Wall 2.391e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.253e-01(2.384e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.07e-03
Itr 000457 | Wall 2.396e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.040e-01(2.374e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.09e-03
Itr 000458 | Wall 2.401e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.178e-01(2.398e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.28e-03
Itr 000459 | Wall 2.406e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.719e-01(2.438e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.34e-03
Itr 000460 | Wall 2.411e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.514e-01(2.440e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.20e-03
Itr 000461 | Wall 2.416e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 6.322e-01(2.556e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 9.77e-04
Itr 000462 | Wall 2.421e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 4.376e-01(2.611e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.21e-03
Itr 000463 | Wall 2.426e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.291e-01(2.601e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.09e-03
Itr 000464 | Wall 2.431e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 5.086e-01(2.676e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.27e-03
Itr 000465 | Wall 2.436e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 5.357e-01(2.756e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.24e-03
Itr 000466 | Wall 2.442e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.824e-01(2.728e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.23e-03
Itr 000467 | Wall 2.447e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 6.659e-01(2.846e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.35e-03
Itr 000468 | Wall 2.452e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 6.464e-01(2.955e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.16e-03
Itr 000469 | Wall 2.457e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.468e-01(2.940e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 9.51e-04
Itr 000470 | Wall 2.462e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.687e-01(2.933e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.10e-03
Itr 000471 | Wall 2.467e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.190e-01(2.910e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.51e-03
Itr 000472 | Wall 2.473e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.383e-01(2.895e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.25e-03
Itr 000473 | Wall 2.478e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.804e-01(2.892e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.09e-03
Itr 000474 | Wall 2.483e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.696e-01(2.886e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.23e-03
Itr 000475 | Wall 2.489e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.743e-01(2.852e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.73e-03
Itr 000476 | Wall 2.494e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.995e-01(2.856e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.34e-03
Itr 000477 | Wall 2.499e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.441e-01(2.874e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.12e-03
Itr 000478 | Wall 2.504e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.901e-01(2.874e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.14e-03
Itr 000479 | Wall 2.509e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.738e-01(2.870e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.39e-03
Itr 000480 | Wall 2.515e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 2.232e-01(2.851e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.52e-03
Itr 000481 | Wall 2.520e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.204e-01(2.862e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.27e-03
Itr 000482 | Wall 2.525e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.730e-01(2.828e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.20e-03
Itr 000483 | Wall 2.530e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 5.074e-01(2.895e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.54e-03
Itr 000484 | Wall 2.536e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.115e-01(3.022e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.53e-03
Itr 000485 | Wall 2.541e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 6.920e-01(3.139e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.06e-03
Itr 000486 | Wall 2.546e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.512e-01(3.150e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.56e-03
Itr 000487 | Wall 2.552e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.779e-01(3.109e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.45e-03
Itr 000488 | Wall 2.557e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 4.716e-01(3.157e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.66e-03
Itr 000489 | Wall 2.562e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 3.205e-01(3.158e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.31e-03
Itr 000490 | Wall 2.567e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 3.445e-01(3.167e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.46e-03
Itr 000491 | Wall 2.573e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 8.666e-01(3.332e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.83e-03
Itr 000492 | Wall 2.578e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.317e+00(3.627e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.58e-03
Itr 000493 | Wall 2.583e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 9.114e-01(3.792e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.83e-03
Itr 000494 | Wall 2.589e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.968e-01(3.737e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.43e-03
Itr 000495 | Wall 2.594e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 6.931e-01(3.833e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.30e-03
Itr 000496 | Wall 2.599e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 5.173e-01(3.873e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.53e-03
Itr 000497 | Wall 2.604e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 3.485e-01(3.861e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.79e-03
Itr 000498 | Wall 2.610e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 9.141e-01(4.020e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.77e-03
Itr 000499 | Wall 2.615e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.120e+00(4.235e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.64e-03
Itr 000500 | Wall 2.620e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.944e-01(4.346e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.61e-03
Itr 000501 | Wall 2.625e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.938e-01(4.274e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.31e-03
Itr 000502 | Wall 2.631e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.061e+00(4.464e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.14e-03
Itr 000503 | Wall 2.636e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.479e+00(4.774e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.58e-03
Itr 000504 | Wall 2.642e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 6.236e-01(4.818e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.98e-03
Itr 000505 | Wall 2.647e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.016e+00(4.978e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.62e-03
Itr 000506 | Wall 2.652e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.630e+00(5.318e-01) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.40e-03
Itr 000507 | Wall 2.657e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 4.952e-01(5.307e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.81e-03
Itr 000508 | Wall 2.663e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.091e+00(5.475e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.65e-03
Itr 000509 | Wall 2.668e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.454e+00(5.747e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.53e-03
Itr 000510 | Wall 2.673e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 5.480e-01(5.739e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.33e-03
Itr 000511 | Wall 2.678e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 6.694e-01(5.768e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.95e-03
Itr 000512 | Wall 2.683e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 8.158e-01(5.839e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.49e-03
Itr 000513 | Wall 2.689e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.690e-01(5.745e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.83e-03
Itr 000514 | Wall 2.694e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 1.307e+00(5.964e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.08e-03
Itr 000515 | Wall 2.700e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.327e+00(6.184e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.94e-03
Itr 000516 | Wall 2.705e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.360e-01(6.159e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.59e-03
Itr 000517 | Wall 2.710e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.628e+00(6.762e-01) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.48e-03
Itr 000518 | Wall 2.715e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.445e+00(7.293e-01) | TT 4.00(4.00) | kinetic_energy: 9.86e-01 | jacobian_norm2: 1.43e-03
Itr 000519 | Wall 2.720e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.036e+00(7.385e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.76e-03
Itr 000520 | Wall 2.725e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 3.371e+00(8.175e-01) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 1.92e-03
Itr 000521 | Wall 2.731e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.457e+00(8.367e-01) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.65e-03
Itr 000522 | Wall 2.736e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.139e+00(8.757e-01) | TT 4.00(4.00) | kinetic_energy: 9.85e-01 | jacobian_norm2: 1.23e-03
Itr 000523 | Wall 2.742e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.282e+00(8.879e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.58e-03
Itr 000524 | Wall 2.747e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.405e+00(9.334e-01) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 2.12e-03
Itr 000525 | Wall 2.753e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.820e+00(9.600e-01) | TT 4.00(4.00) | kinetic_energy: 9.91e-01 | jacobian_norm2: 1.58e-03
Itr 000526 | Wall 2.758e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.792e+00(9.850e-01) | TT 4.00(4.00) | kinetic_energy: 9.89e-01 | jacobian_norm2: 1.21e-03
Itr 000527 | Wall 2.763e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.286e+00(9.940e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.60e-03
Itr 000528 | Wall 2.768e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.090e+00(1.027e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.45e-03
Itr 000529 | Wall 2.773e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.635e+00(1.045e+00) | TT 4.00(4.00) | kinetic_energy: 9.91e-01 | jacobian_norm2: 1.44e-03
Itr 000530 | Wall 2.779e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.696e+00(1.065e+00) | TT 4.00(4.00) | kinetic_energy: 9.88e-01 | jacobian_norm2: 1.78e-03
Itr 000531 | Wall 2.784e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.145e+00(1.067e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.64e-03
Itr 000532 | Wall 2.789e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.473e+00(1.079e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.61e-03
Itr 000533 | Wall 2.794e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 8.152e-01(1.071e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.44e-03
Itr 000534 | Wall 2.799e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 1.128e+00(1.073e+00) | TT 4.00(4.00) | kinetic_energy: 1.00e+00 | jacobian_norm2: 1.42e-03
Itr 000535 | Wall 2.804e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 4.204e-01(1.053e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.43e-03
Itr 000536 | Wall 2.810e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.472e+00(1.066e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.99e-03
Itr 000537 | Wall 2.815e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 4.555e-01(1.048e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.70e-03
Itr 000538 | Wall 2.820e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 9.397e-01(1.044e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.76e-03
Itr 000539 | Wall 2.826e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.140e-01(1.034e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.41e-03
Itr 000540 | Wall 2.831e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.77(1.76) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 9.722e-01(1.033e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.91e-03
Itr 000541 | Wall 2.836e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.385e-01(1.024e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.06e-03
Itr 000542 | Wall 2.842e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.384e-01(1.015e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.37e-03
Itr 000543 | Wall 2.847e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.496e-01(1.007e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.85e-03
Itr 000544 | Wall 2.852e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.986e-01(9.890e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.67e-03
Itr 000545 | Wall 2.857e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.014e-01(9.804e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.67e-03
Itr 000546 | Wall 2.862e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.654e-01(9.559e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.00e-03
Itr 000547 | Wall 2.868e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.643e-01(9.472e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.78e-03
Itr 000548 | Wall 2.873e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.480e-01(9.262e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.84e-03
Itr 000549 | Wall 2.878e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.684e-01(9.155e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.27e-03
Itr 000550 | Wall 2.883e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.505e-01(8.985e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.95e-03
Itr 000551 | Wall 2.888e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.294e-01(8.874e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.01e-03
Itr 000552 | Wall 2.894e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.628e-01(8.687e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.82e-03
Itr 000553 | Wall 2.899e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.082e-01(8.579e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.12e-03
Itr 000554 | Wall 2.904e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.876e-01(8.408e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.02e-03
Itr 000555 | Wall 2.909e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.615e-01(8.264e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.13e-03
Itr 000556 | Wall 2.914e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.979e-01(8.105e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.93e-03
Itr 000557 | Wall 2.919e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.431e-01(7.995e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.04e-03
Itr 000558 | Wall 2.924e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.971e-01(7.904e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.20e-03
Itr 000559 | Wall 2.930e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.060e-01(7.729e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.79e-03
Itr 000560 | Wall 2.935e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.060e-01(7.619e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.20e-03
Itr 000561 | Wall 2.940e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.587e-01(7.438e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.77e-03
Itr 000562 | Wall 2.945e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.985e-01(7.334e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.02e-03
Itr 000563 | Wall 2.950e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.135e-01(7.209e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.54e-03
Itr 000564 | Wall 2.955e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.038e-01(7.083e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.08e-03
Itr 000565 | Wall 2.960e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.253e-01(6.968e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.30e-03
Itr 000566 | Wall 2.966e+02(0.52) | Time/Itr 0.58(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.911e-01(6.817e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.25e-03
Itr 000567 | Wall 2.971e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.123e-01(6.766e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.56e-03
Itr 000568 | Wall 2.976e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.565e-01(6.700e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.15e-03
Itr 000569 | Wall 2.981e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.808e-01(6.613e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.64e-03
Itr 000570 | Wall 2.986e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.149e+00(6.760e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.89e-03
Itr 000571 | Wall 2.992e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.043e+00(6.870e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.28e-03
Itr 000572 | Wall 2.997e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.056e-01(6.815e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.65e-03
Itr 000573 | Wall 3.003e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.368e+00(7.021e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 3.06e-03
Itr 000574 | Wall 3.008e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.459e+00(7.248e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.29e-03
Itr 000575 | Wall 3.013e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.475e-01(7.225e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.51e-03
Itr 000576 | Wall 3.018e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.433e+00(7.438e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.71e-03
Itr 000577 | Wall 3.023e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.384e+00(7.630e-01) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.28e-03
Itr 000578 | Wall 3.029e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.045e+00(7.715e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.40e-03
Itr 000579 | Wall 3.034e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.741e+00(8.006e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.39e-03
Itr 000580 | Wall 3.039e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.073e+00(8.087e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.88e-03
Itr 000581 | Wall 3.044e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.068e+00(8.165e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.91e-03
Itr 000582 | Wall 3.049e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.303e+00(8.611e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.64e-03
Itr 000583 | Wall 3.055e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.879e+00(8.917e-01) | TT 4.00(4.00) | kinetic_energy: 9.98e-01 | jacobian_norm2: 2.60e-03
Itr 000584 | Wall 3.060e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.76(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 1.564e+00(9.118e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.02e-03
Itr 000585 | Wall 3.065e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.596e+00(9.624e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.13e-03
Itr 000586 | Wall 3.071e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.870e+00(1.020e+00) | TT 4.00(4.00) | kinetic_energy: 9.65e-01 | jacobian_norm2: 1.73e-03
Itr 000587 | Wall 3.076e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.381e-01(1.011e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.13e-03
Itr 000588 | Wall 3.081e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 3.002e+00(1.071e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.86e-03
Itr 000589 | Wall 3.088e+02(0.52) | Time/Itr 0.64(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.289e+00(1.077e+00) | TT 4.00(4.00) | kinetic_energy: 9.97e-01 | jacobian_norm2: 1.82e-03
Itr 000590 | Wall 3.093e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 1.944e+00(1.103e+00) | TT 4.00(4.00) | kinetic_energy: 9.78e-01 | jacobian_norm2: 2.16e-03
Itr 000591 | Wall 3.098e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.769e+00(1.123e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.81e-03
Itr 000592 | Wall 3.104e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.070e+00(1.122e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.56e-03
Itr 000593 | Wall 3.109e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.241e+00(1.125e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.36e-03
Itr 000594 | Wall 3.114e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.735e-01(1.118e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.47e-03
Itr 000595 | Wall 3.120e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.176e+00(1.120e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.68e-03
Itr 000596 | Wall 3.125e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.252e-01(1.105e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.47e-03
Itr 000597 | Wall 3.130e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.058e+00(1.103e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.69e-03
Itr 000598 | Wall 3.136e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 5.466e-01(1.087e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.69e-03
Itr 000599 | Wall 3.141e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.855e-01(1.081e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.53e-03
Itr 000600 | Wall 3.146e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.692e-01(1.071e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.67e-03
Itr 000601 | Wall 3.151e+02(0.52) | Time/Itr 0.48(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.113e-01(1.057e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.32e-03
Itr 000602 | Wall 3.156e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.519e+00(1.071e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.82e-03
Itr 000603 | Wall 3.162e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.176e-01(1.064e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.50e-03
Itr 000604 | Wall 3.167e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.559e-01(1.057e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 2.37e-03
Itr 000605 | Wall 3.172e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.002e+00(1.056e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.71e-03
Itr 000606 | Wall 3.177e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.264e-01(1.049e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.06e-03
Itr 000607 | Wall 3.182e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 9.215e-01(1.045e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.89e-03
Itr 000608 | Wall 3.188e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.667e-01(1.037e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.50e-03
Itr 000609 | Wall 3.193e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.025e+00(1.036e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.77e-03
Itr 000610 | Wall 3.198e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 5.428e-01(1.022e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.20e-03
Itr 000611 | Wall 3.203e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.279e-01(1.016e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.51e-03
Itr 000612 | Wall 3.208e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.176e-01(1.001e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.38e-03
Itr 000613 | Wall 3.213e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 6.891e-01(9.915e-01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.88e-03
Itr 000614 | Wall 3.219e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 5.582e-01(9.785e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.99e-03
Itr 000615 | Wall 3.224e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.920e-01(9.759e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.72e-03
Itr 000616 | Wall 3.229e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.056e-01(9.588e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.91e-03
Itr 000617 | Wall 3.235e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.336e-01(9.520e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.93e-03
Itr 000618 | Wall 3.240e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.200e-01(9.451e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.85e-03
Itr 000619 | Wall 3.245e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.055e-01(9.379e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.57e-03
Itr 000620 | Wall 3.250e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.352e-01(9.288e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.35e-03
Itr 000621 | Wall 3.256e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 9.589e-01(9.297e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.81e-03
Itr 000622 | Wall 3.261e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 9.897e-01(9.315e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.90e-03
Itr 000623 | Wall 3.267e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.140e+00(9.377e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 4.40e-03
Itr 000624 | Wall 3.272e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.066e+00(9.416e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.50e-03
Itr 000625 | Wall 3.277e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.406e+00(9.555e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.49e-03
Itr 000626 | Wall 3.283e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.310e+00(9.662e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 3.58e-03
Itr 000627 | Wall 3.288e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.013e+00(9.676e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.64e-03
Itr 000628 | Wall 3.293e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 7.479e-01(9.610e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.69e-03
Itr 000629 | Wall 3.299e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.079e+00(9.645e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 4.07e-03
Itr 000630 | Wall 3.304e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 8.508e-01(9.611e-01) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.45e-03
Itr 000631 | Wall 3.309e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.302e-01(9.482e-01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.31e-03
Itr 000632 | Wall 3.314e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 4.194e-01(9.323e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.79e-03
Itr 000633 | Wall 3.320e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.611e-01(9.272e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.77e-03
Itr 000634 | Wall 3.325e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.738e-01(9.256e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.81e-03
Itr 000635 | Wall 3.330e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 5.377e-01(9.139e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.94e-03
Itr 000636 | Wall 3.335e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.151e+00(9.211e-01) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.11e-03
Itr 000637 | Wall 3.340e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.825e+00(9.482e-01) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 2.94e-03
Itr 000638 | Wall 3.345e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.589e+00(9.974e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 4.64e-03
Itr 000639 | Wall 3.351e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.78(1.76) | Loss 1.79(1.77) | FE 128(128) | Grad Norm 3.477e+00(1.072e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 4.94e-03
Itr 000640 | Wall 3.356e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.148e+00(1.104e+00) | TT 4.00(4.00) | kinetic_energy: 9.95e-01 | jacobian_norm2: 3.77e-03
Itr 000641 | Wall 3.361e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.79(1.76) | Loss 1.80(1.77) | FE 128(128) | Grad Norm 4.828e+00(1.216e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 5.69e-03
Itr 000642 | Wall 3.367e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.78(1.76) | Loss 1.79(1.77) | FE 128(128) | Grad Norm 2.957e+00(1.268e+00) | TT 4.00(4.00) | kinetic_energy: 9.78e-01 | jacobian_norm2: 4.14e-03
Itr 000643 | Wall 3.372e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.78(1.76) | Loss 1.79(1.77) | FE 128(128) | Grad Norm 2.379e+00(1.301e+00) | TT 4.00(4.00) | kinetic_energy: 9.66e-01 | jacobian_norm2: 4.29e-03
Itr 000644 | Wall 3.377e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 1.773e+00(1.315e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.24e-03
Itr 000645 | Wall 3.382e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.78(1.76) | Loss 1.79(1.77) | FE 128(128) | Grad Norm 2.443e+00(1.349e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 4.06e-03
Itr 000646 | Wall 3.387e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.731e+00(1.361e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 4.11e-03
Itr 000647 | Wall 3.393e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.239e+00(1.387e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.84e-03
Itr 000648 | Wall 3.398e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.930e+00(1.403e+00) | TT 4.00(4.00) | kinetic_energy: 9.94e-01 | jacobian_norm2: 4.81e-03
Itr 000649 | Wall 3.403e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.912e+00(1.419e+00) | TT 4.00(4.00) | kinetic_energy: 9.77e-01 | jacobian_norm2: 3.26e-03
Itr 000650 | Wall 3.409e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 7.425e-01(1.398e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 4.39e-03
Itr 000651 | Wall 3.414e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.386e+00(1.428e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.10e-03
Itr 000652 | Wall 3.419e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 2.124e+00(1.449e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 5.32e-03
Itr 000653 | Wall 3.424e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.292e+00(1.444e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 5.07e-03
Itr 000654 | Wall 3.430e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.583e+00(1.448e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 4.37e-03
Itr 000655 | Wall 3.435e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.77(1.76) | Loss 1.78(1.77) | FE 128(128) | Grad Norm 2.416e+00(1.477e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 4.31e-03
Itr 000656 | Wall 3.440e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 6.224e-01(1.452e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 4.03e-03
Itr 000657 | Wall 3.446e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.807e+00(1.462e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 3.87e-03
Itr 000658 | Wall 3.451e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.815e+00(1.473e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 4.42e-03
Itr 000659 | Wall 3.456e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.897e+00(1.486e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.24e-03
Itr 000660 | Wall 3.462e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.890e+00(1.498e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 5.65e-03
Itr 000661 | Wall 3.467e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.441e+00(1.496e+00) | TT 4.00(4.00) | kinetic_energy: 9.89e-01 | jacobian_norm2: 7.37e-03
Itr 000662 | Wall 3.473e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.840e+00(1.506e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 6.43e-03
Itr 000663 | Wall 3.478e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.693e+00(1.512e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 5.68e-03
Itr 000664 | Wall 3.483e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.777e+00(1.520e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 5.46e-03
Itr 000665 | Wall 3.489e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.875e+00(1.531e+00) | TT 4.00(4.00) | kinetic_energy: 9.97e-01 | jacobian_norm2: 4.61e-03
Itr 000666 | Wall 3.494e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.286e+00(1.523e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.81e-03
Itr 000667 | Wall 3.499e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 9.825e-01(1.507e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 6.72e-03
Itr 000668 | Wall 3.504e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.336e+00(1.502e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 5.55e-03
Itr 000669 | Wall 3.510e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 1.389e+00(1.499e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 6.17e-03
Itr 000670 | Wall 3.515e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.053e+00(1.485e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 5.95e-03
Itr 000671 | Wall 3.520e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 7.290e-01(1.462e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 8.30e-03
Itr 000672 | Wall 3.526e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.229e+00(1.455e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 7.08e-03
Itr 000673 | Wall 3.531e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.156e+00(1.446e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 7.49e-03
Itr 000674 | Wall 3.537e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.031e+00(1.434e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 6.82e-03
Itr 000675 | Wall 3.542e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.76) | Loss 1.77(1.77) | FE 128(128) | Grad Norm 5.565e-01(1.408e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.44e-03
Itr 000676 | Wall 3.547e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.110e+00(1.399e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 6.21e-03
Itr 000677 | Wall 3.552e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 5.627e-01(1.374e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 6.67e-03
Itr 000678 | Wall 3.557e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.573e-01(1.358e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 9.91e-03
Itr 000679 | Wall 3.563e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.040e-01(1.342e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 7.34e-03
Itr 000680 | Wall 3.568e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.345e-01(1.326e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.53e-03
Itr 000681 | Wall 3.573e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.868e-01(1.313e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 7.04e-03
Itr 000682 | Wall 3.578e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 7.293e-01(1.296e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 7.86e-03
Itr 000683 | Wall 3.584e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 9.977e-01(1.287e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 8.72e-03
Itr 000684 | Wall 3.589e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 6.253e-01(1.267e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 9.40e-03
Itr 000685 | Wall 3.594e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.003e+00(1.259e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.87e-03
Itr 000686 | Wall 3.600e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.164e+00(1.256e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.35e-03
Itr 000687 | Wall 3.605e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 4.795e-01(1.233e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 7.65e-03
Itr 000688 | Wall 3.610e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 9.023e-01(1.223e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 8.30e-03
Itr 000689 | Wall 3.615e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 8.808e-01(1.213e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.97e-03
Itr 000690 | Wall 3.620e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.025e+00(1.207e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 7.57e-03
Itr 000691 | Wall 3.626e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.657e+00(1.220e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 9.03e-03
Itr 000692 | Wall 3.631e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.76) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.292e+00(1.223e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 7.48e-03
Itr 000693 | Wall 3.636e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.75) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.175e+00(1.221e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 6.39e-03
Itr 000694 | Wall 3.641e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.75(1.75) | Loss 1.76(1.77) | FE 128(128) | Grad Norm 1.167e+00(1.220e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 8.86e-03
Itr 000695 | Wall 3.647e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 6.916e-01(1.204e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 7.55e-03
Itr 000696 | Wall 3.651e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 9.517e-01(1.196e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 7.58e-03
Itr 000697 | Wall 3.657e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 9.102e-01(1.188e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.04e-02
Itr 000698 | Wall 3.662e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 6.923e-01(1.173e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 8.04e-03
Itr 000699 | Wall 3.667e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 7.642e-01(1.161e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 9.81e-03
Itr 000700 | Wall 3.672e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 7.304e-01(1.148e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 8.59e-03
Itr 000701 | Wall 3.678e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 6.015e-01(1.131e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 9.46e-03
Itr 000702 | Wall 3.683e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 8.434e-01(1.123e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 9.32e-03
Itr 000703 | Wall 3.688e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.822e+00(1.144e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 9.09e-03
Itr 000704 | Wall 3.693e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 2.478e+00(1.184e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 9.36e-03
Itr 000705 | Wall 3.699e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 2.645e+00(1.227e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 9.73e-03
Itr 000706 | Wall 3.704e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 2.941e+00(1.279e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.00e-02
Itr 000707 | Wall 3.710e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.185e+00(1.276e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.06e-02
Itr 000708 | Wall 3.715e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 2.624e+00(1.316e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.18e-02
Itr 000709 | Wall 3.720e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.914e+00(1.334e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.06e-02
Itr 000710 | Wall 3.725e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.77(1.75) | Loss 1.78(1.76) | FE 128(128) | Grad Norm 2.343e+00(1.365e+00) | TT 4.00(4.00) | kinetic_energy: 9.96e-01 | jacobian_norm2: 9.65e-03
Itr 000711 | Wall 3.731e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 5.598e-01(1.340e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.22e-02
Itr 000712 | Wall 3.736e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 2.171e+00(1.365e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.04e-02
Itr 000713 | Wall 3.741e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.771e+00(1.378e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 9.66e-03
Itr 000714 | Wall 3.746e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 2.175e+00(1.402e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.57e-03
Itr 000715 | Wall 3.751e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.77(1.75) | Loss 1.78(1.76) | FE 128(128) | Grad Norm 3.351e+00(1.460e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 8.83e-03
Itr 000716 | Wall 3.757e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 3.148e+00(1.511e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.09e-02
Itr 000717 | Wall 3.762e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 2.623e+00(1.544e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.11e-02
Itr 000718 | Wall 3.767e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.884e+00(1.554e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.02e-02
Itr 000719 | Wall 3.772e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 2.975e+00(1.597e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.10e-02
Itr 000720 | Wall 3.777e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.76(1.75) | Loss 1.77(1.76) | FE 128(128) | Grad Norm 1.806e+00(1.603e+00) | TT 4.00(4.00) | kinetic_energy: 9.94e-01 | jacobian_norm2: 8.72e-03
Itr 000721 | Wall 3.782e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 2.409e+00(1.627e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.05e-02
Itr 000722 | Wall 3.787e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.720e+00(1.630e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.19e-02
Itr 000723 | Wall 3.793e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 2.205e+00(1.647e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.02e-02
Itr 000724 | Wall 3.798e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.409e+00(1.640e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.01e-02
Itr 000725 | Wall 3.803e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.579e+00(1.638e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.01e-02
Itr 000726 | Wall 3.809e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.301e+00(1.628e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 9.05e-03
Itr 000727 | Wall 3.814e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.353e+00(1.620e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.11e-02
Itr 000728 | Wall 3.819e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.088e+00(1.604e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.21e-02
Itr 000729 | Wall 3.824e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.75(1.75) | Loss 1.76(1.76) | FE 128(128) | Grad Norm 1.315e+00(1.595e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.12e-02
Itr 000730 | Wall 3.830e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 9.965e-01(1.577e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.12e-02
Itr 000731 | Wall 3.835e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.260e+00(1.568e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.24e-02
Itr 000732 | Wall 3.840e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.160e+00(1.556e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.38e-02
Itr 000733 | Wall 3.846e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 6.976e-01(1.530e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.12e-02
Itr 000734 | Wall 3.851e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.091e+00(1.517e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.37e-02
Itr 000735 | Wall 3.857e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.047e+00(1.503e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.30e-02
Itr 000736 | Wall 3.862e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 7.810e-01(1.481e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.19e-02
Itr 000737 | Wall 3.867e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.090e+00(1.469e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.50e-02
Itr 000738 | Wall 3.872e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 7.174e-01(1.447e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.40e-02
Itr 000739 | Wall 3.877e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 8.422e-01(1.429e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.38e-02
Itr 000740 | Wall 3.883e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 8.913e-01(1.412e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.70e-02
Itr 000741 | Wall 3.888e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.73(1.75) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 8.847e-01(1.397e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.47e-02
Itr 000742 | Wall 3.893e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.73(1.75) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 8.370e-01(1.380e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.65e-02
Itr 000743 | Wall 3.898e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 6.182e-01(1.357e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.53e-02
Itr 000744 | Wall 3.904e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.75) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.126e+00(1.350e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.73e-02
Itr 000745 | Wall 3.909e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.73(1.75) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 9.600e-01(1.338e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.83e-02
Itr 000746 | Wall 3.915e+02(0.52) | Time/Itr 0.58(0.53) | BPD 1.73(1.75) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 9.248e-01(1.326e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.82e-02
Itr 000747 | Wall 3.920e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.73(1.75) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 7.364e-01(1.308e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.91e-02
Itr 000748 | Wall 3.925e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.73(1.74) | Loss 1.74(1.76) | FE 128(128) | Grad Norm 1.278e+00(1.307e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.90e-02
Itr 000749 | Wall 3.931e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.74) | Loss 1.75(1.76) | FE 128(128) | Grad Norm 1.429e+00(1.311e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.85e-02
Itr 000750 | Wall 3.936e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.065e+00(1.304e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.98e-02
Itr 000751 | Wall 3.941e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.291e+00(1.303e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.30e-02
Itr 000752 | Wall 3.946e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.204e+00(1.300e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 2.37e-02
Itr 000753 | Wall 3.952e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 9.412e-01(1.290e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.56e-02
Itr 000754 | Wall 3.957e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.72(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 7.058e-01(1.272e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.44e-02
Itr 000755 | Wall 3.962e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.026e+00(1.265e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.33e-02
Itr 000756 | Wall 3.967e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 8.653e-01(1.253e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.58e-02
Itr 000757 | Wall 3.972e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 5.854e-01(1.233e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.66e-02
Itr 000758 | Wall 3.978e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 7.226e-01(1.217e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.92e-02
Itr 000759 | Wall 3.983e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.020e+00(1.211e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.55e-02
Itr 000760 | Wall 3.988e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.72(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.726e+00(1.227e+00) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.78e-02
Itr 000761 | Wall 3.993e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.74(1.74) | Loss 1.75(1.75) | FE 128(128) | Grad Norm 2.777e+00(1.273e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.86e-02
Itr 000762 | Wall 3.999e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.74(1.74) | Loss 1.75(1.75) | FE 128(128) | Grad Norm 4.082e+00(1.358e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 3.52e-02
Itr 000763 | Wall 4.003e+02(0.52) | Time/Itr 0.47(0.52) | BPD 1.78(1.74) | Loss 1.79(1.75) | FE 128(128) | Grad Norm 3.662e+00(1.427e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 2.61e-02
Itr 000764 | Wall 4.009e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.74(1.74) | Loss 1.75(1.75) | FE 128(128) | Grad Norm 2.596e+00(1.462e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 3.27e-02
Itr 000765 | Wall 4.014e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.76(1.74) | Loss 1.77(1.75) | FE 128(128) | Grad Norm 4.506e+00(1.553e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.74e-02
Itr 000766 | Wall 4.020e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.72(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.708e+00(1.558e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.26e-02
Itr 000767 | Wall 4.025e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.74(1.74) | Loss 1.75(1.75) | FE 128(128) | Grad Norm 2.498e+00(1.586e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.96e-02
Itr 000768 | Wall 4.030e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 2.304e+00(1.608e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.59e-02
Itr 000769 | Wall 4.035e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.72(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.016e+00(1.590e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.32e-02
Itr 000770 | Wall 4.040e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.188e+00(1.578e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.28e-02
Itr 000771 | Wall 4.045e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 1.937e+00(1.589e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.98e-02
Itr 000772 | Wall 4.051e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.807e+00(1.595e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 3.14e-02
Itr 000773 | Wall 4.056e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.455e+00(1.591e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.00e-02
Itr 000774 | Wall 4.062e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.451e+00(1.587e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.57e-02
Itr 000775 | Wall 4.067e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 1.692e+00(1.590e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 3.98e-02
Itr 000776 | Wall 4.072e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.71(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 1.221e+00(1.579e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.19e-02
Itr 000777 | Wall 4.078e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.72(1.74) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 1.440e+00(1.575e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 4.24e-02
Itr 000778 | Wall 4.083e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.73(1.74) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.820e+00(1.582e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 3.62e-02
Itr 000779 | Wall 4.088e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.71(1.73) | Loss 1.72(1.75) | FE 128(128) | Grad Norm 9.977e-01(1.564e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.75e-02
Itr 000780 | Wall 4.093e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.72(1.73) | Loss 1.73(1.75) | FE 128(128) | Grad Norm 1.388e+00(1.559e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.45e-02
Itr 000781 | Wall 4.099e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.73(1.73) | Loss 1.74(1.75) | FE 128(128) | Grad Norm 1.380e+00(1.554e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 3.50e-02
Itr 000782 | Wall 4.104e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 1.310e+00(1.547e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.09e-02
Itr 000783 | Wall 4.109e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 1.566e+00(1.547e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 4.41e-02
Itr 000784 | Wall 4.114e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.485e+00(1.545e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 5.13e-02
Itr 000785 | Wall 4.119e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 1.413e+00(1.541e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 3.98e-02
Itr 000786 | Wall 4.125e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.660e+00(1.545e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 4.54e-02
Itr 000787 | Wall 4.130e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.202e+00(1.535e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.33e-02
Itr 000788 | Wall 4.135e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 1.475e+00(1.533e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 4.65e-02
Itr 000789 | Wall 4.141e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.70(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.050e+00(1.518e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.23e-02
Itr 000790 | Wall 4.146e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.929e+00(1.531e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.32e-02
Itr 000791 | Wall 4.151e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 2.350e+00(1.555e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 4.47e-02
Itr 000792 | Wall 4.157e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 3.046e+00(1.600e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.06e-02
Itr 000793 | Wall 4.162e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 3.537e+00(1.658e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 5.36e-02
Itr 000794 | Wall 4.168e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.72(1.73) | Loss 1.74(1.74) | FE 128(128) | Grad Norm 2.738e+00(1.690e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.44e-02
Itr 000795 | Wall 4.173e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.961e+00(1.698e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.33e-02
Itr 000796 | Wall 4.178e+02(0.52) | Time/Itr 0.58(0.53) | BPD 1.73(1.73) | Loss 1.74(1.74) | FE 128(128) | Grad Norm 3.682e+00(1.758e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 5.42e-02
Itr 000797 | Wall 4.184e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.74(1.73) | Loss 1.75(1.74) | FE 128(128) | Grad Norm 2.724e+00(1.787e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 5.25e-02
Itr 000798 | Wall 4.189e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 2.630e+00(1.812e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 5.63e-02
Itr 000799 | Wall 4.194e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.71(1.73) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 2.535e+00(1.834e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 5.72e-02
Itr 000800 | Wall 4.200e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 2.412e+00(1.851e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 5.92e-02
Itr 000801 | Wall 4.205e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 2.070e+00(1.858e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 5.30e-02
Itr 000802 | Wall 4.210e+02(0.52) | Time/Itr 0.47(0.53) | BPD 1.72(1.73) | Loss 1.73(1.74) | FE 128(128) | Grad Norm 2.472e+00(1.876e+00) | TT 4.00(4.00) | kinetic_energy: 9.88e-01 | jacobian_norm2: 5.24e-02
Itr 000803 | Wall 4.215e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.70(1.72) | Loss 1.71(1.74) | FE 128(128) | Grad Norm 1.456e+00(1.864e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 6.18e-02
Itr 000804 | Wall 4.220e+02(0.52) | Time/Itr 0.48(0.53) | BPD 1.71(1.72) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 2.347e+00(1.878e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 6.32e-02
Itr 000805 | Wall 4.225e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.71(1.72) | Loss 1.72(1.74) | FE 128(128) | Grad Norm 1.569e+00(1.869e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 6.01e-02
Itr 000806 | Wall 4.230e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.71(1.72) | Loss 1.72(1.73) | FE 128(128) | Grad Norm 1.487e+00(1.857e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.96e-02
Itr 000807 | Wall 4.236e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.801e+00(1.856e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 6.83e-02
Itr 000808 | Wall 4.241e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.71(1.72) | Loss 1.72(1.73) | FE 128(128) | Grad Norm 1.586e+00(1.848e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 5.46e-02
Itr 000809 | Wall 4.246e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.71(1.72) | Loss 1.72(1.73) | FE 128(128) | Grad Norm 1.467e+00(1.836e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 5.32e-02
Itr 000810 | Wall 4.251e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.70(1.72) | Loss 1.72(1.73) | FE 128(128) | Grad Norm 2.039e+00(1.842e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 6.81e-02
Itr 000811 | Wall 4.257e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.106e+00(1.820e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 6.70e-02
Itr 000812 | Wall 4.262e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.68(1.72) | Loss 1.70(1.73) | FE 128(128) | Grad Norm 1.655e+00(1.815e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 8.36e-02
Itr 000813 | Wall 4.267e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.816e+00(1.815e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 7.60e-02
Itr 000814 | Wall 4.272e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.69(1.72) | Loss 1.70(1.73) | FE 128(128) | Grad Norm 1.255e+00(1.798e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.92e-02
Itr 000815 | Wall 4.277e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.232e+00(1.781e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 7.30e-02
Itr 000816 | Wall 4.282e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.751e+00(1.781e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 7.88e-02
Itr 000817 | Wall 4.287e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 8.850e-01(1.754e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 7.69e-02
Itr 000818 | Wall 4.293e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.565e+00(1.748e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 7.76e-02
Itr 000819 | Wall 4.297e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.70(1.72) | Loss 1.71(1.73) | FE 128(128) | Grad Norm 1.479e+00(1.740e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 8.11e-02
Itr 000820 | Wall 4.302e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.69(1.71) | Loss 1.70(1.73) | FE 128(128) | Grad Norm 1.844e+00(1.743e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 9.37e-02
Itr 000821 | Wall 4.308e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.69(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.965e+00(1.750e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 9.08e-02
Itr 000822 | Wall 4.313e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.68(1.71) | Loss 1.69(1.72) | FE 128(128) | Grad Norm 1.556e+00(1.744e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.01e-01
Itr 000823 | Wall 4.318e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.68(1.71) | Loss 1.69(1.72) | FE 128(128) | Grad Norm 2.103e+00(1.755e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.01e-01
Itr 000824 | Wall 4.324e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.68(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.952e+00(1.761e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 9.91e-02
Itr 000825 | Wall 4.329e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.69(1.71) | Loss 1.71(1.72) | FE 128(128) | Grad Norm 2.330e+00(1.778e+00) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 8.57e-02
Itr 000826 | Wall 4.334e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.69(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.516e+00(1.770e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 9.01e-02
Itr 000827 | Wall 4.339e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.69(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 2.085e+00(1.779e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.00e-01
Itr 000828 | Wall 4.345e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.68(1.71) | Loss 1.69(1.72) | FE 128(128) | Grad Norm 1.764e+00(1.779e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.06e-01
Itr 000829 | Wall 4.350e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.69(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.738e+00(1.778e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 9.83e-02
Itr 000830 | Wall 4.355e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.67(1.71) | Loss 1.69(1.72) | FE 128(128) | Grad Norm 1.711e+00(1.776e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.04e-01
Itr 000831 | Wall 4.360e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.69(1.71) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.908e+00(1.780e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.06e-01
Itr 000832 | Wall 4.366e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.70(1.71) | Loss 1.71(1.72) | FE 128(128) | Grad Norm 1.557e+00(1.773e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 8.79e-02
Itr 000833 | Wall 4.371e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.68(1.70) | Loss 1.69(1.72) | FE 128(128) | Grad Norm 1.299e+00(1.759e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.03e-01
Itr 000834 | Wall 4.376e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.68(1.70) | Loss 1.70(1.72) | FE 128(128) | Grad Norm 1.570e+00(1.753e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.54e-02
Itr 000835 | Wall 4.381e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.68(1.70) | Loss 1.69(1.71) | FE 128(128) | Grad Norm 1.430e+00(1.743e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.04e-01
Itr 000836 | Wall 4.387e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.68(1.70) | Loss 1.69(1.71) | FE 128(128) | Grad Norm 1.240e+00(1.728e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.98e-02
Itr 000837 | Wall 4.392e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.69(1.70) | Loss 1.70(1.71) | FE 128(128) | Grad Norm 1.458e+00(1.720e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 9.52e-02
Itr 000838 | Wall 4.397e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.68(1.70) | Loss 1.69(1.71) | FE 128(128) | Grad Norm 1.243e+00(1.706e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.06e-01
Itr 000839 | Wall 4.402e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.67(1.70) | Loss 1.68(1.71) | FE 128(128) | Grad Norm 1.277e+00(1.693e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.20e-01
Itr 000840 | Wall 4.407e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.67(1.70) | Loss 1.69(1.71) | FE 128(128) | Grad Norm 1.267e+00(1.680e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.10e-01
Itr 000841 | Wall 4.413e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.68(1.70) | Loss 1.70(1.71) | FE 128(128) | Grad Norm 1.443e+00(1.673e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.02e-01
Itr 000842 | Wall 4.418e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.66(1.70) | Loss 1.67(1.71) | FE 128(128) | Grad Norm 1.026e+00(1.654e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.24e-01
Itr 000843 | Wall 4.423e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.67(1.70) | Loss 1.68(1.71) | FE 128(128) | Grad Norm 1.280e+00(1.643e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.16e-01
Itr 000844 | Wall 4.428e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.67(1.70) | Loss 1.69(1.71) | FE 128(128) | Grad Norm 1.150e+00(1.628e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.11e-01
Itr 000845 | Wall 4.433e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.67(1.70) | Loss 1.68(1.71) | FE 128(128) | Grad Norm 1.333e+00(1.619e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.24e-01
Itr 000846 | Wall 4.439e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.66(1.69) | Loss 1.67(1.71) | FE 128(128) | Grad Norm 1.028e+00(1.601e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.36e-01
Itr 000847 | Wall 4.444e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.67(1.69) | Loss 1.68(1.70) | FE 128(128) | Grad Norm 1.221e+00(1.590e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.30e-01
Itr 000848 | Wall 4.449e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.68(1.69) | Loss 1.69(1.70) | FE 128(128) | Grad Norm 1.110e+00(1.575e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.20e-01
Itr 000849 | Wall 4.454e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.66(1.69) | Loss 1.68(1.70) | FE 128(128) | Grad Norm 1.032e+00(1.559e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.28e-01
Itr 000850 | Wall 4.459e+02(0.52) | Time/Itr 0.47(0.52) | BPD 1.66(1.69) | Loss 1.68(1.70) | FE 128(128) | Grad Norm 1.296e+00(1.551e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.18e-01
Itr 000851 | Wall 4.464e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.66(1.69) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 9.975e-01(1.535e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.32e-01
Itr 000852 | Wall 4.470e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.67(1.69) | Loss 1.69(1.70) | FE 128(128) | Grad Norm 1.265e+00(1.526e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.17e-01
Itr 000853 | Wall 4.475e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.67(1.69) | Loss 1.68(1.70) | FE 128(128) | Grad Norm 1.105e+00(1.514e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.28e-01
Itr 000854 | Wall 4.480e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.66(1.69) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 9.516e-01(1.497e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.19e-01
Itr 000855 | Wall 4.485e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.66(1.69) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 1.253e+00(1.490e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.38e-01
Itr 000856 | Wall 4.490e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.66(1.69) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 1.380e+00(1.486e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.34e-01
Itr 000857 | Wall 4.495e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.66(1.69) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 1.954e+00(1.500e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.31e-01
Itr 000858 | Wall 4.500e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.66(1.68) | Loss 1.67(1.70) | FE 128(128) | Grad Norm 2.693e+00(1.536e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.50e-01
Itr 000859 | Wall 4.506e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.67(1.68) | Loss 1.68(1.70) | FE 128(128) | Grad Norm 2.907e+00(1.577e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.28e-01
Itr 000860 | Wall 4.511e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.66(1.68) | Loss 1.67(1.69) | FE 128(128) | Grad Norm 1.939e+00(1.588e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.38e-01
Itr 000861 | Wall 4.516e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.66(1.68) | Loss 1.67(1.69) | FE 128(128) | Grad Norm 1.815e+00(1.595e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.39e-01
Itr 000862 | Wall 4.521e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.65(1.68) | Loss 1.66(1.69) | FE 128(128) | Grad Norm 1.712e+00(1.598e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.50e-01
Itr 000863 | Wall 4.527e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.65(1.68) | Loss 1.66(1.69) | FE 128(128) | Grad Norm 1.622e+00(1.599e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.51e-01
Itr 000864 | Wall 4.532e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.66(1.68) | Loss 1.67(1.69) | FE 128(128) | Grad Norm 1.707e+00(1.602e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.45e-01
Itr 000865 | Wall 4.538e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.65(1.68) | Loss 1.67(1.69) | FE 128(128) | Grad Norm 2.113e+00(1.618e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.56e-01
Itr 000866 | Wall 4.543e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.65(1.68) | Loss 1.66(1.69) | FE 128(128) | Grad Norm 1.219e+00(1.606e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.47e-01
Itr 000867 | Wall 4.548e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.66(1.68) | Loss 1.67(1.69) | FE 128(128) | Grad Norm 1.593e+00(1.605e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.40e-01
Itr 000868 | Wall 4.554e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.65(1.68) | Loss 1.66(1.69) | FE 128(128) | Grad Norm 2.028e+00(1.618e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.47e-01
Itr 000869 | Wall 4.559e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.64(1.68) | Loss 1.66(1.69) | FE 128(128) | Grad Norm 1.910e+00(1.627e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.46e-01
Itr 000870 | Wall 4.564e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.64(1.67) | Loss 1.65(1.69) | FE 128(128) | Grad Norm 1.485e+00(1.623e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.65e-01
Itr 000871 | Wall 4.569e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.63(1.67) | Loss 1.64(1.69) | FE 128(128) | Grad Norm 1.592e+00(1.622e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.69e-01
Itr 000872 | Wall 4.575e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.64(1.67) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 1.679e+00(1.623e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.51e-01
Itr 000873 | Wall 4.580e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.65(1.67) | Loss 1.66(1.68) | FE 128(128) | Grad Norm 1.044e+00(1.606e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.54e-01
Itr 000874 | Wall 4.585e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.64(1.67) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 1.419e+00(1.600e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.57e-01
Itr 000875 | Wall 4.591e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.64(1.67) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 1.322e+00(1.592e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.62e-01
Itr 000876 | Wall 4.596e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.64(1.67) | Loss 1.66(1.68) | FE 128(128) | Grad Norm 1.390e+00(1.586e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.62e-01
Itr 000877 | Wall 4.601e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.62(1.67) | Loss 1.63(1.68) | FE 128(128) | Grad Norm 1.485e+00(1.583e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.91e-01
Itr 000878 | Wall 4.606e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.62(1.67) | Loss 1.64(1.68) | FE 128(128) | Grad Norm 1.442e+00(1.579e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.87e-01
Itr 000879 | Wall 4.611e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.65(1.67) | Loss 1.66(1.68) | FE 128(128) | Grad Norm 1.259e+00(1.569e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.50e-01
Itr 000880 | Wall 4.616e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.63(1.66) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 1.386e+00(1.564e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.67e-01
Itr 000881 | Wall 4.622e+02(0.52) | Time/Itr 0.56(0.52) | BPD 1.64(1.66) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 9.863e-01(1.546e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.58e-01
Itr 000882 | Wall 4.628e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.63(1.66) | Loss 1.65(1.68) | FE 128(128) | Grad Norm 8.834e-01(1.526e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.69e-01
Itr 000883 | Wall 4.633e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.62(1.66) | Loss 1.63(1.67) | FE 128(128) | Grad Norm 1.062e+00(1.512e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.83e-01
Itr 000884 | Wall 4.638e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.63(1.66) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 1.043e+00(1.498e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.69e-01
Itr 000885 | Wall 4.643e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.62(1.66) | Loss 1.63(1.67) | FE 128(128) | Grad Norm 1.022e+00(1.484e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.79e-01
Itr 000886 | Wall 4.649e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.62(1.66) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 1.190e+00(1.475e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.73e-01
Itr 000887 | Wall 4.654e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.61(1.66) | Loss 1.62(1.67) | FE 128(128) | Grad Norm 1.321e+00(1.471e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.07e-01
Itr 000888 | Wall 4.659e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.62(1.66) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 1.811e+00(1.481e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.89e-01
Itr 000889 | Wall 4.664e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.64(1.66) | Loss 1.65(1.67) | FE 128(128) | Grad Norm 2.656e+00(1.516e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.71e-01
Itr 000890 | Wall 4.670e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.63(1.65) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 3.411e+00(1.573e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.88e-01
Itr 000891 | Wall 4.675e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.63(1.65) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 4.432e+00(1.659e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.02e-01
Itr 000892 | Wall 4.680e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.63(1.65) | Loss 1.64(1.67) | FE 128(128) | Grad Norm 3.735e+00(1.721e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 1.92e-01
Itr 000893 | Wall 4.686e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.63(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 2.817e+00(1.754e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.65e-01
Itr 000894 | Wall 4.691e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.63(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 3.407e+00(1.803e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 1.95e-01
Itr 000895 | Wall 4.696e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.64(1.65) | Loss 1.65(1.66) | FE 128(128) | Grad Norm 4.435e+00(1.882e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 1.86e-01
Itr 000896 | Wall 4.701e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.63(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 3.463e+00(1.930e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 1.82e-01
Itr 000897 | Wall 4.707e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.63(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 4.069e+00(1.994e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 1.96e-01
Itr 000898 | Wall 4.712e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.62(1.65) | Loss 1.63(1.66) | FE 128(128) | Grad Norm 2.924e+00(2.022e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.78e-01
Itr 000899 | Wall 4.718e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.61(1.65) | Loss 1.63(1.66) | FE 128(128) | Grad Norm 2.880e+00(2.048e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.94e-01
Itr 000900 | Wall 4.723e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.63(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 3.355e+00(2.087e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.82e-01
Itr 000901 | Wall 4.728e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.62(1.65) | Loss 1.63(1.66) | FE 128(128) | Grad Norm 4.131e+00(2.148e+00) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 2.22e-01
Itr 000902 | Wall 4.733e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.62(1.65) | Loss 1.64(1.66) | FE 128(128) | Grad Norm 3.467e+00(2.188e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.94e-01
Itr 000903 | Wall 4.739e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.62(1.64) | Loss 1.63(1.66) | FE 128(128) | Grad Norm 2.297e+00(2.191e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.01e-01
Itr 000904 | Wall 4.744e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.61(1.64) | Loss 1.62(1.66) | FE 128(128) | Grad Norm 3.582e+00(2.233e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.29e-01
Itr 000905 | Wall 4.749e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.62(1.64) | Loss 1.63(1.66) | FE 128(128) | Grad Norm 3.172e+00(2.261e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 2.03e-01
Itr 000906 | Wall 4.754e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.60(1.64) | Loss 1.62(1.65) | FE 128(128) | Grad Norm 1.942e+00(2.251e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.10e-01
Itr 000907 | Wall 4.760e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.61(1.64) | Loss 1.62(1.65) | FE 128(128) | Grad Norm 2.873e+00(2.270e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.14e-01
Itr 000908 | Wall 4.765e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.60(1.64) | Loss 1.61(1.65) | FE 128(128) | Grad Norm 3.359e+00(2.303e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.35e-01
Itr 000909 | Wall 4.770e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.60(1.64) | Loss 1.61(1.65) | FE 128(128) | Grad Norm 2.297e+00(2.303e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.21e-01
Itr 000910 | Wall 4.775e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.61(1.64) | Loss 1.63(1.65) | FE 128(128) | Grad Norm 2.794e+00(2.317e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 1.97e-01
Itr 000911 | Wall 4.781e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.60(1.64) | Loss 1.61(1.65) | FE 128(128) | Grad Norm 2.529e+00(2.324e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.17e-01
Itr 000912 | Wall 4.786e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.59(1.64) | Loss 1.60(1.65) | FE 128(128) | Grad Norm 2.538e+00(2.330e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 2.39e-01
Itr 000913 | Wall 4.791e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.59(1.63) | Loss 1.60(1.65) | FE 128(128) | Grad Norm 2.312e+00(2.330e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.41e-01
Itr 000914 | Wall 4.796e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.58(1.63) | Loss 1.59(1.64) | FE 128(128) | Grad Norm 2.472e+00(2.334e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 2.43e-01
Itr 000915 | Wall 4.801e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.58(1.63) | Loss 1.60(1.64) | FE 128(128) | Grad Norm 2.134e+00(2.328e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.46e-01
Itr 000916 | Wall 4.807e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.58(1.63) | Loss 1.60(1.64) | FE 128(128) | Grad Norm 2.746e+00(2.340e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.54e-01
Itr 000917 | Wall 4.812e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.59(1.63) | Loss 1.60(1.64) | FE 128(128) | Grad Norm 2.235e+00(2.337e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 2.32e-01
Itr 000918 | Wall 4.817e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.60(1.63) | Loss 1.61(1.64) | FE 128(128) | Grad Norm 2.719e+00(2.349e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.17e-01
Itr 000919 | Wall 4.823e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.58(1.63) | Loss 1.59(1.64) | FE 128(128) | Grad Norm 3.277e+00(2.376e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.52e-01
Itr 000920 | Wall 4.828e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.58(1.62) | Loss 1.60(1.64) | FE 128(128) | Grad Norm 3.586e+00(2.413e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.44e-01
Itr 000921 | Wall 4.833e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.60(1.62) | Loss 1.61(1.64) | FE 128(128) | Grad Norm 4.114e+00(2.464e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.31e-01
Itr 000922 | Wall 4.838e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.58(1.62) | Loss 1.59(1.64) | FE 128(128) | Grad Norm 3.479e+00(2.494e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.47e-01
Itr 000923 | Wall 4.844e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.58(1.62) | Loss 1.59(1.63) | FE 128(128) | Grad Norm 3.012e+00(2.510e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.48e-01
Itr 000924 | Wall 4.849e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.59(1.62) | Loss 1.60(1.63) | FE 128(128) | Grad Norm 2.965e+00(2.523e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.37e-01
Itr 000925 | Wall 4.854e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.58(1.62) | Loss 1.59(1.63) | FE 128(128) | Grad Norm 2.439e+00(2.521e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 2.36e-01
Itr 000926 | Wall 4.859e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.58(1.62) | Loss 1.59(1.63) | FE 128(128) | Grad Norm 3.019e+00(2.536e+00) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.44e-01
Itr 000927 | Wall 4.865e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.56(1.62) | Loss 1.58(1.63) | FE 128(128) | Grad Norm 2.364e+00(2.531e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.42e-01
Itr 000928 | Wall 4.870e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.57(1.61) | Loss 1.58(1.63) | FE 128(128) | Grad Norm 1.984e+00(2.514e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.31e-01
Itr 000929 | Wall 4.875e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.55(1.61) | Loss 1.56(1.63) | FE 128(128) | Grad Norm 2.357e+00(2.510e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.53e-01
Itr 000930 | Wall 4.881e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.54(1.61) | Loss 1.55(1.62) | FE 128(128) | Grad Norm 2.417e+00(2.507e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.80e-01
Itr 000931 | Wall 4.885e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.57(1.61) | Loss 1.58(1.62) | FE 128(128) | Grad Norm 3.868e+00(2.548e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.69e-01
Itr 000932 | Wall 4.891e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.61(1.61) | Loss 1.62(1.62) | FE 128(128) | Grad Norm 6.417e+00(2.664e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 2.62e-01
Itr 000933 | Wall 4.896e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.63(1.61) | Loss 1.65(1.62) | FE 128(128) | Grad Norm 6.666e+00(2.784e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.57e-01
Itr 000934 | Wall 4.901e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.56(1.61) | Loss 1.57(1.62) | FE 128(128) | Grad Norm 3.660e+00(2.810e+00) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 2.57e-01
Itr 000935 | Wall 4.907e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.63(1.61) | Loss 1.64(1.62) | FE 128(128) | Grad Norm 4.790e+00(2.869e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 2.37e-01
Itr 000936 | Wall 4.912e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.63(1.61) | Loss 1.65(1.62) | FE 128(128) | Grad Norm 7.707e+00(3.015e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 2.52e-01
Itr 000937 | Wall 4.918e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.63(1.61) | Loss 1.65(1.62) | FE 128(128) | Grad Norm 3.952e+00(3.043e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 2.13e-01
Itr 000938 | Wall 4.923e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.63(1.61) | Loss 1.64(1.62) | FE 128(128) | Grad Norm 4.465e+00(3.085e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 2.12e-01
Itr 000939 | Wall 4.928e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.63(1.61) | Loss 1.65(1.62) | FE 128(128) | Grad Norm 5.994e+00(3.173e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 2.30e-01
Itr 000940 | Wall 4.933e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.62(1.61) | Loss 1.64(1.63) | FE 128(128) | Grad Norm 5.341e+00(3.238e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 2.36e-01
Itr 000941 | Wall 4.939e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.61(1.61) | Loss 1.63(1.63) | FE 128(128) | Grad Norm 4.343e+00(3.271e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.42e-01
Itr 000942 | Wall 4.944e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.56(1.61) | Loss 1.58(1.62) | FE 128(128) | Grad Norm 3.626e+00(3.281e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.85e-01
Itr 000943 | Wall 4.949e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.58(1.61) | Loss 1.59(1.62) | FE 128(128) | Grad Norm 5.236e+00(3.340e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 3.03e-01
Itr 000944 | Wall 4.954e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.55(1.61) | Loss 1.56(1.62) | FE 128(128) | Grad Norm 4.003e+00(3.360e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 2.96e-01
Itr 000945 | Wall 4.960e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.56(1.61) | Loss 1.58(1.62) | FE 128(128) | Grad Norm 4.755e+00(3.402e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 3.04e-01
Itr 000946 | Wall 4.965e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.62(1.61) | Loss 1.63(1.62) | FE 128(128) | Grad Norm 6.446e+00(3.493e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 2.77e-01
Itr 000947 | Wall 4.970e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.56(1.61) | Loss 1.58(1.62) | FE 128(128) | Grad Norm 4.484e+00(3.523e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 3.00e-01
Itr 000948 | Wall 4.975e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.56(1.60) | Loss 1.57(1.62) | FE 128(128) | Grad Norm 3.819e+00(3.532e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 2.86e-01
Itr 000949 | Wall 4.981e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.56(1.60) | Loss 1.58(1.62) | FE 128(128) | Grad Norm 4.291e+00(3.555e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 2.86e-01
Itr 000950 | Wall 4.986e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.54(1.60) | Loss 1.56(1.61) | FE 128(128) | Grad Norm 4.036e+00(3.569e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 3.13e-01
Itr 000951 | Wall 4.991e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.55(1.60) | Loss 1.57(1.61) | FE 128(128) | Grad Norm 3.194e+00(3.558e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 2.82e-01
Itr 000952 | Wall 4.996e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.52(1.60) | Loss 1.54(1.61) | FE 128(128) | Grad Norm 3.142e+00(3.545e+00) | TT 4.00(4.00) | kinetic_energy: 1.18e+00 | jacobian_norm2: 3.01e-01
Itr 000953 | Wall 5.001e+02(0.52) | Time/Itr 0.48(0.52) | BPD 1.53(1.60) | Loss 1.55(1.61) | FE 128(128) | Grad Norm 4.446e+00(3.572e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 3.22e-01
Itr 000954 | Wall 5.007e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.56(1.59) | Loss 1.58(1.61) | FE 128(128) | Grad Norm 4.367e+00(3.596e+00) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.94e-01
Itr 000955 | Wall 5.012e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.52(1.59) | Loss 1.53(1.61) | FE 128(128) | Grad Norm 2.935e+00(3.576e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 3.18e-01
Itr 000956 | Wall 5.017e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.55(1.59) | Loss 1.56(1.60) | FE 128(128) | Grad Norm 4.438e+00(3.602e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.37e-01
Itr 000957 | Wall 5.022e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.48(1.59) | Loss 1.50(1.60) | FE 128(128) | Grad Norm 2.387e+00(3.566e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 3.64e-01
Itr 000958 | Wall 5.027e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.52(1.59) | Loss 1.54(1.60) | FE 128(128) | Grad Norm 2.804e+00(3.543e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 3.52e-01
Itr 000959 | Wall 5.033e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.50(1.58) | Loss 1.51(1.60) | FE 128(128) | Grad Norm 3.071e+00(3.529e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 3.71e-01
Itr 000960 | Wall 5.038e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.50(1.58) | Loss 1.52(1.59) | FE 128(128) | Grad Norm 2.272e+00(3.491e+00) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 3.84e-01
Itr 000961 | Wall 5.043e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.50(1.58) | Loss 1.52(1.59) | FE 128(128) | Grad Norm 3.397e+00(3.488e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 4.04e-01
Itr 000962 | Wall 5.048e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.52(1.58) | Loss 1.54(1.59) | FE 128(128) | Grad Norm 3.816e+00(3.498e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 3.81e-01
Itr 000963 | Wall 5.054e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.53(1.58) | Loss 1.55(1.59) | FE 128(128) | Grad Norm 4.890e+00(3.540e+00) | TT 4.00(4.00) | kinetic_energy: 1.17e+00 | jacobian_norm2: 3.54e-01
Itr 000964 | Wall 5.059e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.53(1.57) | Loss 1.55(1.59) | FE 128(128) | Grad Norm 3.899e+00(3.551e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.37e-01
Itr 000965 | Wall 5.064e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.50(1.57) | Loss 1.51(1.59) | FE 128(128) | Grad Norm 2.646e+00(3.523e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.58e-01
Itr 000966 | Wall 5.070e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.51(1.57) | Loss 1.53(1.58) | FE 128(128) | Grad Norm 2.743e+00(3.500e+00) | TT 4.00(4.00) | kinetic_energy: 1.19e+00 | jacobian_norm2: 3.26e-01
Itr 000967 | Wall 5.075e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.50(1.57) | Loss 1.51(1.58) | FE 128(128) | Grad Norm 2.317e+00(3.464e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 3.35e-01
Itr 000968 | Wall 5.081e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.46(1.56) | Loss 1.48(1.58) | FE 128(128) | Grad Norm 2.247e+00(3.428e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 3.87e-01
Itr 000969 | Wall 5.086e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.47(1.56) | Loss 1.48(1.58) | FE 128(128) | Grad Norm 2.578e+00(3.402e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.92e-01
Itr 000970 | Wall 5.091e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.46(1.56) | Loss 1.48(1.57) | FE 128(128) | Grad Norm 2.928e+00(3.388e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 4.19e-01
Itr 000971 | Wall 5.096e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.47(1.56) | Loss 1.49(1.57) | FE 128(128) | Grad Norm 3.794e+00(3.400e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 4.03e-01
Itr 000972 | Wall 5.101e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.44(1.55) | Loss 1.46(1.57) | FE 128(128) | Grad Norm 4.341e+00(3.429e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 4.70e-01
Itr 000973 | Wall 5.107e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.49(1.55) | Loss 1.51(1.57) | FE 128(128) | Grad Norm 3.998e+00(3.446e+00) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 4.09e-01
Itr 000974 | Wall 5.112e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.46(1.55) | Loss 1.48(1.56) | FE 128(128) | Grad Norm 3.855e+00(3.458e+00) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 3.98e-01
Itr 000975 | Wall 5.117e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.52(1.55) | Loss 1.53(1.56) | FE 128(128) | Grad Norm 7.758e+00(3.587e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.28e-01
Itr 000976 | Wall 5.123e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.58(1.55) | Loss 1.59(1.56) | FE 128(128) | Grad Norm 7.981e+00(3.719e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.58e-01
Itr 000977 | Wall 5.128e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.56(1.55) | Loss 1.58(1.56) | FE 128(128) | Grad Norm 7.505e+00(3.832e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 3.38e-01
Itr 000978 | Wall 5.133e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.51(1.55) | Loss 1.53(1.56) | FE 128(128) | Grad Norm 4.982e+00(3.867e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 3.15e-01
Itr 000979 | Wall 5.138e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.50(1.55) | Loss 1.51(1.56) | FE 128(128) | Grad Norm 3.943e+00(3.869e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 3.44e-01
Itr 000980 | Wall 5.144e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.50(1.54) | Loss 1.51(1.56) | FE 128(128) | Grad Norm 4.642e+00(3.892e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 3.75e-01
Itr 000981 | Wall 5.149e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.47(1.54) | Loss 1.49(1.56) | FE 128(128) | Grad Norm 4.069e+00(3.898e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 3.84e-01
Itr 000982 | Wall 5.154e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.48(1.54) | Loss 1.49(1.56) | FE 128(128) | Grad Norm 3.284e+00(3.879e+00) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 3.50e-01
Itr 000983 | Wall 5.159e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.50(1.54) | Loss 1.51(1.55) | FE 128(128) | Grad Norm 3.825e+00(3.878e+00) | TT 4.00(4.00) | kinetic_energy: 1.20e+00 | jacobian_norm2: 3.55e-01
Itr 000984 | Wall 5.165e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.48(1.54) | Loss 1.49(1.55) | FE 128(128) | Grad Norm 3.403e+00(3.863e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 3.65e-01
Itr 000985 | Wall 5.170e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.45(1.53) | Loss 1.47(1.55) | FE 128(128) | Grad Norm 3.688e+00(3.858e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.18e-01
Itr 000986 | Wall 5.175e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.42(1.53) | Loss 1.44(1.55) | FE 128(128) | Grad Norm 3.224e+00(3.839e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 4.60e-01
Itr 000987 | Wall 5.180e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.45(1.53) | Loss 1.47(1.54) | FE 128(128) | Grad Norm 3.699e+00(3.835e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 4.47e-01
Itr 000988 | Wall 5.186e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.45(1.53) | Loss 1.47(1.54) | FE 128(128) | Grad Norm 3.149e+00(3.814e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 4.40e-01
Itr 000989 | Wall 5.191e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.41(1.52) | Loss 1.42(1.54) | FE 128(128) | Grad Norm 3.257e+00(3.798e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 4.78e-01
Itr 000990 | Wall 5.197e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.43(1.52) | Loss 1.45(1.54) | FE 128(128) | Grad Norm 3.561e+00(3.790e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 4.74e-01
Itr 000991 | Wall 5.202e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.44(1.52) | Loss 1.45(1.53) | FE 128(128) | Grad Norm 4.296e+00(3.806e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 4.77e-01
Itr 000992 | Wall 5.207e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.47(1.52) | Loss 1.49(1.53) | FE 128(128) | Grad Norm 7.030e+00(3.902e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 4.80e-01
Itr 000993 | Wall 5.212e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.47(1.51) | Loss 1.48(1.53) | FE 128(128) | Grad Norm 5.956e+00(3.964e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 4.44e-01
Itr 000994 | Wall 5.217e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.46(1.51) | Loss 1.47(1.53) | FE 128(128) | Grad Norm 3.496e+00(3.950e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 4.32e-01
Itr 000995 | Wall 5.222e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.40(1.51) | Loss 1.42(1.53) | FE 128(128) | Grad Norm 3.172e+00(3.927e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 4.95e-01
Itr 000996 | Wall 5.228e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.45(1.51) | Loss 1.46(1.52) | FE 128(128) | Grad Norm 4.051e+00(3.930e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 4.58e-01
Itr 000997 | Wall 5.233e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.40(1.50) | Loss 1.41(1.52) | FE 128(128) | Grad Norm 3.555e+00(3.919e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 4.91e-01
Itr 000998 | Wall 5.238e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.42(1.50) | Loss 1.43(1.52) | FE 128(128) | Grad Norm 4.597e+00(3.939e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 4.89e-01
Itr 000999 | Wall 5.243e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.46(1.50) | Loss 1.48(1.52) | FE 128(128) | Grad Norm 7.038e+00(4.032e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 4.98e-01
Itr 001000 | Wall 5.248e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.44(1.50) | Loss 1.46(1.51) | FE 128(128) | Grad Norm 4.754e+00(4.054e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 4.46e-01
Itr 001001 | Wall 5.253e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.43(1.50) | Loss 1.44(1.51) | FE 128(128) | Grad Norm 4.482e+00(4.067e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 4.67e-01
Itr 001002 | Wall 5.258e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.46(1.50) | Loss 1.47(1.51) | FE 128(128) | Grad Norm 6.171e+00(4.130e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.45e-01
Itr 001003 | Wall 5.264e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.51(1.50) | Loss 1.53(1.51) | FE 128(128) | Grad Norm 7.705e+00(4.237e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.69e-01
Itr 001004 | Wall 5.269e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.41(1.49) | Loss 1.43(1.51) | FE 128(128) | Grad Norm 3.536e+00(4.216e+00) | TT 4.00(4.00) | kinetic_energy: 1.21e+00 | jacobian_norm2: 4.48e-01
Itr 001005 | Wall 5.274e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.45(1.49) | Loss 1.47(1.51) | FE 128(128) | Grad Norm 4.259e+00(4.218e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 4.31e-01
Itr 001006 | Wall 5.279e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.40(1.49) | Loss 1.42(1.51) | FE 128(128) | Grad Norm 3.857e+00(4.207e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 4.89e-01
Itr 001007 | Wall 5.284e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.41(1.49) | Loss 1.43(1.50) | FE 128(128) | Grad Norm 2.849e+00(4.166e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 4.75e-01
Itr 001008 | Wall 5.289e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.39(1.48) | Loss 1.41(1.50) | FE 128(128) | Grad Norm 3.667e+00(4.151e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 4.91e-01
Itr 001009 | Wall 5.295e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.37(1.48) | Loss 1.39(1.50) | FE 128(128) | Grad Norm 3.100e+00(4.119e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 5.40e-01
Itr 001010 | Wall 5.300e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.37(1.48) | Loss 1.38(1.49) | FE 128(128) | Grad Norm 3.226e+00(4.093e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 5.49e-01
Itr 001011 | Wall 5.306e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.38(1.47) | Loss 1.39(1.49) | FE 128(128) | Grad Norm 2.814e+00(4.054e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 5.17e-01
Itr 001012 | Wall 5.311e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.36(1.47) | Loss 1.38(1.49) | FE 128(128) | Grad Norm 2.428e+00(4.006e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 5.34e-01
Itr 001013 | Wall 5.316e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.36(1.47) | Loss 1.38(1.48) | FE 128(128) | Grad Norm 3.313e+00(3.985e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 5.90e-01
Itr 001014 | Wall 5.321e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.34(1.46) | Loss 1.35(1.48) | FE 128(128) | Grad Norm 2.352e+00(3.936e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 5.93e-01
Itr 001015 | Wall 5.326e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.34(1.46) | Loss 1.36(1.48) | FE 128(128) | Grad Norm 2.434e+00(3.891e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 6.11e-01
Itr 001016 | Wall 5.331e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.32(1.46) | Loss 1.34(1.47) | FE 128(128) | Grad Norm 2.912e+00(3.861e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 6.43e-01
Itr 001017 | Wall 5.337e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.29(1.45) | Loss 1.31(1.47) | FE 128(128) | Grad Norm 2.564e+00(3.822e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 6.67e-01
Itr 001018 | Wall 5.342e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.32(1.45) | Loss 1.34(1.46) | FE 128(128) | Grad Norm 2.639e+00(3.787e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 6.75e-01
Itr 001019 | Wall 5.347e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.33(1.44) | Loss 1.35(1.46) | FE 128(128) | Grad Norm 2.947e+00(3.762e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 6.54e-01
Itr 001020 | Wall 5.352e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.33(1.44) | Loss 1.35(1.46) | FE 128(128) | Grad Norm 2.990e+00(3.739e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 6.16e-01
Itr 001021 | Wall 5.358e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.31(1.44) | Loss 1.33(1.45) | FE 128(128) | Grad Norm 3.126e+00(3.720e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 6.65e-01
Itr 001022 | Wall 5.363e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.32(1.43) | Loss 1.34(1.45) | FE 128(128) | Grad Norm 3.518e+00(3.714e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 6.31e-01
Itr 001023 | Wall 5.368e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.30(1.43) | Loss 1.32(1.45) | FE 128(128) | Grad Norm 3.329e+00(3.703e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 6.37e-01
Itr 001024 | Wall 5.373e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.27(1.42) | Loss 1.29(1.44) | FE 128(128) | Grad Norm 2.889e+00(3.678e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 6.81e-01
Itr 001025 | Wall 5.378e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.28(1.42) | Loss 1.30(1.44) | FE 128(128) | Grad Norm 3.315e+00(3.667e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 6.76e-01
Itr 001026 | Wall 5.383e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.28(1.41) | Loss 1.30(1.43) | FE 128(128) | Grad Norm 3.790e+00(3.671e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 6.52e-01
Itr 001027 | Wall 5.389e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.28(1.41) | Loss 1.30(1.43) | FE 128(128) | Grad Norm 4.304e+00(3.690e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 6.48e-01
Itr 001028 | Wall 5.394e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.32(1.41) | Loss 1.35(1.43) | FE 128(128) | Grad Norm 5.826e+00(3.754e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 6.97e-01
Itr 001029 | Wall 5.399e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.29(1.40) | Loss 1.30(1.42) | FE 128(128) | Grad Norm 5.320e+00(3.801e+00) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 6.80e-01
Itr 001030 | Wall 5.404e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.27(1.40) | Loss 1.29(1.42) | FE 128(128) | Grad Norm 3.182e+00(3.782e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 7.43e-01
Itr 001031 | Wall 5.410e+02(0.52) | Time/Itr 0.57(0.52) | BPD 1.29(1.40) | Loss 1.31(1.41) | FE 128(128) | Grad Norm 6.851e+00(3.874e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 7.69e-01
Itr 001032 | Wall 5.415e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.26(1.39) | Loss 1.28(1.41) | FE 128(128) | Grad Norm 5.016e+00(3.909e+00) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 6.92e-01
Itr 001033 | Wall 5.421e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.29(1.39) | Loss 1.31(1.41) | FE 128(128) | Grad Norm 4.965e+00(3.940e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 6.92e-01
Itr 001034 | Wall 5.426e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.25(1.39) | Loss 1.27(1.40) | FE 128(128) | Grad Norm 4.294e+00(3.951e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 7.32e-01
Itr 001035 | Wall 5.431e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.27(1.38) | Loss 1.29(1.40) | FE 128(128) | Grad Norm 5.400e+00(3.995e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 7.26e-01
Itr 001036 | Wall 5.436e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.28(1.38) | Loss 1.30(1.40) | FE 128(128) | Grad Norm 3.742e+00(3.987e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 6.95e-01
Itr 001037 | Wall 5.441e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.27(1.38) | Loss 1.29(1.39) | FE 128(128) | Grad Norm 3.695e+00(3.978e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 7.02e-01
Itr 001038 | Wall 5.446e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.24(1.37) | Loss 1.26(1.39) | FE 128(128) | Grad Norm 3.718e+00(3.970e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 7.33e-01
Itr 001039 | Wall 5.451e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.27(1.37) | Loss 1.29(1.39) | FE 128(128) | Grad Norm 3.098e+00(3.944e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 7.15e-01
Itr 001040 | Wall 5.457e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.23(1.36) | Loss 1.25(1.38) | FE 128(128) | Grad Norm 3.216e+00(3.922e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 7.65e-01
Itr 001041 | Wall 5.462e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.23(1.36) | Loss 1.25(1.38) | FE 128(128) | Grad Norm 3.375e+00(3.906e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 7.57e-01
Itr 001042 | Wall 5.468e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.23(1.36) | Loss 1.26(1.37) | FE 128(128) | Grad Norm 3.230e+00(3.886e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 7.68e-01
Itr 001043 | Wall 5.473e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.20(1.35) | Loss 1.22(1.37) | FE 128(128) | Grad Norm 3.383e+00(3.871e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 8.17e-01
Itr 001044 | Wall 5.478e+02(0.52) | Time/Itr 0.54(0.52) | BPD 1.19(1.35) | Loss 1.21(1.37) | FE 128(128) | Grad Norm 2.956e+00(3.843e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 8.13e-01
Itr 001045 | Wall 5.484e+02(0.52) | Time/Itr 0.52(0.52) | BPD 1.21(1.34) | Loss 1.24(1.36) | FE 128(128) | Grad Norm 3.230e+00(3.825e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 8.24e-01
Itr 001046 | Wall 5.489e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.21(1.34) | Loss 1.23(1.36) | FE 128(128) | Grad Norm 3.145e+00(3.804e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 8.17e-01
Itr 001047 | Wall 5.494e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.21(1.33) | Loss 1.23(1.35) | FE 128(128) | Grad Norm 3.891e+00(3.807e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 8.54e-01
Itr 001048 | Wall 5.500e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.20(1.33) | Loss 1.22(1.35) | FE 128(128) | Grad Norm 4.723e+00(3.834e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 7.88e-01
Itr 001049 | Wall 5.505e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.19(1.33) | Loss 1.21(1.35) | FE 128(128) | Grad Norm 4.471e+00(3.854e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.46e-01
Itr 001050 | Wall 5.510e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.20(1.32) | Loss 1.22(1.34) | FE 128(128) | Grad Norm 3.777e+00(3.851e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 8.24e-01
Itr 001051 | Wall 5.516e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.20(1.32) | Loss 1.22(1.34) | FE 128(128) | Grad Norm 3.143e+00(3.830e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.17e-01
Itr 001052 | Wall 5.521e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.17(1.31) | Loss 1.20(1.33) | FE 128(128) | Grad Norm 3.885e+00(3.832e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 8.65e-01
Itr 001053 | Wall 5.526e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.16(1.31) | Loss 1.18(1.33) | FE 128(128) | Grad Norm 3.993e+00(3.836e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.61e-01
Itr 001054 | Wall 5.531e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.16(1.31) | Loss 1.18(1.32) | FE 128(128) | Grad Norm 4.101e+00(3.844e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 8.89e-01
Itr 001055 | Wall 5.537e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.22(1.30) | Loss 1.24(1.32) | FE 128(128) | Grad Norm 6.509e+00(3.924e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 8.46e-01
Itr 001056 | Wall 5.542e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.20(1.30) | Loss 1.22(1.32) | FE 128(128) | Grad Norm 6.924e+00(4.014e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 8.49e-01
Itr 001057 | Wall 5.547e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.16(1.30) | Loss 1.18(1.32) | FE 128(128) | Grad Norm 5.516e+00(4.059e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 8.84e-01
Itr 001058 | Wall 5.552e+02(0.52) | Time/Itr 0.49(0.53) | BPD 1.16(1.29) | Loss 1.18(1.31) | FE 128(128) | Grad Norm 6.849e+00(4.143e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 9.10e-01
Itr 001059 | Wall 5.558e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.19(1.29) | Loss 1.21(1.31) | FE 128(128) | Grad Norm 6.340e+00(4.209e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 8.93e-01
Itr 001060 | Wall 5.563e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.20(1.29) | Loss 1.22(1.31) | FE 128(128) | Grad Norm 6.128e+00(4.267e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 7.91e-01
Itr 001061 | Wall 5.568e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.20(1.28) | Loss 1.22(1.30) | FE 128(128) | Grad Norm 5.541e+00(4.305e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.15e-01
Itr 001062 | Wall 5.573e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.19(1.28) | Loss 1.21(1.30) | FE 128(128) | Grad Norm 5.240e+00(4.333e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 8.12e-01
Itr 001063 | Wall 5.579e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.17(1.28) | Loss 1.19(1.30) | FE 128(128) | Grad Norm 3.397e+00(4.305e+00) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 8.38e-01
Itr 001064 | Wall 5.584e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.15(1.27) | Loss 1.17(1.29) | FE 128(128) | Grad Norm 4.915e+00(4.323e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 9.06e-01
Itr 001065 | Wall 5.589e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.14(1.27) | Loss 1.16(1.29) | FE 128(128) | Grad Norm 4.534e+00(4.329e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 8.86e-01
Itr 001066 | Wall 5.595e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.14(1.27) | Loss 1.16(1.29) | FE 128(128) | Grad Norm 3.955e+00(4.318e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 9.53e-01
Itr 001067 | Wall 5.600e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.13(1.26) | Loss 1.16(1.28) | FE 128(128) | Grad Norm 4.411e+00(4.321e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 9.64e-01
Itr 001068 | Wall 5.605e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.17(1.26) | Loss 1.19(1.28) | FE 128(128) | Grad Norm 4.422e+00(4.324e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 9.16e-01
Itr 001069 | Wall 5.610e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.16(1.26) | Loss 1.18(1.28) | FE 128(128) | Grad Norm 4.549e+00(4.331e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 8.69e-01
Itr 001070 | Wall 5.616e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.15(1.25) | Loss 1.17(1.27) | FE 128(128) | Grad Norm 4.891e+00(4.348e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 9.22e-01
Itr 001071 | Wall 5.621e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.13(1.25) | Loss 1.15(1.27) | FE 128(128) | Grad Norm 5.666e+00(4.387e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 9.44e-01
Itr 001072 | Wall 5.626e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.13(1.25) | Loss 1.15(1.27) | FE 128(128) | Grad Norm 4.879e+00(4.402e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 9.35e-01
Itr 001073 | Wall 5.631e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.12(1.24) | Loss 1.15(1.26) | FE 128(128) | Grad Norm 4.884e+00(4.416e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 9.18e-01
Itr 001074 | Wall 5.637e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.12(1.24) | Loss 1.14(1.26) | FE 128(128) | Grad Norm 3.913e+00(4.401e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.89e-01
Itr 001075 | Wall 5.642e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.12(1.23) | Loss 1.14(1.25) | FE 128(128) | Grad Norm 5.600e+00(4.437e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.01e+00
Itr 001076 | Wall 5.648e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.10(1.23) | Loss 1.12(1.25) | FE 128(128) | Grad Norm 4.247e+00(4.431e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 9.78e-01
Itr 001077 | Wall 5.653e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.10(1.23) | Loss 1.13(1.25) | FE 128(128) | Grad Norm 5.150e+00(4.453e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 9.83e-01
Itr 001078 | Wall 5.658e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.09(1.22) | Loss 1.11(1.24) | FE 128(128) | Grad Norm 4.726e+00(4.461e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.08e+00
Itr 001079 | Wall 5.663e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.12(1.22) | Loss 1.14(1.24) | FE 128(128) | Grad Norm 5.310e+00(4.487e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.01e+00
Itr 001080 | Wall 5.668e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.10(1.22) | Loss 1.12(1.24) | FE 128(128) | Grad Norm 3.845e+00(4.467e+00) | TT 4.00(4.00) | kinetic_energy: 1.24e+00 | jacobian_norm2: 9.75e-01
Itr 001081 | Wall 5.674e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.07(1.21) | Loss 1.09(1.23) | FE 128(128) | Grad Norm 4.515e+00(4.469e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.04e+00
Itr 001082 | Wall 5.679e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.06(1.21) | Loss 1.08(1.23) | FE 128(128) | Grad Norm 4.198e+00(4.461e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 9.93e-01
Itr 001083 | Wall 5.684e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.08(1.20) | Loss 1.10(1.22) | FE 128(128) | Grad Norm 4.292e+00(4.456e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 9.94e-01
Itr 001084 | Wall 5.690e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.10(1.20) | Loss 1.13(1.22) | FE 128(128) | Grad Norm 5.527e+00(4.488e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.06e+00
Itr 001085 | Wall 5.695e+02(0.52) | Time/Itr 0.48(0.53) | BPD 1.10(1.20) | Loss 1.13(1.22) | FE 128(128) | Grad Norm 5.526e+00(4.519e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 9.68e-01
Itr 001086 | Wall 5.700e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.10(1.19) | Loss 1.13(1.22) | FE 128(128) | Grad Norm 6.697e+00(4.584e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 9.77e-01
Itr 001087 | Wall 5.705e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.07(1.19) | Loss 1.09(1.21) | FE 128(128) | Grad Norm 5.919e+00(4.624e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.05e+00
Itr 001088 | Wall 5.711e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.10(1.19) | Loss 1.12(1.21) | FE 128(128) | Grad Norm 5.195e+00(4.641e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.02e+00
Itr 001089 | Wall 5.716e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.10(1.18) | Loss 1.12(1.21) | FE 128(128) | Grad Norm 4.506e+00(4.637e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 9.64e-01
Itr 001090 | Wall 5.721e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.08(1.18) | Loss 1.10(1.20) | FE 128(128) | Grad Norm 4.334e+00(4.628e+00) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 9.48e-01
Itr 001091 | Wall 5.727e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.05(1.18) | Loss 1.07(1.20) | FE 128(128) | Grad Norm 4.598e+00(4.627e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.03e+00
Itr 001092 | Wall 5.732e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.07(1.17) | Loss 1.09(1.20) | FE 128(128) | Grad Norm 4.950e+00(4.637e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.08e+00
Itr 001093 | Wall 5.737e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.08(1.17) | Loss 1.11(1.19) | FE 128(128) | Grad Norm 4.602e+00(4.636e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.05e+00
Itr 001094 | Wall 5.743e+02(0.52) | Time/Itr 0.53(0.53) | BPD 1.06(1.17) | Loss 1.09(1.19) | FE 128(128) | Grad Norm 4.195e+00(4.623e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.04e+00
Itr 001095 | Wall 5.748e+02(0.52) | Time/Itr 0.55(0.53) | BPD 1.05(1.16) | Loss 1.07(1.19) | FE 128(128) | Grad Norm 4.131e+00(4.608e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.04e+00
Itr 001096 | Wall 5.754e+02(0.52) | Time/Itr 0.56(0.53) | BPD 1.06(1.16) | Loss 1.08(1.18) | FE 128(128) | Grad Norm 5.062e+00(4.622e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.07e+00
Itr 001097 | Wall 5.759e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.08(1.16) | Loss 1.11(1.18) | FE 128(128) | Grad Norm 5.801e+00(4.657e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.05e+00
Itr 001098 | Wall 5.764e+02(0.52) | Time/Itr 0.50(0.53) | BPD 1.04(1.16) | Loss 1.06(1.18) | FE 128(128) | Grad Norm 5.595e+00(4.685e+00) | TT 4.00(4.00) | kinetic_energy: 1.28e+00 | jacobian_norm2: 1.08e+00
Itr 001099 | Wall 5.769e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.04(1.15) | Loss 1.07(1.17) | FE 128(128) | Grad Norm 4.528e+00(4.680e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.11e+00
Itr 001100 | Wall 5.774e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.06(1.15) | Loss 1.08(1.17) | FE 128(128) | Grad Norm 4.768e+00(4.683e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.07e+00
Itr 001101 | Wall 5.779e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.06(1.15) | Loss 1.08(1.17) | FE 128(128) | Grad Norm 4.716e+00(4.684e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.10e+00
Itr 001102 | Wall 5.784e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.04(1.14) | Loss 1.07(1.17) | FE 128(128) | Grad Norm 3.664e+00(4.653e+00) | TT 4.00(4.00) | kinetic_energy: 1.25e+00 | jacobian_norm2: 1.03e+00
Itr 001103 | Wall 5.790e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.05(1.14) | Loss 1.07(1.16) | FE 128(128) | Grad Norm 3.257e+00(4.612e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.05e+00
Itr 001104 | Wall 5.795e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.02(1.14) | Loss 1.04(1.16) | FE 128(128) | Grad Norm 3.626e+00(4.582e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.11e+00
Itr 001105 | Wall 5.800e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.04(1.13) | Loss 1.06(1.16) | FE 128(128) | Grad Norm 3.869e+00(4.561e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.13e+00
Itr 001106 | Wall 5.805e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.00(1.13) | Loss 1.03(1.15) | FE 128(128) | Grad Norm 3.560e+00(4.531e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.14e+00
Itr 001107 | Wall 5.810e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.97(1.13) | Loss 0.99(1.15) | FE 128(128) | Grad Norm 4.906e+00(4.542e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.24e+00
Itr 001108 | Wall 5.816e+02(0.52) | Time/Itr 0.57(0.53) | BPD 1.05(1.12) | Loss 1.08(1.15) | FE 128(128) | Grad Norm 4.903e+00(4.553e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.10e+00
Itr 001109 | Wall 5.821e+02(0.52) | Time/Itr 0.51(0.53) | BPD 1.00(1.12) | Loss 1.03(1.14) | FE 128(128) | Grad Norm 4.624e+00(4.555e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.15e+00
Itr 001110 | Wall 5.826e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.03(1.12) | Loss 1.05(1.14) | FE 128(128) | Grad Norm 6.114e+00(4.602e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.15e+00
Itr 001111 | Wall 5.831e+02(0.52) | Time/Itr 0.52(0.53) | BPD 1.05(1.11) | Loss 1.08(1.14) | FE 128(128) | Grad Norm 5.966e+00(4.643e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.06e+00
Itr 001112 | Wall 5.836e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.99(1.11) | Loss 1.01(1.13) | FE 128(128) | Grad Norm 5.173e+00(4.658e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.13e+00
Itr 001113 | Wall 5.841e+02(0.52) | Time/Itr 0.49(0.52) | BPD 1.01(1.11) | Loss 1.04(1.13) | FE 128(128) | Grad Norm 6.111e+00(4.702e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.16e+00
Itr 001114 | Wall 5.847e+02(0.52) | Time/Itr 0.55(0.52) | BPD 1.03(1.11) | Loss 1.05(1.13) | FE 128(128) | Grad Norm 6.579e+00(4.758e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.12e+00
Itr 001115 | Wall 5.852e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.00(1.10) | Loss 1.03(1.13) | FE 128(128) | Grad Norm 5.526e+00(4.781e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.08e+00
Itr 001116 | Wall 5.857e+02(0.52) | Time/Itr 0.53(0.52) | BPD 1.05(1.10) | Loss 1.07(1.12) | FE 128(128) | Grad Norm 5.812e+00(4.812e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.09e+00
Itr 001117 | Wall 5.863e+02(0.52) | Time/Itr 0.57(0.53) | BPD 0.96(1.10) | Loss 0.99(1.12) | FE 128(128) | Grad Norm 5.879e+00(4.844e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.28e+00
Itr 001118 | Wall 5.868e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.98(1.09) | Loss 1.01(1.12) | FE 128(128) | Grad Norm 5.253e+00(4.857e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.18e+00
Itr 001119 | Wall 5.873e+02(0.52) | Time/Itr 0.54(0.53) | BPD 1.01(1.09) | Loss 1.03(1.11) | FE 128(128) | Grad Norm 5.391e+00(4.873e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.20e+00
Itr 001120 | Wall 5.878e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.99(1.09) | Loss 1.01(1.11) | FE 128(128) | Grad Norm 4.180e+00(4.852e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.15e+00
Itr 001121 | Wall 5.883e+02(0.52) | Time/Itr 0.50(0.52) | BPD 1.00(1.09) | Loss 1.03(1.11) | FE 128(128) | Grad Norm 4.579e+00(4.844e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.15e+00
Itr 001122 | Wall 5.888e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.94(1.08) | Loss 0.97(1.10) | FE 128(128) | Grad Norm 4.013e+00(4.819e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.28e+00
Itr 001123 | Wall 5.894e+02(0.52) | Time/Itr 0.56(0.52) | BPD 0.96(1.08) | Loss 0.99(1.10) | FE 128(128) | Grad Norm 3.778e+00(4.787e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.22e+00
Itr 001124 | Wall 5.899e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.93(1.07) | Loss 0.96(1.10) | FE 128(128) | Grad Norm 4.032e+00(4.765e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 1.25e+00
Itr 001125 | Wall 5.905e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.97(1.07) | Loss 0.99(1.09) | FE 128(128) | Grad Norm 4.396e+00(4.754e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.21e+00
Itr 001126 | Wall 5.910e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.96(1.07) | Loss 0.98(1.09) | FE 128(128) | Grad Norm 4.535e+00(4.747e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.23e+00
Itr 001127 | Wall 5.915e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.91(1.06) | Loss 0.93(1.09) | FE 128(128) | Grad Norm 4.317e+00(4.734e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.33e+00
Itr 001128 | Wall 5.920e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.99(1.06) | Loss 1.02(1.08) | FE 128(128) | Grad Norm 6.395e+00(4.784e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.20e+00
Itr 001129 | Wall 5.925e+02(0.52) | Time/Itr 0.51(0.52) | BPD 1.02(1.06) | Loss 1.04(1.08) | FE 128(128) | Grad Norm 6.384e+00(4.832e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 1.19e+00
Itr 001130 | Wall 5.931e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.92(1.05) | Loss 0.94(1.08) | FE 128(128) | Grad Norm 4.251e+00(4.815e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.22e+00
Itr 001131 | Wall 5.936e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.97(1.05) | Loss 1.00(1.08) | FE 128(128) | Grad Norm 4.728e+00(4.812e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.23e+00
Itr 001132 | Wall 5.941e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.99(1.05) | Loss 1.02(1.07) | FE 128(128) | Grad Norm 4.739e+00(4.810e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.15e+00
Itr 001133 | Wall 5.947e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.95(1.05) | Loss 0.98(1.07) | FE 128(128) | Grad Norm 4.875e+00(4.812e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.29e+00
Itr 001134 | Wall 5.952e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.94(1.04) | Loss 0.97(1.07) | FE 128(128) | Grad Norm 4.420e+00(4.800e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.25e+00
Itr 001135 | Wall 5.957e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.98(1.04) | Loss 1.00(1.07) | FE 128(128) | Grad Norm 4.531e+00(4.792e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.19e+00
Itr 001136 | Wall 5.962e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.95(1.04) | Loss 0.97(1.06) | FE 128(128) | Grad Norm 4.465e+00(4.782e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.23e+00
Itr 001137 | Wall 5.967e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.96(1.04) | Loss 0.99(1.06) | FE 128(128) | Grad Norm 4.179e+00(4.764e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.20e+00
Itr 001138 | Wall 5.973e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.94(1.03) | Loss 0.97(1.06) | FE 128(128) | Grad Norm 3.990e+00(4.741e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.23e+00
Itr 001139 | Wall 5.979e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.98(1.03) | Loss 1.00(1.06) | FE 128(128) | Grad Norm 3.659e+00(4.708e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.20e+00
Itr 001140 | Wall 5.984e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.95(1.03) | Loss 0.98(1.05) | FE 128(128) | Grad Norm 3.922e+00(4.685e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.22e+00
Itr 001141 | Wall 5.989e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.94(1.03) | Loss 0.96(1.05) | FE 128(128) | Grad Norm 4.171e+00(4.669e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.28e+00
Itr 001142 | Wall 5.994e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.93(1.02) | Loss 0.95(1.05) | FE 128(128) | Grad Norm 3.674e+00(4.639e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.26e+00
Itr 001143 | Wall 5.999e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.89(1.02) | Loss 0.92(1.04) | FE 128(128) | Grad Norm 3.390e+00(4.602e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.31e+00
Itr 001144 | Wall 6.005e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.92(1.02) | Loss 0.95(1.04) | FE 128(128) | Grad Norm 4.230e+00(4.591e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.33e+00
Itr 001145 | Wall 6.010e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.91(1.01) | Loss 0.93(1.04) | FE 128(128) | Grad Norm 4.805e+00(4.597e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.31e+00
Itr 001146 | Wall 6.015e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.87(1.01) | Loss 0.89(1.03) | FE 128(128) | Grad Norm 5.946e+00(4.638e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.43e+00
Itr 001147 | Wall 6.020e+02(0.52) | Time/Itr 0.50(0.53) | BPD 0.92(1.01) | Loss 0.95(1.03) | FE 128(128) | Grad Norm 7.054e+00(4.710e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.33e+00
Itr 001148 | Wall 6.026e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.91(1.00) | Loss 0.93(1.03) | FE 128(128) | Grad Norm 6.348e+00(4.759e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.36e+00
Itr 001149 | Wall 6.031e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.92(1.00) | Loss 0.95(1.03) | FE 128(128) | Grad Norm 4.268e+00(4.745e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.29e+00
Itr 001150 | Wall 6.037e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.91(1.00) | Loss 0.94(1.02) | FE 128(128) | Grad Norm 5.738e+00(4.774e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.28e+00
Itr 001151 | Wall 6.042e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.90(1.00) | Loss 0.93(1.02) | FE 128(128) | Grad Norm 4.513e+00(4.767e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.33e+00
Itr 001152 | Wall 6.048e+02(0.52) | Time/Itr 0.57(0.53) | BPD 0.93(0.99) | Loss 0.96(1.02) | FE 128(128) | Grad Norm 5.860e+00(4.799e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.30e+00
Itr 001153 | Wall 6.053e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.90(0.99) | Loss 0.93(1.02) | FE 128(128) | Grad Norm 4.913e+00(4.803e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.28e+00
Itr 001154 | Wall 6.058e+02(0.52) | Time/Itr 0.48(0.53) | BPD 0.91(0.99) | Loss 0.94(1.01) | FE 128(128) | Grad Norm 4.866e+00(4.805e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.34e+00
Itr 001155 | Wall 6.063e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.90(0.99) | Loss 0.93(1.01) | FE 128(128) | Grad Norm 5.798e+00(4.834e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.34e+00
Itr 001156 | Wall 6.068e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.87(0.98) | Loss 0.90(1.01) | FE 128(128) | Grad Norm 5.579e+00(4.857e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.40e+00
Itr 001157 | Wall 6.073e+02(0.52) | Time/Itr 0.50(0.53) | BPD 0.88(0.98) | Loss 0.90(1.00) | FE 128(128) | Grad Norm 4.743e+00(4.853e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.38e+00
Itr 001158 | Wall 6.078e+02(0.52) | Time/Itr 0.50(0.53) | BPD 0.90(0.98) | Loss 0.93(1.00) | FE 128(128) | Grad Norm 5.469e+00(4.872e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.37e+00
Itr 001159 | Wall 6.084e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.92(0.98) | Loss 0.95(1.00) | FE 128(128) | Grad Norm 6.051e+00(4.907e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.32e+00
Itr 001160 | Wall 6.089e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.89(0.97) | Loss 0.92(1.00) | FE 128(128) | Grad Norm 5.266e+00(4.918e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.40e+00
Itr 001161 | Wall 6.094e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.89(0.97) | Loss 0.92(1.00) | FE 128(128) | Grad Norm 4.555e+00(4.907e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.33e+00
Itr 001162 | Wall 6.100e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.89(0.97) | Loss 0.92(0.99) | FE 128(128) | Grad Norm 4.295e+00(4.889e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.31e+00
Itr 001163 | Wall 6.105e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.87(0.96) | Loss 0.90(0.99) | FE 128(128) | Grad Norm 5.295e+00(4.901e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.40e+00
Itr 001164 | Wall 6.111e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.89(0.96) | Loss 0.92(0.99) | FE 128(128) | Grad Norm 5.308e+00(4.913e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.27e+00
Itr 001165 | Wall 6.116e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.88(0.96) | Loss 0.91(0.99) | FE 128(128) | Grad Norm 4.394e+00(4.898e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.35e+00
Itr 001166 | Wall 6.121e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.87(0.96) | Loss 0.90(0.98) | FE 128(128) | Grad Norm 4.677e+00(4.891e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.48e+00
Itr 001167 | Wall 6.126e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.87(0.96) | Loss 0.90(0.98) | FE 128(128) | Grad Norm 6.889e+00(4.951e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 1.39e+00
Itr 001168 | Wall 6.131e+02(0.52) | Time/Itr 0.49(0.53) | BPD 0.90(0.95) | Loss 0.92(0.98) | FE 128(128) | Grad Norm 6.445e+00(4.996e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.43e+00
Itr 001169 | Wall 6.137e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.93(0.95) | Loss 0.96(0.98) | FE 128(128) | Grad Norm 6.374e+00(5.037e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.28e+00
Itr 001170 | Wall 6.142e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.83(0.95) | Loss 0.86(0.98) | FE 128(128) | Grad Norm 4.617e+00(5.024e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.37e+00
Itr 001171 | Wall 6.147e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.89(0.95) | Loss 0.92(0.97) | FE 128(128) | Grad Norm 8.001e+00(5.114e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.48e+00
Itr 001172 | Wall 6.153e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.94(0.95) | Loss 0.96(0.97) | FE 128(128) | Grad Norm 6.625e+00(5.159e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 1.26e+00
Itr 001173 | Wall 6.158e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.93(0.95) | Loss 0.96(0.97) | FE 128(128) | Grad Norm 5.050e+00(5.156e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.23e+00
Itr 001174 | Wall 6.163e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.90(0.95) | Loss 0.92(0.97) | FE 128(128) | Grad Norm 5.845e+00(5.176e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.32e+00
Itr 001175 | Wall 6.168e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.87(0.94) | Loss 0.89(0.97) | FE 128(128) | Grad Norm 4.509e+00(5.156e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.43e+00
Itr 001176 | Wall 6.174e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.88(0.94) | Loss 0.91(0.97) | FE 128(128) | Grad Norm 5.481e+00(5.166e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.43e+00
Itr 001177 | Wall 6.179e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.86(0.94) | Loss 0.89(0.96) | FE 128(128) | Grad Norm 4.239e+00(5.138e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.41e+00
Itr 001178 | Wall 6.184e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.83(0.94) | Loss 0.86(0.96) | FE 128(128) | Grad Norm 5.382e+00(5.146e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.51e+00
Itr 001179 | Wall 6.189e+02(0.52) | Time/Itr 0.50(0.53) | BPD 0.85(0.93) | Loss 0.88(0.96) | FE 128(128) | Grad Norm 4.906e+00(5.139e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.41e+00
Itr 001180 | Wall 6.194e+02(0.52) | Time/Itr 0.50(0.53) | BPD 0.87(0.93) | Loss 0.89(0.96) | FE 128(128) | Grad Norm 5.193e+00(5.140e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.40e+00
Itr 001181 | Wall 6.199e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.87(0.93) | Loss 0.90(0.96) | FE 128(128) | Grad Norm 5.439e+00(5.149e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.37e+00
Itr 001182 | Wall 6.204e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.83(0.93) | Loss 0.86(0.95) | FE 128(128) | Grad Norm 7.075e+00(5.207e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.57e+00
Itr 001183 | Wall 6.209e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.92(0.93) | Loss 0.95(0.95) | FE 128(128) | Grad Norm 7.593e+00(5.278e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.40e+00
Itr 001184 | Wall 6.214e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.91(0.93) | Loss 0.94(0.95) | FE 128(128) | Grad Norm 5.935e+00(5.298e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 1.31e+00
Itr 001185 | Wall 6.220e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.89(0.92) | Loss 0.92(0.95) | FE 128(128) | Grad Norm 6.560e+00(5.336e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.44e+00
Itr 001186 | Wall 6.225e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.90(0.92) | Loss 0.93(0.95) | FE 128(128) | Grad Norm 5.849e+00(5.351e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.39e+00
Itr 001187 | Wall 6.230e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.89(0.92) | Loss 0.92(0.95) | FE 128(128) | Grad Norm 5.922e+00(5.369e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 1.29e+00
Itr 001188 | Wall 6.236e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.88(0.92) | Loss 0.90(0.95) | FE 128(128) | Grad Norm 4.886e+00(5.354e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.33e+00
Itr 001189 | Wall 6.241e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.85(0.92) | Loss 0.88(0.95) | FE 128(128) | Grad Norm 5.705e+00(5.365e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.45e+00
Itr 001190 | Wall 6.246e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.93(0.92) | Loss 0.95(0.95) | FE 128(128) | Grad Norm 6.000e+00(5.384e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.28e+00
Itr 001191 | Wall 6.252e+02(0.52) | Time/Itr 0.56(0.52) | BPD 0.88(0.92) | Loss 0.90(0.95) | FE 128(128) | Grad Norm 4.347e+00(5.353e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 1.33e+00
Itr 001192 | Wall 6.257e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.84(0.92) | Loss 0.87(0.94) | FE 128(128) | Grad Norm 5.188e+00(5.348e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.46e+00
Itr 001193 | Wall 6.262e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.89(0.92) | Loss 0.91(0.94) | FE 128(128) | Grad Norm 6.967e+00(5.396e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.48e+00
Itr 001194 | Wall 6.267e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.87(0.91) | Loss 0.90(0.94) | FE 128(128) | Grad Norm 5.176e+00(5.390e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.38e+00
Itr 001195 | Wall 6.272e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.88(0.91) | Loss 0.91(0.94) | FE 128(128) | Grad Norm 5.763e+00(5.401e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.40e+00
Itr 001196 | Wall 6.277e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.85(0.91) | Loss 0.88(0.94) | FE 128(128) | Grad Norm 6.541e+00(5.435e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.41e+00
Itr 001197 | Wall 6.283e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.85(0.91) | Loss 0.88(0.94) | FE 128(128) | Grad Norm 6.388e+00(5.464e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.48e+00
Itr 001198 | Wall 6.288e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.88(0.91) | Loss 0.90(0.94) | FE 128(128) | Grad Norm 4.196e+00(5.426e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.38e+00
Itr 001199 | Wall 6.293e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.87(0.91) | Loss 0.89(0.93) | FE 128(128) | Grad Norm 5.340e+00(5.423e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.43e+00
Itr 001200 | Wall 6.298e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.85(0.91) | Loss 0.88(0.93) | FE 128(128) | Grad Norm 3.839e+00(5.375e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.37e+00
Itr 001201 | Wall 6.303e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.85(0.90) | Loss 0.88(0.93) | FE 128(128) | Grad Norm 3.768e+00(5.327e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.39e+00
Itr 001202 | Wall 6.308e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.79(0.90) | Loss 0.82(0.93) | FE 128(128) | Grad Norm 4.293e+00(5.296e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.57e+00
Itr 001203 | Wall 6.313e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.85(0.90) | Loss 0.87(0.93) | FE 128(128) | Grad Norm 3.909e+00(5.255e+00) | TT 4.00(4.00) | kinetic_energy: 1.46e+00 | jacobian_norm2: 1.50e+00
Itr 001204 | Wall 6.319e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.83(0.90) | Loss 0.86(0.92) | FE 128(128) | Grad Norm 4.206e+00(5.223e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.50e+00
Itr 001205 | Wall 6.324e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.81(0.89) | Loss 0.84(0.92) | FE 128(128) | Grad Norm 4.089e+00(5.189e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.47e+00
Itr 001206 | Wall 6.329e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.81(0.89) | Loss 0.84(0.92) | FE 128(128) | Grad Norm 4.034e+00(5.154e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.49e+00
Itr 001207 | Wall 6.334e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.79(0.89) | Loss 0.82(0.92) | FE 128(128) | Grad Norm 4.391e+00(5.132e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.56e+00
Itr 001208 | Wall 6.340e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.81(0.89) | Loss 0.84(0.91) | FE 128(128) | Grad Norm 3.187e+00(5.073e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.53e+00
Itr 001209 | Wall 6.345e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.80(0.88) | Loss 0.83(0.91) | FE 128(128) | Grad Norm 3.815e+00(5.035e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.52e+00
Itr 001210 | Wall 6.351e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.79(0.88) | Loss 0.82(0.91) | FE 128(128) | Grad Norm 4.655e+00(5.024e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.54e+00
Itr 001211 | Wall 6.356e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.80(0.88) | Loss 0.83(0.91) | FE 128(128) | Grad Norm 7.082e+00(5.086e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.58e+00
Itr 001212 | Wall 6.361e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.86(0.88) | Loss 0.88(0.91) | FE 128(128) | Grad Norm 8.358e+00(5.184e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.49e+00
Itr 001213 | Wall 6.366e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.89(0.88) | Loss 0.91(0.91) | FE 128(128) | Grad Norm 7.621e+00(5.257e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.39e+00
Itr 001214 | Wall 6.371e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.82(0.88) | Loss 0.85(0.90) | FE 128(128) | Grad Norm 4.924e+00(5.247e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.39e+00
Itr 001215 | Wall 6.376e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.82(0.87) | Loss 0.85(0.90) | FE 128(128) | Grad Norm 5.313e+00(5.249e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.45e+00
Itr 001216 | Wall 6.382e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.83(0.87) | Loss 0.86(0.90) | FE 128(128) | Grad Norm 5.690e+00(5.262e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.48e+00
Itr 001217 | Wall 6.387e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.84(0.87) | Loss 0.87(0.90) | FE 128(128) | Grad Norm 4.339e+00(5.235e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.41e+00
Itr 001218 | Wall 6.392e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.84(0.87) | Loss 0.87(0.90) | FE 128(128) | Grad Norm 4.589e+00(5.215e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.46e+00
Itr 001219 | Wall 6.398e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.84(0.87) | Loss 0.87(0.90) | FE 128(128) | Grad Norm 4.027e+00(5.180e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.48e+00
Itr 001220 | Wall 6.403e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.74(0.87) | Loss 0.77(0.89) | FE 128(128) | Grad Norm 3.528e+00(5.130e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.62e+00
Itr 001221 | Wall 6.409e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.85(0.87) | Loss 0.88(0.89) | FE 128(128) | Grad Norm 4.803e+00(5.120e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.53e+00
Itr 001222 | Wall 6.413e+02(0.52) | Time/Itr 0.48(0.52) | BPD 0.81(0.86) | Loss 0.84(0.89) | FE 128(128) | Grad Norm 3.958e+00(5.085e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.47e+00
Itr 001223 | Wall 6.419e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.78(0.86) | Loss 0.81(0.89) | FE 128(128) | Grad Norm 3.013e+00(5.023e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.50e+00
Itr 001224 | Wall 6.424e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.77(0.86) | Loss 0.80(0.89) | FE 128(128) | Grad Norm 3.720e+00(4.984e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.55e+00
Itr 001225 | Wall 6.430e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.81(0.86) | Loss 0.83(0.89) | FE 128(128) | Grad Norm 3.432e+00(4.938e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.50e+00
Itr 001226 | Wall 6.435e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.74(0.85) | Loss 0.77(0.88) | FE 128(128) | Grad Norm 3.372e+00(4.891e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.56e+00
Itr 001227 | Wall 6.440e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.83(0.85) | Loss 0.86(0.88) | FE 128(128) | Grad Norm 4.191e+00(4.870e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.53e+00
Itr 001228 | Wall 6.446e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.76(0.85) | Loss 0.79(0.88) | FE 128(128) | Grad Norm 4.402e+00(4.856e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.59e+00
Itr 001229 | Wall 6.451e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.80(0.85) | Loss 0.83(0.88) | FE 128(128) | Grad Norm 4.026e+00(4.831e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.53e+00
Itr 001230 | Wall 6.456e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.79(0.85) | Loss 0.82(0.88) | FE 128(128) | Grad Norm 3.882e+00(4.802e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.57e+00
Itr 001231 | Wall 6.461e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.76(0.84) | Loss 0.79(0.87) | FE 128(128) | Grad Norm 3.764e+00(4.771e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.57e+00
Itr 001232 | Wall 6.467e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.77(0.84) | Loss 0.79(0.87) | FE 128(128) | Grad Norm 4.641e+00(4.767e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.55e+00
Itr 001233 | Wall 6.472e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.78(0.84) | Loss 0.81(0.87) | FE 128(128) | Grad Norm 5.284e+00(4.783e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.60e+00
Itr 001234 | Wall 6.478e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.78(0.84) | Loss 0.81(0.87) | FE 128(128) | Grad Norm 5.896e+00(4.816e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.55e+00
Itr 001235 | Wall 6.483e+02(0.52) | Time/Itr 0.55(0.53) | BPD 0.80(0.84) | Loss 0.83(0.87) | FE 128(128) | Grad Norm 5.527e+00(4.837e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.52e+00
Itr 001236 | Wall 6.488e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.81(0.84) | Loss 0.84(0.87) | FE 128(128) | Grad Norm 5.946e+00(4.871e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.48e+00
Itr 001237 | Wall 6.494e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.77(0.83) | Loss 0.80(0.86) | FE 128(128) | Grad Norm 5.420e+00(4.887e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.54e+00
Itr 001238 | Wall 6.499e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.82(0.83) | Loss 0.85(0.86) | FE 128(128) | Grad Norm 5.917e+00(4.918e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.47e+00
Itr 001239 | Wall 6.505e+02(0.52) | Time/Itr 0.56(0.53) | BPD 0.77(0.83) | Loss 0.80(0.86) | FE 128(128) | Grad Norm 4.335e+00(4.901e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.52e+00
Itr 001240 | Wall 6.510e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.80(0.83) | Loss 0.83(0.86) | FE 128(128) | Grad Norm 5.077e+00(4.906e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.53e+00
Itr 001241 | Wall 6.515e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.79(0.83) | Loss 0.82(0.86) | FE 128(128) | Grad Norm 4.181e+00(4.884e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.54e+00
Itr 001242 | Wall 6.520e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.73(0.83) | Loss 0.76(0.86) | FE 128(128) | Grad Norm 4.808e+00(4.882e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.62e+00
Itr 001243 | Wall 6.525e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.76(0.83) | Loss 0.79(0.85) | FE 128(128) | Grad Norm 3.853e+00(4.851e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.58e+00
Itr 001244 | Wall 6.530e+02(0.52) | Time/Itr 0.49(0.53) | BPD 0.76(0.82) | Loss 0.79(0.85) | FE 128(128) | Grad Norm 4.809e+00(4.850e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.61e+00
Itr 001245 | Wall 6.535e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.73(0.82) | Loss 0.76(0.85) | FE 128(128) | Grad Norm 5.460e+00(4.868e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.63e+00
Itr 001246 | Wall 6.541e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.72(0.82) | Loss 0.75(0.85) | FE 128(128) | Grad Norm 5.798e+00(4.896e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.67e+00
Itr 001247 | Wall 6.546e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.79(0.82) | Loss 0.82(0.85) | FE 128(128) | Grad Norm 6.110e+00(4.932e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.58e+00
Itr 001248 | Wall 6.551e+02(0.52) | Time/Itr 0.52(0.53) | BPD 0.76(0.81) | Loss 0.79(0.84) | FE 128(128) | Grad Norm 5.834e+00(4.959e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.59e+00
Itr 001249 | Wall 6.556e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.73(0.81) | Loss 0.76(0.84) | FE 128(128) | Grad Norm 4.047e+00(4.932e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.59e+00
Itr 001250 | Wall 6.562e+02(0.52) | Time/Itr 0.53(0.53) | BPD 0.72(0.81) | Loss 0.76(0.84) | FE 128(128) | Grad Norm 5.153e+00(4.939e+00) | TT 4.00(4.00) | kinetic_energy: 1.47e+00 | jacobian_norm2: 1.66e+00
Itr 001251 | Wall 6.567e+02(0.52) | Time/Itr 0.54(0.53) | BPD 0.75(0.81) | Loss 0.78(0.84) | FE 128(128) | Grad Norm 4.518e+00(4.926e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.56e+00
Itr 001252 | Wall 6.572e+02(0.52) | Time/Itr 0.51(0.53) | BPD 0.74(0.81) | Loss 0.77(0.83) | FE 128(128) | Grad Norm 3.930e+00(4.896e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.57e+00
Itr 001253 | Wall 6.577e+02(0.52) | Time/Itr 0.48(0.52) | BPD 0.72(0.80) | Loss 0.75(0.83) | FE 128(128) | Grad Norm 5.439e+00(4.912e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.72e+00
Itr 001254 | Wall 6.582e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.71(0.80) | Loss 0.74(0.83) | FE 128(128) | Grad Norm 5.914e+00(4.942e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.66e+00
Itr 001255 | Wall 6.587e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.74(0.80) | Loss 0.77(0.83) | FE 128(128) | Grad Norm 5.976e+00(4.973e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.65e+00
Itr 001256 | Wall 6.592e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.74(0.80) | Loss 0.77(0.83) | FE 128(128) | Grad Norm 7.056e+00(5.036e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.71e+00
Itr 001257 | Wall 6.597e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.72(0.79) | Loss 0.75(0.82) | FE 128(128) | Grad Norm 6.012e+00(5.065e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.61e+00
Itr 001258 | Wall 6.603e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.76(0.79) | Loss 0.79(0.82) | FE 128(128) | Grad Norm 5.274e+00(5.071e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.59e+00
Itr 001259 | Wall 6.608e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.74(0.79) | Loss 0.77(0.82) | FE 128(128) | Grad Norm 6.831e+00(5.124e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.60e+00
Itr 001260 | Wall 6.614e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.78(0.79) | Loss 0.81(0.82) | FE 128(128) | Grad Norm 6.048e+00(5.152e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.51e+00
Itr 001261 | Wall 6.619e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.73(0.79) | Loss 0.76(0.82) | FE 128(128) | Grad Norm 5.628e+00(5.166e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.57e+00
Itr 001262 | Wall 6.624e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.75(0.79) | Loss 0.78(0.82) | FE 128(128) | Grad Norm 6.258e+00(5.199e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.57e+00
Itr 001263 | Wall 6.629e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.73(0.79) | Loss 0.76(0.82) | FE 128(128) | Grad Norm 5.045e+00(5.194e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.63e+00
Itr 001264 | Wall 6.634e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.78(0.79) | Loss 0.81(0.82) | FE 128(128) | Grad Norm 6.230e+00(5.225e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.58e+00
Itr 001265 | Wall 6.639e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.73(0.78) | Loss 0.76(0.81) | FE 128(128) | Grad Norm 6.377e+00(5.260e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 1.62e+00
Itr 001266 | Wall 6.645e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.73(0.78) | Loss 0.76(0.81) | FE 128(128) | Grad Norm 5.617e+00(5.271e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.73e+00
Itr 001267 | Wall 6.650e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.75(0.78) | Loss 0.79(0.81) | FE 128(128) | Grad Norm 6.998e+00(5.323e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.63e+00
Itr 001268 | Wall 6.655e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.75(0.78) | Loss 0.78(0.81) | FE 128(128) | Grad Norm 5.709e+00(5.334e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.57e+00
Itr 001269 | Wall 6.660e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.72(0.78) | Loss 0.75(0.81) | FE 128(128) | Grad Norm 6.860e+00(5.380e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.62e+00
Itr 001270 | Wall 6.666e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.74(0.78) | Loss 0.77(0.81) | FE 128(128) | Grad Norm 7.344e+00(5.439e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.57e+00
Itr 001271 | Wall 6.671e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.72(0.78) | Loss 0.75(0.81) | FE 128(128) | Grad Norm 6.665e+00(5.476e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.58e+00
Itr 001272 | Wall 6.676e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.75(0.78) | Loss 0.78(0.81) | FE 128(128) | Grad Norm 5.478e+00(5.476e+00) | TT 4.00(4.00) | kinetic_energy: 1.46e+00 | jacobian_norm2: 1.61e+00
Itr 001273 | Wall 6.681e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.74(0.77) | Loss 0.77(0.80) | FE 128(128) | Grad Norm 4.845e+00(5.457e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.57e+00
Itr 001274 | Wall 6.686e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.71(0.77) | Loss 0.74(0.80) | FE 128(128) | Grad Norm 4.951e+00(5.442e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.64e+00
Itr 001275 | Wall 6.692e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.74(0.77) | Loss 0.77(0.80) | FE 128(128) | Grad Norm 5.442e+00(5.442e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.67e+00
Itr 001276 | Wall 6.697e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.74(0.77) | Loss 0.77(0.80) | FE 128(128) | Grad Norm 6.043e+00(5.460e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.60e+00
Itr 001277 | Wall 6.702e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.68(0.77) | Loss 0.71(0.80) | FE 128(128) | Grad Norm 5.646e+00(5.465e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.69e+00
Itr 001278 | Wall 6.707e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.75(0.77) | Loss 0.78(0.80) | FE 128(128) | Grad Norm 6.129e+00(5.485e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.66e+00
Itr 001279 | Wall 6.712e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.64(0.76) | Loss 0.67(0.79) | FE 128(128) | Grad Norm 5.847e+00(5.496e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.72e+00
Itr 001280 | Wall 6.718e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.70(0.76) | Loss 0.73(0.79) | FE 128(128) | Grad Norm 7.035e+00(5.542e+00) | TT 4.00(4.00) | kinetic_energy: 1.50e+00 | jacobian_norm2: 1.75e+00
Itr 001281 | Wall 6.723e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.66(0.76) | Loss 0.69(0.79) | FE 128(128) | Grad Norm 6.438e+00(5.569e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.73e+00
Itr 001282 | Wall 6.728e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.72(0.76) | Loss 0.75(0.79) | FE 128(128) | Grad Norm 5.605e+00(5.570e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 1.65e+00
Itr 001283 | Wall 6.733e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.70(0.76) | Loss 0.73(0.79) | FE 128(128) | Grad Norm 5.371e+00(5.564e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.71e+00
Itr 001284 | Wall 6.739e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.69(0.75) | Loss 0.73(0.78) | FE 128(128) | Grad Norm 6.318e+00(5.587e+00) | TT 4.00(4.00) | kinetic_energy: 1.48e+00 | jacobian_norm2: 1.68e+00
Itr 001285 | Wall 6.743e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.69(0.75) | Loss 0.72(0.78) | FE 128(128) | Grad Norm 5.221e+00(5.576e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.71e+00
Itr 001286 | Wall 6.749e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.71(0.75) | Loss 0.74(0.78) | FE 128(128) | Grad Norm 6.258e+00(5.596e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.74e+00
Itr 001287 | Wall 6.754e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.71(0.75) | Loss 0.74(0.78) | FE 128(128) | Grad Norm 5.220e+00(5.585e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.65e+00
Itr 001288 | Wall 6.759e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.70(0.75) | Loss 0.73(0.78) | FE 128(128) | Grad Norm 5.278e+00(5.576e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.70e+00
Itr 001289 | Wall 6.764e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.73(0.75) | Loss 0.76(0.78) | FE 128(128) | Grad Norm 6.039e+00(5.590e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 1.67e+00
Itr 001290 | Wall 6.769e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.69(0.75) | Loss 0.72(0.78) | FE 128(128) | Grad Norm 5.864e+00(5.598e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.65e+00
Itr 001291 | Wall 6.775e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.66(0.74) | Loss 0.69(0.77) | FE 128(128) | Grad Norm 5.741e+00(5.602e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.71e+00
Itr 001292 | Wall 6.780e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.74(0.74) | Loss 0.77(0.77) | FE 128(128) | Grad Norm 7.489e+00(5.659e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.70e+00
Itr 001293 | Wall 6.785e+02(0.52) | Time/Itr 0.56(0.52) | BPD 0.72(0.74) | Loss 0.75(0.77) | FE 128(128) | Grad Norm 5.723e+00(5.661e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.63e+00
Itr 001294 | Wall 6.790e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.71(0.74) | Loss 0.74(0.77) | FE 128(128) | Grad Norm 4.938e+00(5.639e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.68e+00
Itr 001295 | Wall 6.795e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.70(0.74) | Loss 0.73(0.77) | FE 128(128) | Grad Norm 5.669e+00(5.640e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.67e+00
Itr 001296 | Wall 6.801e+02(0.52) | Time/Itr 0.55(0.52) | BPD 0.64(0.74) | Loss 0.67(0.77) | FE 128(128) | Grad Norm 4.069e+00(5.593e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.75e+00
Itr 001297 | Wall 6.806e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.69(0.74) | Loss 0.72(0.77) | FE 128(128) | Grad Norm 5.132e+00(5.579e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.73e+00
Itr 001298 | Wall 6.811e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.67(0.73) | Loss 0.70(0.76) | FE 128(128) | Grad Norm 4.637e+00(5.551e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 1.70e+00
Itr 001299 | Wall 6.816e+02(0.52) | Time/Itr 0.48(0.52) | BPD 0.66(0.73) | Loss 0.70(0.76) | FE 128(128) | Grad Norm 4.914e+00(5.532e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.78e+00
Itr 001300 | Wall 6.821e+02(0.52) | Time/Itr 0.50(0.52) | BPD 0.70(0.73) | Loss 0.73(0.76) | FE 128(128) | Grad Norm 5.500e+00(5.531e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.70e+00
Itr 001301 | Wall 6.827e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.68(0.73) | Loss 0.71(0.76) | FE 128(128) | Grad Norm 4.486e+00(5.499e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 1.67e+00
Itr 001302 | Wall 6.832e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.66(0.73) | Loss 0.69(0.76) | FE 128(128) | Grad Norm 4.184e+00(5.460e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.76e+00
Itr 001303 | Wall 6.837e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.65(0.72) | Loss 0.69(0.76) | FE 128(128) | Grad Norm 6.061e+00(5.478e+00) | TT 4.00(4.00) | kinetic_energy: 1.49e+00 | jacobian_norm2: 1.83e+00
Itr 001304 | Wall 6.842e+02(0.52) | Time/Itr 0.49(0.52) | BPD 0.71(0.72) | Loss 0.74(0.75) | FE 128(128) | Grad Norm 5.825e+00(5.488e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.68e+00
Itr 001305 | Wall 6.848e+02(0.52) | Time/Itr 0.58(0.52) | BPD 0.63(0.72) | Loss 0.66(0.75) | FE 128(128) | Grad Norm 5.478e+00(5.488e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 1.77e+00
Itr 001306 | Wall 6.853e+02(0.52) | Time/Itr 0.54(0.52) | BPD 0.67(0.72) | Loss 0.70(0.75) | FE 128(128) | Grad Norm 5.066e+00(5.475e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.79e+00
Itr 001307 | Wall 6.858e+02(0.52) | Time/Itr 0.51(0.52) | BPD 0.64(0.72) | Loss 0.68(0.75) | FE 128(128) | Grad Norm 4.118e+00(5.435e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 1.73e+00
Itr 001308 | Wall 6.863e+02(0.52) | Time/Itr 0.53(0.52) | BPD 0.67(0.72) | Loss 0.70(0.75) | FE 128(128) | Grad Norm 4.862e+00(5.417e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 1.76e+00
Itr 001309 | Wall 6.869e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.64(0.71) | Loss 0.67(0.74) | FE 128(128) | Grad Norm 4.777e+00(5.398e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.75e+00
Itr 001310 | Wall 6.874e+02(0.52) | Time/Itr 0.52(0.52) | BPD 0.68(0.71) | Loss 0.71(0.74) | FE 128(128) | Grad Norm 5.667e+00(5.406e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 1.70e+00
Itr 001311 | Wall 6.879e+02(0.52) | Time/Itr 0.56(0.52) | BPD 0.64(0.71) | Loss 0.68(0.74) | FE 128(128) | Grad Norm 5.828e+00(5.419e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 1.75e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    block = 2
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt.pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 7.747e-01(0.77) | Time/Itr 0.77(0.77) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt.pth"),
                                        os.path.join(args.save, "best.pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 7.525e-01(0.75) | Time/Itr 0.75(0.75) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
Itr 000100 | Wall 5.478e+01(0.54) | Time/Itr 0.56(0.55) | BPD 2.23(3.50) | Loss 2.24(3.50) | FE 128(128) | Grad Norm 1.788e+00(1.288e+01) | TT 4.00(4.00) | kinetic_energy: 9.10e-01 | jacobian_norm2: 2.14e-05
Itr 000200 | Wall 1.087e+02(0.54) | Time/Itr 0.53(0.54) | BPD 1.82(2.00) | Loss 1.83(2.02) | FE 128(128) | Grad Norm 6.313e-01(1.656e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 9.97e-05
Itr 000300 | Wall 1.627e+02(0.54) | Time/Itr 0.55(0.54) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.865e-01(2.891e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.79e-04
Itr 000400 | Wall 2.166e+02(0.54) | Time/Itr 0.55(0.54) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.304e-01(2.489e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.55e-04
Itr 000500 | Wall 2.706e+02(0.54) | Time/Itr 0.54(0.54) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.943e-01(4.346e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.61e-03
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                length_of_trainloader = len(train_loader.dataset)
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=64, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=16, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss")
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=128, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 2.438e+00(2.44) | Time/Itr 2.44(2.44) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=128, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 7.649e-01(0.76) | Time/Itr 0.76(0.76) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
Itr 000100 | Wall 5.436e+01(0.54) | Time/Itr 0.49(0.55) | BPD 2.23(3.50) | Loss 2.24(3.50) | FE 128(128) | Grad Norm 1.788e+00(1.288e+01) | TT 4.00(4.00) | kinetic_energy: 9.10e-01 | jacobian_norm2: 2.14e-05
Itr 000200 | Wall 1.078e+02(0.54) | Time/Itr 0.51(0.53) | BPD 1.82(2.00) | Loss 1.83(2.02) | FE 128(128) | Grad Norm 6.313e-01(1.656e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 9.97e-05
Itr 000300 | Wall 1.612e+02(0.54) | Time/Itr 0.53(0.53) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.865e-01(2.891e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 2.79e-04
Itr 000400 | Wall 2.146e+02(0.54) | Time/Itr 0.51(0.53) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 2.304e-01(2.489e-01) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 6.55e-04
Itr 000500 | Wall 2.684e+02(0.54) | Time/Itr 0.55(0.54) | BPD 1.76(1.77) | Loss 1.77(1.78) | FE 128(128) | Grad Norm 7.943e-01(4.346e-01) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.61e-03
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=128, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 7.951e-01(0.80) | Time/Itr 0.80(0.80) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=16,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=16,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 329
Iters per test: 141
Itr 000000 | Wall 7.887e-01(0.79) | Time/Itr 0.79(0.79) | BPD 11.79(11.79) | Loss 11.79(11.79) | FE 128(128) | Grad Norm 5.528e+01(5.528e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=100, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 750
Itr 000000 | Wall 3.937e+00(3.94) | Time/Itr 3.94(3.94) | BPD 18.34(18.34) | Loss 18.34(18.34) | FE 128(128) | Grad Norm 7.407e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 750
Itr 000000 | Wall 2.157e+00(2.16) | Time/Itr 2.16(2.16) | BPD 18.34(18.34) | Loss 18.34(18.34) | FE 128(128) | Grad Norm 7.407e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
Itr 000001 | Wall 3.896e+00(1.95) | Time/Itr 1.74(2.14) | BPD 18.33(18.34) | Loss 18.33(18.34) | FE 128(128) | Grad Norm 7.405e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 7.94e-13
Itr 000002 | Wall 7.217e+00(2.41) | Time/Itr 3.32(2.18) | BPD 18.30(18.34) | Loss 18.30(18.34) | FE 128(128) | Grad Norm 7.403e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.37e-03 | jacobian_norm2: 7.12e-12
Itr 000003 | Wall 9.896e+00(2.47) | Time/Itr 2.68(2.20) | BPD 18.27(18.33) | Loss 18.27(18.33) | FE 128(128) | Grad Norm 7.400e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.37e-03 | jacobian_norm2: 2.84e-11
Itr 000004 | Wall 1.274e+01(2.55) | Time/Itr 2.85(2.21) | BPD 18.22(18.33) | Loss 18.22(18.33) | FE 128(128) | Grad Norm 7.398e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.39e-03 | jacobian_norm2: 7.99e-11
Itr 000005 | Wall 1.547e+01(2.58) | Time/Itr 2.72(2.23) | BPD 18.16(18.33) | Loss 18.16(18.33) | FE 128(128) | Grad Norm 7.396e+01(7.406e+01) | TT 4.00(4.00) | kinetic_energy: 1.42e-03 | jacobian_norm2: 1.81e-10
Itr 000006 | Wall 1.850e+01(2.64) | Time/Itr 3.03(2.25) | BPD 18.09(18.32) | Loss 18.09(18.32) | FE 128(128) | Grad Norm 7.392e+01(7.406e+01) | TT 4.00(4.00) | kinetic_energy: 1.46e-03 | jacobian_norm2: 3.53e-10
Itr 000007 | Wall 2.189e+01(2.74) | Time/Itr 3.39(2.29) | BPD 18.00(18.31) | Loss 18.00(18.31) | FE 128(128) | Grad Norm 7.387e+01(7.405e+01) | TT 4.00(4.00) | kinetic_energy: 1.54e-03 | jacobian_norm2: 6.19e-10
Itr 000008 | Wall 2.332e+01(2.59) | Time/Itr 1.43(2.26) | BPD 17.90(18.30) | Loss 17.90(18.30) | FE 128(128) | Grad Norm 7.382e+01(7.405e+01) | TT 4.00(4.00) | kinetic_energy: 1.66e-03 | jacobian_norm2: 1.06e-09
Itr 000009 | Wall 2.665e+01(2.66) | Time/Itr 3.33(2.29) | BPD 17.79(18.28) | Loss 17.79(18.28) | FE 128(128) | Grad Norm 7.377e+01(7.404e+01) | TT 4.00(4.00) | kinetic_energy: 1.83e-03 | jacobian_norm2: 1.69e-09
Itr 000010 | Wall 2.987e+01(2.72) | Time/Itr 3.22(2.32) | BPD 17.66(18.26) | Loss 17.66(18.26) | FE 128(128) | Grad Norm 7.371e+01(7.403e+01) | TT 4.00(4.00) | kinetic_energy: 2.07e-03 | jacobian_norm2: 2.51e-09
Itr 000011 | Wall 3.308e+01(2.76) | Time/Itr 3.20(2.35) | BPD 17.53(18.24) | Loss 17.53(18.24) | FE 128(128) | Grad Norm 7.366e+01(7.402e+01) | TT 4.00(4.00) | kinetic_energy: 2.38e-03 | jacobian_norm2: 3.67e-09
Itr 000012 | Wall 3.621e+01(2.79) | Time/Itr 3.14(2.37) | BPD 17.38(18.21) | Loss 17.38(18.21) | FE 128(128) | Grad Norm 7.359e+01(7.400e+01) | TT 4.00(4.00) | kinetic_energy: 2.79e-03 | jacobian_norm2: 5.13e-09
Itr 000013 | Wall 3.870e+01(2.76) | Time/Itr 2.49(2.38) | BPD 17.22(18.19) | Loss 17.22(18.19) | FE 128(128) | Grad Norm 7.353e+01(7.399e+01) | TT 4.00(4.00) | kinetic_energy: 3.32e-03 | jacobian_norm2: 7.22e-09
Itr 000014 | Wall 4.216e+01(2.81) | Time/Itr 3.46(2.41) | BPD 17.05(18.15) | Loss 17.05(18.15) | FE 128(128) | Grad Norm 7.347e+01(7.397e+01) | TT 4.00(4.00) | kinetic_energy: 4.00e-03 | jacobian_norm2: 9.69e-09
Itr 000015 | Wall 4.504e+01(2.82) | Time/Itr 2.88(2.42) | BPD 16.86(18.11) | Loss 16.86(18.11) | FE 128(128) | Grad Norm 7.339e+01(7.396e+01) | TT 4.00(4.00) | kinetic_energy: 4.84e-03 | jacobian_norm2: 1.32e-08
Itr 000016 | Wall 4.858e+01(2.86) | Time/Itr 3.54(2.46) | BPD 16.66(18.07) | Loss 16.66(18.07) | FE 128(128) | Grad Norm 7.331e+01(7.394e+01) | TT 4.00(4.00) | kinetic_energy: 5.87e-03 | jacobian_norm2: 1.75e-08
Itr 000017 | Wall 5.203e+01(2.89) | Time/Itr 3.44(2.49) | BPD 16.44(18.02) | Loss 16.45(18.02) | FE 128(128) | Grad Norm 7.323e+01(7.392e+01) | TT 4.00(4.00) | kinetic_energy: 7.13e-03 | jacobian_norm2: 2.29e-08
Itr 000018 | Wall 5.504e+01(2.90) | Time/Itr 3.01(2.50) | BPD 16.22(17.97) | Loss 16.22(17.97) | FE 128(128) | Grad Norm 7.313e+01(7.389e+01) | TT 4.00(4.00) | kinetic_energy: 8.65e-03 | jacobian_norm2: 2.89e-08
Itr 000019 | Wall 5.788e+01(2.89) | Time/Itr 2.85(2.51) | BPD 15.97(17.91) | Loss 15.97(17.91) | FE 128(128) | Grad Norm 7.302e+01(7.387e+01) | TT 4.00(4.00) | kinetic_energy: 1.05e-02 | jacobian_norm2: 3.77e-08
Itr 000020 | Wall 6.043e+01(2.88) | Time/Itr 2.55(2.51) | BPD 15.72(17.84) | Loss 15.72(17.84) | FE 128(128) | Grad Norm 7.291e+01(7.384e+01) | TT 4.00(4.00) | kinetic_energy: 1.27e-02 | jacobian_norm2: 4.77e-08
Itr 000021 | Wall 6.355e+01(2.89) | Time/Itr 3.12(2.53) | BPD 15.45(17.77) | Loss 15.45(17.77) | FE 128(128) | Grad Norm 7.278e+01(7.381e+01) | TT 4.00(4.00) | kinetic_energy: 1.52e-02 | jacobian_norm2: 6.03e-08
Itr 000022 | Wall 6.694e+01(2.91) | Time/Itr 3.39(2.56) | BPD 15.17(17.69) | Loss 15.17(17.69) | FE 128(128) | Grad Norm 7.264e+01(7.377e+01) | TT 4.00(4.00) | kinetic_energy: 1.83e-02 | jacobian_norm2: 7.55e-08
Itr 000023 | Wall 6.989e+01(2.91) | Time/Itr 2.95(2.57) | BPD 14.87(17.61) | Loss 14.87(17.61) | FE 128(128) | Grad Norm 7.248e+01(7.373e+01) | TT 4.00(4.00) | kinetic_energy: 2.18e-02 | jacobian_norm2: 9.60e-08
Itr 000024 | Wall 7.272e+01(2.91) | Time/Itr 2.83(2.58) | BPD 14.56(17.51) | Loss 14.56(17.52) | FE 128(128) | Grad Norm 7.231e+01(7.369e+01) | TT 4.00(4.00) | kinetic_energy: 2.60e-02 | jacobian_norm2: 1.20e-07
Itr 000025 | Wall 7.578e+01(2.91) | Time/Itr 3.06(2.59) | BPD 14.24(17.42) | Loss 14.24(17.42) | FE 128(128) | Grad Norm 7.209e+01(7.364e+01) | TT 4.00(4.00) | kinetic_energy: 3.08e-02 | jacobian_norm2: 1.51e-07
Itr 000026 | Wall 7.917e+01(2.93) | Time/Itr 3.40(2.61) | BPD 13.89(17.31) | Loss 13.89(17.31) | FE 128(128) | Grad Norm 7.184e+01(7.359e+01) | TT 4.00(4.00) | kinetic_energy: 3.64e-02 | jacobian_norm2: 1.87e-07
Itr 000027 | Wall 8.184e+01(2.92) | Time/Itr 2.67(2.62) | BPD 13.54(17.20) | Loss 13.54(17.20) | FE 128(128) | Grad Norm 7.157e+01(7.353e+01) | TT 4.00(4.00) | kinetic_energy: 4.28e-02 | jacobian_norm2: 2.37e-07
Itr 000028 | Wall 8.326e+01(2.87) | Time/Itr 1.42(2.58) | BPD 13.16(17.08) | Loss 13.16(17.08) | FE 128(128) | Grad Norm 7.125e+01(7.346e+01) | TT 4.00(4.00) | kinetic_energy: 5.03e-02 | jacobian_norm2: 3.04e-07
Itr 000029 | Wall 8.667e+01(2.89) | Time/Itr 3.41(2.61) | BPD 12.77(16.95) | Loss 12.77(16.95) | FE 128(128) | Grad Norm 7.087e+01(7.338e+01) | TT 4.00(4.00) | kinetic_energy: 5.88e-02 | jacobian_norm2: 3.71e-07
Itr 000030 | Wall 8.932e+01(2.88) | Time/Itr 2.64(2.61) | BPD 12.38(16.81) | Loss 12.38(16.81) | FE 128(128) | Grad Norm 7.046e+01(7.329e+01) | TT 4.00(4.00) | kinetic_energy: 6.86e-02 | jacobian_norm2: 4.81e-07
Itr 000031 | Wall 9.243e+01(2.89) | Time/Itr 3.11(2.62) | BPD 11.96(16.66) | Loss 11.96(16.66) | FE 128(128) | Grad Norm 6.998e+01(7.319e+01) | TT 4.00(4.00) | kinetic_energy: 7.99e-02 | jacobian_norm2: 5.81e-07
Itr 000032 | Wall 9.595e+01(2.91) | Time/Itr 3.52(2.65) | BPD 11.53(16.51) | Loss 11.53(16.51) | FE 128(128) | Grad Norm 6.943e+01(7.308e+01) | TT 4.00(4.00) | kinetic_energy: 9.27e-02 | jacobian_norm2: 7.48e-07
Itr 000033 | Wall 9.762e+01(2.87) | Time/Itr 1.68(2.62) | BPD 11.08(16.35) | Loss 11.09(16.35) | FE 128(128) | Grad Norm 6.879e+01(7.295e+01) | TT 4.00(4.00) | kinetic_energy: 1.07e-01 | jacobian_norm2: 9.27e-07
Itr 000034 | Wall 1.007e+02(2.88) | Time/Itr 3.11(2.63) | BPD 10.62(16.18) | Loss 10.63(16.18) | FE 128(128) | Grad Norm 6.804e+01(7.280e+01) | TT 4.00(4.00) | kinetic_energy: 1.24e-01 | jacobian_norm2: 1.17e-06
Itr 000035 | Wall 1.033e+02(2.87) | Time/Itr 2.58(2.63) | BPD 10.15(16.00) | Loss 10.16(16.00) | FE 128(128) | Grad Norm 6.720e+01(7.264e+01) | TT 4.00(4.00) | kinetic_energy: 1.43e-01 | jacobian_norm2: 1.49e-06
Itr 000036 | Wall 1.064e+02(2.88) | Time/Itr 3.10(2.65) | BPD 9.67(15.81) | Loss 9.67(15.81) | FE 128(128) | Grad Norm 6.622e+01(7.244e+01) | TT 4.00(4.00) | kinetic_energy: 1.65e-01 | jacobian_norm2: 1.83e-06
Itr 000037 | Wall 1.096e+02(2.88) | Time/Itr 3.16(2.66) | BPD 9.18(15.61) | Loss 9.18(15.61) | FE 128(128) | Grad Norm 6.510e+01(7.222e+01) | TT 4.00(4.00) | kinetic_energy: 1.89e-01 | jacobian_norm2: 2.47e-06
Itr 000038 | Wall 1.128e+02(2.89) | Time/Itr 3.21(2.68) | BPD 8.68(15.40) | Loss 8.68(15.40) | FE 128(128) | Grad Norm 6.384e+01(7.197e+01) | TT 4.00(4.00) | kinetic_energy: 2.17e-01 | jacobian_norm2: 2.97e-06
Itr 000039 | Wall 1.157e+02(2.89) | Time/Itr 2.87(2.68) | BPD 8.17(15.18) | Loss 8.17(15.18) | FE 128(128) | Grad Norm 6.237e+01(7.168e+01) | TT 4.00(4.00) | kinetic_energy: 2.49e-01 | jacobian_norm2: 3.84e-06
Itr 000040 | Wall 1.186e+02(2.89) | Time/Itr 2.92(2.69) | BPD 7.66(14.96) | Loss 7.66(14.96) | FE 128(128) | Grad Norm 6.071e+01(7.136e+01) | TT 4.00(4.00) | kinetic_energy: 2.84e-01 | jacobian_norm2: 4.84e-06
validating...
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        print("shapes: ",x.shape,z.shape, out.shape)
                        dist = (x.view(x.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=3, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 750
validating...
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        print("shapes: ",x.shape,z.shape, out.shape)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt"+ str(block) +".pth"),
                                        os.path.join(args.save, "best"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
validating...
Epoch 0001 | Time 8.0640, Bit/dim 4.3565, Steps 56.0000, TT 4.00, Transport Cost 7.30e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_"+ str(block) +".pth"),
                                        os.path.join(args.save, "best_"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
validating...
Epoch 0001 | Time 7.6028, Bit/dim 4.3565, Steps 56.0000, TT 4.00, Transport Cost 7.30e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_"+ str(block) +".pth"),
                                        os.path.join(args.save, "best_"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    print(type(fixed_z))
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
validating...
Epoch 0001 | Time 7.5376, Bit/dim 4.3565, Steps 56.0000, TT 4.00, Transport Cost 7.30e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    print("args.local_rank", args.local_rank)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        print("one epoch completed ...")
        model.eval()
        if args.local_rank == 0:
            print("saving model")
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            print("validation started ")
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_"+ str(block) +".pth"),
                                        os.path.join(args.save, "best_"+ str(block) +".pth"))

            # visualize samples and density
            if write_log:
                with torch.no_grad():
                    fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    utils.makedirs(os.path.dirname(fig_filename))
                    print(fixed_z.size())
                    generated_samples, _, _ = model(fixed_z, reverse=True)
                    generated_samples = generated_samples.view(-1, *data_shape)
                    nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=10, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
validating...
Epoch 0001 | Time 7.4863, Bit/dim 4.3565, Steps 56.0000, TT 4.00, Transport Cost 7.30e+00
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        if args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_"+ str(block) +".pth"),
                                        os.path.join(args.save, "best_"+ str(block) +".pth"))

            # visualize samples and density
            #if write_log:
                #with torch.no_grad():
                    #fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    #utils.makedirs(os.path.dirname(fig_filename))
                    #generated_samples, _, _ = model(fixed_z, reverse=True)
                    #generated_samples = generated_samples.view(-1, *data_shape)
                    #nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    #save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=1, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=30, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
validating...
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss


SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 2
downscale_factor=2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])
    
    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)
    
    def fast_collate(batch):
        
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]
        
        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32())/255.0
            tensor[i] += torch.from_numpy(nump_array)
        
        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32())/255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    #print("number of blocks: ", args.num_blocks)
    #print("hidden_dims: ", hidden_dims)
    #print("div_samples", args.div_samples)
    #print("strides ", strides)
    #print("squeeze_first ", args.squeeze_first)
    #print("non linearity ", args.nonlinearity)
    #print("layer_type ", args.layer_type)
    #print("zero_last ", args.zero_last)
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d"%torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))
  
    # get deivce
    device = torch.device("cuda:%d"%torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    #device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ",epoch)
        itr = 0
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)
                
                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)
                    
                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd
                    
                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)
                    
                    
                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_"+str(block)+".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)
                        
                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)
            
                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )
            
                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]
                        
                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_"+ str(block) +".pth"),
                                        os.path.join(args.save, "best_"+ str(block) +".pth"))

            # visualize samples and density
            #if write_log:
                #with torch.no_grad():
                    #fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
                    #utils.makedirs(os.path.dirname(fig_filename))
                    #generated_samples, _, _ = model(fixed_z, reverse=True)
                    #generated_samples = generated_samples.view(-1, *data_shape)
                    #nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
                    #save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=10, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=30, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 442032
Iters per train epoch: 11
Iters per test: 18
Itr 000000 | Wall 4.115e+00(4.11) | Time/Itr 4.11(4.11) | BPD 18.34(18.34) | Loss 18.34(18.34) | FE 128(128) | Grad Norm 7.407e+01(7.407e+01) | TT 4.00(4.00) | kinetic_energy: 1.36e-03 | jacobian_norm2: 0.00e+00
Itr 000010 | Wall 3.406e+01(3.10) | Time/Itr 3.47(3.82) | BPD 17.66(18.26) | Loss 17.66(18.26) | FE 128(128) | Grad Norm 7.371e+01(7.403e+01) | TT 4.00(4.00) | kinetic_energy: 2.07e-03 | jacobian_norm2: 2.51e-09
Itr 000020 | Wall 6.451e+01(3.07) | Time/Itr 3.10(3.61) | BPD 15.72(17.84) | Loss 15.72(17.84) | FE 128(128) | Grad Norm 7.291e+01(7.384e+01) | TT 4.00(4.00) | kinetic_energy: 1.27e-02 | jacobian_norm2: 4.77e-08
Itr 000030 | Wall 9.418e+01(3.04) | Time/Itr 2.74(3.44) | BPD 12.38(16.81) | Loss 12.38(16.81) | FE 128(128) | Grad Norm 7.046e+01(7.329e+01) | TT 4.00(4.00) | kinetic_energy: 6.86e-02 | jacobian_norm2: 4.81e-07
Itr 000040 | Wall 1.241e+02(3.03) | Time/Itr 2.31(3.32) | BPD 7.66(14.96) | Loss 7.66(14.96) | FE 128(128) | Grad Norm 6.071e+01(7.136e+01) | TT 4.00(4.00) | kinetic_energy: 2.84e-01 | jacobian_norm2: 4.84e-06
validating...
Epoch 0001 | Time 9.5543, Bit/dim 2.3075, Steps 80.0000, TT 4.00, Transport Cost 1.57e+00
Itr 000000 | Wall 1.276e+02(127.58) | Time/Itr 3.50(3.33) | BPD 7.14(14.72) | Loss 7.15(14.72) | FE 128(128) | Grad Norm 5.882e+01(7.098e+01) | TT 4.00(4.00) | kinetic_energy: 3.25e-01 | jacobian_norm2: 6.18e-06
Itr 000010 | Wall 1.596e+02(14.51) | Time/Itr 3.49(3.30) | BPD 6.50(12.66) | Loss 6.50(12.66) | FE 128(128) | Grad Norm 5.610e+01(6.749e+01) | TT 4.00(4.00) | kinetic_energy: 3.83e-01 | jacobian_norm2: 8.43e-06
Itr 000020 | Wall 1.886e+02(8.98) | Time/Itr 1.75(3.19) | BPD 4.91(10.81) | Loss 4.91(10.82) | FE 128(128) | Grad Norm 4.666e+01(6.326e+01) | TT 4.00(4.00) | kinetic_energy: 5.76e-01 | jacobian_norm2: 1.81e-05
Itr 000030 | Wall 2.188e+02(7.06) | Time/Itr 3.17(3.14) | BPD 3.22(9.00) | Loss 3.23(9.00) | FE 128(128) | Grad Norm 2.609e+01(5.607e+01) | TT 4.00(4.00) | kinetic_energy: 9.87e-01 | jacobian_norm2: 5.03e-05
Itr 000040 | Wall 2.499e+02(6.10) | Time/Itr 3.15(3.13) | BPD 2.80(7.38) | Loss 2.81(7.39) | FE 128(128) | Grad Norm 7.763e+00(4.453e+01) | TT 4.00(4.00) | kinetic_energy: 1.51e+00 | jacobian_norm2: 1.06e-04
validating...
Epoch 0002 | Time 9.4119, Bit/dim 1.9936, Steps 80.0000, TT 4.00, Transport Cost 6.52e-01
Itr 000000 | Wall 2.526e+02(252.58) | Time/Itr 2.68(3.12) | BPD 2.82(7.25) | Loss 2.83(7.25) | FE 128(128) | Grad Norm 9.316e+00(4.347e+01) | TT 4.00(4.00) | kinetic_energy: 1.55e+00 | jacobian_norm2: 1.13e-04
Itr 000010 | Wall 2.832e+02(25.75) | Time/Itr 2.28(3.10) | BPD 2.80(6.08) | Loss 2.81(6.09) | FE 128(128) | Grad Norm 9.093e+00(3.452e+01) | TT 4.00(4.00) | kinetic_energy: 1.53e+00 | jacobian_norm2: 1.09e-04
Itr 000020 | Wall 3.150e+02(15.00) | Time/Itr 2.45(3.12) | BPD 2.72(5.21) | Loss 2.73(5.22) | FE 128(128) | Grad Norm 4.991e+00(2.729e+01) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 9.62e-05
Itr 000030 | Wall 3.466e+02(11.18) | Time/Itr 2.67(3.13) | BPD 2.67(4.55) | Loss 2.68(4.56) | FE 128(128) | Grad Norm 3.836e+00(2.108e+01) | TT 4.00(4.00) | kinetic_energy: 1.26e+00 | jacobian_norm2: 7.45e-05
Itr 000040 | Wall 3.766e+02(9.19) | Time/Itr 3.31(3.10) | BPD 2.65(4.05) | Loss 2.66(4.06) | FE 128(128) | Grad Norm 4.868e+00(1.678e+01) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 6.69e-05
validating...
Epoch 0003 | Time 9.4338, Bit/dim 1.9201, Steps 80.0000, TT 4.00, Transport Cost 4.64e-01
Itr 000000 | Wall 3.801e+02(380.05) | Time/Itr 3.46(3.11) | BPD 2.64(4.01) | Loss 2.65(4.02) | FE 128(128) | Grad Norm 4.774e+00(1.642e+01) | TT 4.00(4.00) | kinetic_energy: 1.22e+00 | jacobian_norm2: 6.88e-05
Itr 000010 | Wall 4.111e+02(37.37) | Time/Itr 3.02(3.11) | BPD 2.63(3.65) | Loss 2.65(3.66) | FE 128(128) | Grad Norm 4.457e+00(1.333e+01) | TT 4.00(4.00) | kinetic_energy: 1.23e+00 | jacobian_norm2: 6.68e-05
Itr 000020 | Wall 4.416e+02(21.03) | Time/Itr 3.22(3.10) | BPD 2.62(3.38) | Loss 2.63(3.39) | FE 128(128) | Grad Norm 3.547e+00(1.087e+01) | TT 4.00(4.00) | kinetic_energy: 1.27e+00 | jacobian_norm2: 7.18e-05
Itr 000030 | Wall 4.714e+02(15.21) | Time/Itr 3.46(3.07) | BPD 2.59(3.18) | Loss 2.61(3.19) | FE 128(128) | Grad Norm 2.973e+00(8.843e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 6.86e-05
Itr 000040 | Wall 5.019e+02(12.24) | Time/Itr 2.93(3.06) | BPD 2.56(3.02) | Loss 2.58(3.03) | FE 128(128) | Grad Norm 2.884e+00(7.282e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 6.48e-05
validating...
Epoch 0004 | Time 9.4188, Bit/dim 1.9019, Steps 80.0000, TT 4.00, Transport Cost 4.09e-01
Itr 000000 | Wall 5.054e+02(505.39) | Time/Itr 3.47(3.08) | BPD 2.56(3.00) | Loss 2.57(3.02) | FE 128(128) | Grad Norm 2.887e+00(7.150e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 6.61e-05
Itr 000010 | Wall 5.362e+02(48.74) | Time/Itr 2.33(3.07) | BPD 2.56(2.89) | Loss 2.57(2.90) | FE 128(128) | Grad Norm 2.906e+00(6.034e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 6.67e-05
Itr 000020 | Wall 5.667e+02(26.99) | Time/Itr 3.39(3.07) | BPD 2.54(2.80) | Loss 2.56(2.81) | FE 128(128) | Grad Norm 2.965e+00(5.222e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 6.12e-05
Itr 000030 | Wall 5.968e+02(19.25) | Time/Itr 2.78(3.05) | BPD 2.52(2.73) | Loss 2.54(2.74) | FE 128(128) | Grad Norm 2.987e+00(4.636e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 5.97e-05
Itr 000040 | Wall 6.283e+02(15.32) | Time/Itr 2.92(3.07) | BPD 2.50(2.67) | Loss 2.51(2.68) | FE 128(128) | Grad Norm 2.852e+00(4.185e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 5.63e-05
validating...
Epoch 0005 | Time 9.5345, Bit/dim 1.8882, Steps 80.0000, TT 4.00, Transport Cost 3.72e-01
Itr 000000 | Wall 6.317e+02(631.73) | Time/Itr 3.48(3.09) | BPD 2.49(2.67) | Loss 2.51(2.68) | FE 128(128) | Grad Norm 2.840e+00(4.144e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 5.71e-05
Itr 000010 | Wall 6.615e+02(60.14) | Time/Itr 2.71(3.05) | BPD 2.49(2.62) | Loss 2.50(2.63) | FE 128(128) | Grad Norm 2.810e+00(3.798e+00) | TT 4.00(4.00) | kinetic_energy: 1.29e+00 | jacobian_norm2: 5.93e-05
Itr 000020 | Wall 6.920e+02(32.95) | Time/Itr 2.81(3.05) | BPD 2.48(2.58) | Loss 2.49(2.60) | FE 128(128) | Grad Norm 2.760e+00(3.532e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 5.50e-05
Itr 000030 | Wall 7.226e+02(23.31) | Time/Itr 2.95(3.05) | BPD 2.46(2.55) | Loss 2.47(2.57) | FE 128(128) | Grad Norm 2.703e+00(3.322e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 5.60e-05
Itr 000040 | Wall 7.530e+02(18.37) | Time/Itr 3.01(3.05) | BPD 2.44(2.53) | Loss 2.45(2.54) | FE 128(128) | Grad Norm 2.642e+00(3.151e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 5.56e-05
validating...
Epoch 0006 | Time 9.4984, Bit/dim 1.8756, Steps 80.0000, TT 4.00, Transport Cost 3.36e-01
Itr 000000 | Wall 7.565e+02(756.47) | Time/Itr 3.48(3.06) | BPD 2.43(2.52) | Loss 2.44(2.54) | FE 128(128) | Grad Norm 2.633e+00(3.135e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 5.13e-05
Itr 000010 | Wall 7.878e+02(71.62) | Time/Itr 2.97(3.08) | BPD 2.43(2.50) | Loss 2.44(2.51) | FE 128(128) | Grad Norm 2.626e+00(3.004e+00) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 5.27e-05
Itr 000020 | Wall 8.174e+02(38.92) | Time/Itr 1.52(3.04) | BPD 2.42(2.48) | Loss 2.43(2.49) | FE 128(128) | Grad Norm 2.604e+00(2.902e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 5.36e-05
Itr 000030 | Wall 8.480e+02(27.35) | Time/Itr 2.55(3.05) | BPD 2.40(2.46) | Loss 2.41(2.47) | FE 128(128) | Grad Norm 2.565e+00(2.817e+00) | TT 4.00(4.00) | kinetic_energy: 1.31e+00 | jacobian_norm2: 5.02e-05
Itr 000040 | Wall 8.787e+02(21.43) | Time/Itr 2.83(3.05) | BPD 2.38(2.44) | Loss 2.39(2.45) | FE 128(128) | Grad Norm 2.504e+00(2.742e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.93e-05
validating...
Epoch 0007 | Time 9.5405, Bit/dim 1.8634, Steps 80.0000, TT 4.00, Transport Cost 3.03e-01
Itr 000000 | Wall 8.822e+02(882.16) | Time/Itr 3.45(3.07) | BPD 2.38(2.44) | Loss 2.39(2.45) | FE 128(128) | Grad Norm 2.491e+00(2.734e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.82e-05
Itr 000010 | Wall 9.123e+02(82.94) | Time/Itr 2.73(3.05) | BPD 2.37(2.42) | Loss 2.39(2.44) | FE 128(128) | Grad Norm 2.488e+00(2.671e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.89e-05
Itr 000020 | Wall 9.424e+02(44.88) | Time/Itr 3.31(3.04) | BPD 2.36(2.41) | Loss 2.38(2.42) | FE 128(128) | Grad Norm 2.463e+00(2.620e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.80e-05
Itr 000030 | Wall 9.730e+02(31.39) | Time/Itr 3.52(3.05) | BPD 2.35(2.39) | Loss 2.36(2.41) | FE 128(128) | Grad Norm 2.431e+00(2.574e+00) | TT 4.00(4.00) | kinetic_energy: 1.32e+00 | jacobian_norm2: 4.57e-05
Itr 000040 | Wall 1.003e+03(24.46) | Time/Itr 2.81(3.03) | BPD 2.32(2.38) | Loss 2.34(2.39) | FE 128(128) | Grad Norm 2.369e+00(2.528e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.50e-05
validating...
Epoch 0008 | Time 9.4328, Bit/dim 1.8516, Steps 80.0000, TT 4.00, Transport Cost 2.69e-01
Itr 000000 | Wall 1.006e+03(1006.19) | Time/Itr 3.42(3.04) | BPD 2.32(2.38) | Loss 2.34(2.39) | FE 128(128) | Grad Norm 2.366e+00(2.523e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.51e-05
Itr 000010 | Wall 1.036e+03(94.17) | Time/Itr 2.75(3.02) | BPD 2.32(2.36) | Loss 2.33(2.38) | FE 128(128) | Grad Norm 2.353e+00(2.481e+00) | TT 4.00(4.00) | kinetic_energy: 1.33e+00 | jacobian_norm2: 4.74e-05
Itr 000020 | Wall 1.066e+03(50.75) | Time/Itr 3.35(3.01) | BPD 2.31(2.35) | Loss 2.32(2.36) | FE 128(128) | Grad Norm 2.340e+00(2.446e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 4.47e-05
Itr 000030 | Wall 1.097e+03(35.37) | Time/Itr 3.33(3.03) | BPD 2.29(2.34) | Loss 2.31(2.35) | FE 128(128) | Grad Norm 2.304e+00(2.414e+00) | TT 4.00(4.00) | kinetic_energy: 1.34e+00 | jacobian_norm2: 4.32e-05
Itr 000040 | Wall 1.127e+03(27.48) | Time/Itr 3.32(3.03) | BPD 2.27(2.32) | Loss 2.29(2.34) | FE 128(128) | Grad Norm 2.250e+00(2.377e+00) | TT 4.00(4.00) | kinetic_energy: 1.35e+00 | jacobian_norm2: 4.29e-05
validating...
Epoch 0009 | Time 9.4941, Bit/dim 1.8398, Steps 80.0000, TT 4.00, Transport Cost 2.37e-01
Itr 000000 | Wall 1.130e+03(1130.24) | Time/Itr 3.39(3.04) | BPD 2.27(2.32) | Loss 2.28(2.33) | FE 128(128) | Grad Norm 2.246e+00(2.373e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 4.32e-05
Itr 000010 | Wall 1.162e+03(105.65) | Time/Itr 3.45(3.08) | BPD 2.27(2.31) | Loss 2.28(2.32) | FE 128(128) | Grad Norm 2.225e+00(2.337e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 4.17e-05
Itr 000020 | Wall 1.191e+03(56.70) | Time/Itr 3.06(3.03) | BPD 2.26(2.30) | Loss 2.27(2.31) | FE 128(128) | Grad Norm 2.213e+00(2.307e+00) | TT 4.00(4.00) | kinetic_energy: 1.36e+00 | jacobian_norm2: 4.27e-05
Itr 000030 | Wall 1.221e+03(39.39) | Time/Itr 3.48(3.04) | BPD 2.24(2.28) | Loss 2.26(2.30) | FE 128(128) | Grad Norm 2.185e+00(2.277e+00) | TT 4.00(4.00) | kinetic_energy: 1.37e+00 | jacobian_norm2: 4.36e-05
Itr 000040 | Wall 1.251e+03(30.52) | Time/Itr 3.08(3.03) | BPD 2.22(2.27) | Loss 2.24(2.28) | FE 128(128) | Grad Norm 2.120e+00(2.242e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 4.17e-05
validating...
Epoch 0010 | Time 9.5013, Bit/dim 1.8285, Steps 80.0000, TT 4.00, Transport Cost 2.05e-01
Itr 000000 | Wall 1.255e+03(1254.60) | Time/Itr 3.48(3.04) | BPD 2.22(2.27) | Loss 2.24(2.28) | FE 128(128) | Grad Norm 2.114e+00(2.239e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 4.28e-05
Itr 000010 | Wall 1.285e+03(116.78) | Time/Itr 2.61(3.02) | BPD 2.22(2.26) | Loss 2.23(2.27) | FE 128(128) | Grad Norm 2.103e+00(2.205e+00) | TT 4.00(4.00) | kinetic_energy: 1.38e+00 | jacobian_norm2: 4.12e-05
Itr 000020 | Wall 1.315e+03(62.60) | Time/Itr 3.00(3.02) | BPD 2.21(2.24) | Loss 2.22(2.26) | FE 128(128) | Grad Norm 2.083e+00(2.175e+00) | TT 4.00(4.00) | kinetic_energy: 1.39e+00 | jacobian_norm2: 4.17e-05
Itr 000030 | Wall 1.345e+03(43.39) | Time/Itr 2.46(3.03) | BPD 2.19(2.23) | Loss 2.21(2.25) | FE 128(128) | Grad Norm 2.041e+00(2.145e+00) | TT 4.00(4.00) | kinetic_energy: 1.40e+00 | jacobian_norm2: 4.16e-05
Itr 000040 | Wall 1.376e+03(33.55) | Time/Itr 2.27(3.03) | BPD 2.17(2.22) | Loss 2.19(2.23) | FE 128(128) | Grad Norm 1.995e+00(2.110e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 4.20e-05
validating...
Epoch 0011 | Time 9.5057, Bit/dim 1.8176, Steps 80.0000, TT 4.00, Transport Cost 1.74e-01
Itr 000000 | Wall 1.379e+03(1379.10) | Time/Itr 3.47(3.04) | BPD 2.17(2.22) | Loss 2.19(2.23) | FE 128(128) | Grad Norm 1.979e+00(2.106e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 4.15e-05
Itr 000010 | Wall 1.409e+03(128.10) | Time/Itr 3.53(3.03) | BPD 2.17(2.21) | Loss 2.18(2.22) | FE 128(128) | Grad Norm 1.977e+00(2.072e+00) | TT 4.00(4.00) | kinetic_energy: 1.41e+00 | jacobian_norm2: 4.36e-05
Itr 000020 | Wall 1.440e+03(68.56) | Time/Itr 3.48(3.04) | BPD 2.16(2.19) | Loss 2.17(2.21) | FE 128(128) | Grad Norm 1.943e+00(2.041e+00) | TT 4.00(4.00) | kinetic_energy: 1.42e+00 | jacobian_norm2: 4.34e-05
Itr 000030 | Wall 1.471e+03(47.46) | Time/Itr 3.45(3.07) | BPD 2.14(2.18) | Loss 2.16(2.20) | FE 128(128) | Grad Norm 1.910e+00(2.011e+00) | TT 4.00(4.00) | kinetic_energy: 1.43e+00 | jacobian_norm2: 4.27e-05
Itr 000040 | Wall 1.501e+03(36.61) | Time/Itr 3.24(3.05) | BPD 2.13(2.17) | Loss 2.14(2.18) | FE 128(128) | Grad Norm 1.842e+00(1.974e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 4.67e-05
validating...
Epoch 0012 | Time 9.6235, Bit/dim 1.8073, Steps 80.0000, TT 4.00, Transport Cost 1.45e-01
Itr 000000 | Wall 1.504e+03(1504.28) | Time/Itr 3.47(3.06) | BPD 2.12(2.17) | Loss 2.14(2.18) | FE 128(128) | Grad Norm 1.839e+00(1.970e+00) | TT 4.00(4.00) | kinetic_energy: 1.44e+00 | jacobian_norm2: 4.52e-05
Itr 000010 | Wall 1.534e+03(139.46) | Time/Itr 2.72(3.04) | BPD 2.12(2.16) | Loss 2.13(2.17) | FE 128(128) | Grad Norm 1.831e+00(1.936e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 4.47e-05
Itr 000020 | Wall 1.566e+03(74.57) | Time/Itr 3.17(3.08) | BPD 2.11(2.15) | Loss 2.13(2.16) | FE 128(128) | Grad Norm 1.809e+00(1.905e+00) | TT 4.00(4.00) | kinetic_energy: 1.45e+00 | jacobian_norm2: 4.58e-05
Itr 000030 | Wall 1.597e+03(51.51) | Time/Itr 3.20(3.08) | BPD 2.10(2.13) | Loss 2.11(2.15) | FE 128(128) | Grad Norm 1.767e+00(1.875e+00) | TT 4.00(4.00) | kinetic_energy: 1.46e+00 | jacobian_norm2: 4.72e-05
Itr 000040 | Wall 1.627e+03(39.68) | Time/Itr 2.69(3.06) | BPD 2.08(2.12) | Loss 2.09(2.14) | FE 128(128) | Grad Norm 1.706e+00(1.837e+00) | TT 4.00(4.00) | kinetic_energy: 1.48e+00 | jacobian_norm2: 5.09e-05
validating...
Epoch 0013 | Time 9.4676, Bit/dim 1.7977, Steps 80.0000, TT 4.00, Transport Cost 1.18e-01
Itr 000000 | Wall 1.630e+03(1630.32) | Time/Itr 3.43(3.07) | BPD 2.08(2.12) | Loss 2.09(2.14) | FE 128(128) | Grad Norm 1.700e+00(1.833e+00) | TT 4.00(4.00) | kinetic_energy: 1.48e+00 | jacobian_norm2: 4.93e-05
Itr 000010 | Wall 1.662e+03(151.13) | Time/Itr 2.74(3.10) | BPD 2.07(2.11) | Loss 2.09(2.12) | FE 128(128) | Grad Norm 1.683e+00(1.797e+00) | TT 4.00(4.00) | kinetic_energy: 1.48e+00 | jacobian_norm2: 5.05e-05
Itr 000020 | Wall 1.693e+03(80.62) | Time/Itr 2.99(3.09) | BPD 2.07(2.10) | Loss 2.08(2.11) | FE 128(128) | Grad Norm 1.666e+00(1.767e+00) | TT 4.00(4.00) | kinetic_energy: 1.49e+00 | jacobian_norm2: 5.14e-05
Itr 000030 | Wall 1.723e+03(55.57) | Time/Itr 3.17(3.06) | BPD 2.05(2.09) | Loss 2.07(2.10) | FE 128(128) | Grad Norm 1.626e+00(1.735e+00) | TT 4.00(4.00) | kinetic_energy: 1.50e+00 | jacobian_norm2: 5.53e-05
Itr 000040 | Wall 1.753e+03(42.75) | Time/Itr 2.81(3.04) | BPD 2.03(2.08) | Loss 2.05(2.09) | FE 128(128) | Grad Norm 1.569e+00(1.698e+00) | TT 4.00(4.00) | kinetic_energy: 1.51e+00 | jacobian_norm2: 5.84e-05
validating...
Epoch 0014 | Time 9.4739, Bit/dim 1.7891, Steps 80.0000, TT 4.00, Transport Cost 9.35e-02
Itr 000000 | Wall 1.756e+03(1756.22) | Time/Itr 3.47(3.06) | BPD 2.03(2.07) | Loss 2.05(2.09) | FE 128(128) | Grad Norm 1.560e+00(1.694e+00) | TT 4.00(4.00) | kinetic_energy: 1.51e+00 | jacobian_norm2: 5.92e-05
Itr 000010 | Wall 1.786e+03(162.38) | Time/Itr 2.98(3.04) | BPD 2.03(2.06) | Loss 2.05(2.08) | FE 128(128) | Grad Norm 1.547e+00(1.657e+00) | TT 4.00(4.00) | kinetic_energy: 1.51e+00 | jacobian_norm2: 5.79e-05
Itr 000020 | Wall 1.817e+03(86.50) | Time/Itr 2.77(3.04) | BPD 2.02(2.05) | Loss 2.04(2.07) | FE 128(128) | Grad Norm 1.527e+00(1.626e+00) | TT 4.00(4.00) | kinetic_energy: 1.52e+00 | jacobian_norm2: 6.08e-05
Itr 000030 | Wall 1.848e+03(59.60) | Time/Itr 3.21(3.06) | BPD 2.01(2.04) | Loss 2.03(2.06) | FE 128(128) | Grad Norm 1.479e+00(1.594e+00) | TT 4.00(4.00) | kinetic_energy: 1.53e+00 | jacobian_norm2: 6.43e-05
Itr 000040 | Wall 1.878e+03(45.80) | Time/Itr 3.18(3.05) | BPD 1.99(2.03) | Loss 2.01(2.05) | FE 128(128) | Grad Norm 1.422e+00(1.556e+00) | TT 4.00(4.00) | kinetic_energy: 1.54e+00 | jacobian_norm2: 6.98e-05
validating...
Epoch 0015 | Time 9.4560, Bit/dim 1.7815, Steps 80.0000, TT 4.00, Transport Cost 7.18e-02
Itr 000000 | Wall 1.881e+03(1881.23) | Time/Itr 3.43(3.06) | BPD 1.99(2.03) | Loss 2.01(2.05) | FE 128(128) | Grad Norm 1.422e+00(1.552e+00) | TT 4.00(4.00) | kinetic_energy: 1.55e+00 | jacobian_norm2: 7.09e-05
Itr 000010 | Wall 1.913e+03(173.94) | Time/Itr 3.54(3.10) | BPD 1.99(2.02) | Loss 2.00(2.04) | FE 128(128) | Grad Norm 1.407e+00(1.516e+00) | TT 4.00(4.00) | kinetic_energy: 1.55e+00 | jacobian_norm2: 7.13e-05
Itr 000020 | Wall 1.943e+03(92.53) | Time/Itr 3.16(3.07) | BPD 1.98(2.01) | Loss 2.00(2.03) | FE 128(128) | Grad Norm 1.389e+00(1.484e+00) | TT 4.00(4.00) | kinetic_energy: 1.55e+00 | jacobian_norm2: 7.47e-05
Itr 000030 | Wall 1.973e+03(63.64) | Time/Itr 3.11(3.04) | BPD 1.97(2.00) | Loss 1.99(2.02) | FE 128(128) | Grad Norm 1.335e+00(1.450e+00) | TT 4.00(4.00) | kinetic_energy: 1.56e+00 | jacobian_norm2: 7.75e-05
Itr 000040 | Wall 2.005e+03(48.89) | Time/Itr 3.12(3.08) | BPD 1.96(1.99) | Loss 1.97(2.01) | FE 128(128) | Grad Norm 1.272e+00(1.411e+00) | TT 4.00(4.00) | kinetic_energy: 1.58e+00 | jacobian_norm2: 8.55e-05
validating...
Epoch 0016 | Time 10.2436, Bit/dim 1.7751, Steps 86.0000, TT 4.00, Transport Cost 5.33e-02
Itr 000000 | Wall 2.008e+03(2008.04) | Time/Itr 3.54(3.09) | BPD 1.95(1.99) | Loss 1.97(2.01) | FE 128(128) | Grad Norm 1.265e+00(1.407e+00) | TT 4.00(4.00) | kinetic_energy: 1.58e+00 | jacobian_norm2: 8.67e-05
Itr 000010 | Wall 2.037e+03(185.14) | Time/Itr 2.91(3.03) | BPD 1.95(1.98) | Loss 1.97(2.00) | FE 128(128) | Grad Norm 1.262e+00(1.369e+00) | TT 4.00(4.00) | kinetic_energy: 1.58e+00 | jacobian_norm2: 8.62e-05
Itr 000020 | Wall 2.066e+03(98.40) | Time/Itr 2.89(3.01) | BPD 1.95(1.97) | Loss 1.96(1.99) | FE 128(128) | Grad Norm 1.231e+00(1.336e+00) | TT 4.00(4.00) | kinetic_energy: 1.58e+00 | jacobian_norm2: 9.03e-05
Itr 000030 | Wall 2.096e+03(67.62) | Time/Itr 2.73(3.01) | BPD 1.94(1.96) | Loss 1.95(1.98) | FE 128(128) | Grad Norm 1.183e+00(1.302e+00) | TT 4.00(4.00) | kinetic_energy: 1.59e+00 | jacobian_norm2: 9.81e-05
Itr 000040 | Wall 2.126e+03(51.86) | Time/Itr 2.79(3.00) | BPD 1.92(1.96) | Loss 1.94(1.97) | FE 128(128) | Grad Norm 1.123e+00(1.262e+00) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 1.04e-04
validating...
Epoch 0017 | Time 10.1887, Bit/dim 1.7699, Steps 86.0000, TT 4.00, Transport Cost 3.85e-02
Itr 000000 | Wall 2.130e+03(2129.53) | Time/Itr 3.45(3.01) | BPD 1.92(1.95) | Loss 1.94(1.97) | FE 128(128) | Grad Norm 1.103e+00(1.257e+00) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 1.07e-04
Itr 000010 | Wall 2.159e+03(196.24) | Time/Itr 2.95(2.99) | BPD 1.92(1.95) | Loss 1.94(1.96) | FE 128(128) | Grad Norm 1.107e+00(1.218e+00) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 1.09e-04
Itr 000020 | Wall 2.189e+03(104.24) | Time/Itr 3.25(3.00) | BPD 1.92(1.94) | Loss 1.93(1.95) | FE 128(128) | Grad Norm 1.077e+00(1.184e+00) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 1.13e-04
Itr 000030 | Wall 2.218e+03(71.56) | Time/Itr 3.01(2.99) | BPD 1.91(1.93) | Loss 1.92(1.95) | FE 128(128) | Grad Norm 1.023e+00(1.149e+00) | TT 4.00(4.00) | kinetic_energy: 1.62e+00 | jacobian_norm2: 1.22e-04
Itr 000040 | Wall 2.249e+03(54.86) | Time/Itr 2.90(3.00) | BPD 1.90(1.92) | Loss 1.91(1.94) | FE 128(128) | Grad Norm 9.611e-01(1.108e+00) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 1.35e-04
validating...
Epoch 0018 | Time 10.1872, Bit/dim 1.7662, Steps 86.0000, TT 4.00, Transport Cost 2.73e-02
Itr 000000 | Wall 2.253e+03(2252.55) | Time/Itr 3.47(3.02) | BPD 1.90(1.92) | Loss 1.91(1.94) | FE 128(128) | Grad Norm 9.510e-01(1.104e+00) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 1.31e-04
Itr 000010 | Wall 2.284e+03(207.62) | Time/Itr 2.84(3.04) | BPD 1.89(1.92) | Loss 1.91(1.93) | FE 128(128) | Grad Norm 9.468e-01(1.064e+00) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 1.34e-04
Itr 000020 | Wall 2.313e+03(110.15) | Time/Itr 2.87(3.01) | BPD 1.89(1.91) | Loss 1.91(1.93) | FE 128(128) | Grad Norm 9.214e-01(1.030e+00) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 1.40e-04
Itr 000030 | Wall 2.343e+03(75.58) | Time/Itr 3.07(3.00) | BPD 1.88(1.90) | Loss 1.90(1.92) | FE 128(128) | Grad Norm 8.788e-01(9.949e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 1.47e-04
Itr 000040 | Wall 2.373e+03(57.87) | Time/Itr 2.94(2.99) | BPD 1.88(1.90) | Loss 1.89(1.91) | FE 128(128) | Grad Norm 8.068e-01(9.541e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 1.55e-04
validating...
Epoch 0019 | Time 10.2534, Bit/dim 1.7635, Steps 86.0000, TT 4.00, Transport Cost 1.96e-02
Itr 000000 | Wall 2.376e+03(2375.94) | Time/Itr 3.39(3.00) | BPD 1.87(1.90) | Loss 1.89(1.91) | FE 128(128) | Grad Norm 8.085e-01(9.498e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 1.61e-04
Itr 000010 | Wall 2.406e+03(218.75) | Time/Itr 3.01(3.01) | BPD 1.87(1.89) | Loss 1.89(1.91) | FE 128(128) | Grad Norm 7.982e-01(9.110e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 1.60e-04
Itr 000020 | Wall 2.437e+03(116.06) | Time/Itr 3.16(3.03) | BPD 1.87(1.89) | Loss 1.89(1.90) | FE 128(128) | Grad Norm 7.718e-01(8.775e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 1.62e-04
Itr 000030 | Wall 2.468e+03(79.61) | Time/Itr 2.75(3.05) | BPD 1.87(1.88) | Loss 1.88(1.90) | FE 128(128) | Grad Norm 7.201e-01(8.429e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 1.74e-04
Itr 000040 | Wall 2.498e+03(60.94) | Time/Itr 3.28(3.04) | BPD 1.86(1.88) | Loss 1.88(1.89) | FE 128(128) | Grad Norm 6.623e-01(8.052e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 1.84e-04
validating...
Epoch 0020 | Time 10.1489, Bit/dim 1.7620, Steps 86.0000, TT 4.00, Transport Cost 1.49e-02
Itr 000000 | Wall 2.502e+03(2501.86) | Time/Itr 3.46(3.06) | BPD 1.86(1.88) | Loss 1.88(1.89) | FE 128(128) | Grad Norm 6.670e-01(8.011e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 1.84e-04
Itr 000010 | Wall 2.533e+03(230.30) | Time/Itr 2.57(3.08) | BPD 1.86(1.87) | Loss 1.88(1.89) | FE 128(128) | Grad Norm 6.560e-01(7.636e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 1.88e-04
Itr 000020 | Wall 2.565e+03(122.15) | Time/Itr 3.40(3.11) | BPD 1.86(1.87) | Loss 1.87(1.88) | FE 128(128) | Grad Norm 6.350e-01(7.333e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 1.90e-04
Itr 000030 | Wall 2.596e+03(83.73) | Time/Itr 3.44(3.09) | BPD 1.85(1.86) | Loss 1.87(1.88) | FE 128(128) | Grad Norm 5.936e-01(7.012e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 1.98e-04
Itr 000040 | Wall 2.625e+03(64.02) | Time/Itr 3.08(3.04) | BPD 1.85(1.86) | Loss 1.87(1.88) | FE 128(128) | Grad Norm 5.508e-01(6.679e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.04e-04
validating...
Epoch 0021 | Time 10.2266, Bit/dim 1.7611, Steps 86.0000, TT 4.00, Transport Cost 1.24e-02
Itr 000000 | Wall 2.628e+03(2628.10) | Time/Itr 3.46(3.05) | BPD 1.85(1.86) | Loss 1.86(1.88) | FE 128(128) | Grad Norm 5.373e-01(6.640e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.10e-04
Itr 000010 | Wall 2.657e+03(241.57) | Time/Itr 3.47(3.02) | BPD 1.85(1.86) | Loss 1.86(1.87) | FE 128(128) | Grad Norm 5.360e-01(6.309e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.10e-04
Itr 000020 | Wall 2.689e+03(128.02) | Time/Itr 3.06(3.05) | BPD 1.85(1.85) | Loss 1.86(1.87) | FE 128(128) | Grad Norm 5.160e-01(6.043e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.10e-04
Itr 000030 | Wall 2.717e+03(87.64) | Time/Itr 2.79(2.99) | BPD 1.84(1.85) | Loss 1.86(1.87) | FE 128(128) | Grad Norm 4.971e-01(5.773e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.16e-04
Itr 000040 | Wall 2.748e+03(67.02) | Time/Itr 2.80(3.02) | BPD 1.84(1.85) | Loss 1.86(1.87) | FE 128(128) | Grad Norm 4.601e-01(5.489e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.23e-04
validating...
Epoch 0022 | Time 10.2260, Bit/dim 1.7606, Steps 86.0000, TT 4.00, Transport Cost 1.11e-02
Itr 000000 | Wall 2.751e+03(2751.26) | Time/Itr 3.49(3.03) | BPD 1.84(1.85) | Loss 1.86(1.87) | FE 128(128) | Grad Norm 4.537e-01(5.461e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.18e-04
Itr 000010 | Wall 2.781e+03(252.81) | Time/Itr 2.67(3.02) | BPD 1.84(1.85) | Loss 1.86(1.86) | FE 128(128) | Grad Norm 4.486e-01(5.203e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.22e-04
Itr 000020 | Wall 2.810e+03(133.83) | Time/Itr 3.26(3.00) | BPD 1.84(1.85) | Loss 1.86(1.86) | FE 128(128) | Grad Norm 4.292e-01(4.976e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.18e-04
Itr 000030 | Wall 2.840e+03(91.62) | Time/Itr 3.51(3.00) | BPD 1.84(1.84) | Loss 1.85(1.86) | FE 128(128) | Grad Norm 4.120e-01(4.771e-01) | TT 4.00(4.00) | kinetic_energy: 1.68e+00 | jacobian_norm2: 2.22e-04
Itr 000040 | Wall 2.872e+03(70.04) | Time/Itr 3.43(3.03) | BPD 1.84(1.84) | Loss 1.85(1.86) | FE 128(128) | Grad Norm 3.801e-01(4.553e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.33e-04
validating...
Epoch 0023 | Time 10.1844, Bit/dim 1.7603, Steps 86.0000, TT 4.00, Transport Cost 1.05e-02
Itr 000000 | Wall 2.875e+03(2875.05) | Time/Itr 3.49(3.05) | BPD 1.84(1.84) | Loss 1.85(1.86) | FE 128(128) | Grad Norm 3.749e-01(4.529e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.25e-04
Itr 000010 | Wall 2.904e+03(264.01) | Time/Itr 3.12(3.02) | BPD 1.84(1.84) | Loss 1.85(1.86) | FE 128(128) | Grad Norm 3.828e-01(4.329e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.23e-04
Itr 000020 | Wall 2.936e+03(139.79) | Time/Itr 2.98(3.05) | BPD 1.84(1.84) | Loss 1.85(1.86) | FE 128(128) | Grad Norm 3.558e-01(4.163e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.29e-04
Itr 000030 | Wall 2.968e+03(95.73) | Time/Itr 2.95(3.08) | BPD 1.83(1.84) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.513e-01(4.010e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.30e-04
Itr 000040 | Wall 2.998e+03(73.13) | Time/Itr 2.21(3.08) | BPD 1.83(1.84) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.307e-01(3.850e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.32e-04
validating...
Epoch 0024 | Time 10.3691, Bit/dim 1.7600, Steps 89.1667, TT 4.00, Transport Cost 1.00e-02
Itr 000000 | Wall 3.002e+03(3001.90) | Time/Itr 3.49(3.09) | BPD 1.83(1.84) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.304e-01(3.833e-01) | TT 4.00(4.00) | kinetic_energy: 1.67e+00 | jacobian_norm2: 2.24e-04
Itr 000010 | Wall 3.033e+03(275.71) | Time/Itr 2.88(3.09) | BPD 1.83(1.84) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.303e-01(3.699e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 2.35e-04
Itr 000020 | Wall 3.062e+03(145.83) | Time/Itr 3.47(3.06) | BPD 1.83(1.83) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.127e-01(3.578e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 2.32e-04
Itr 000030 | Wall 3.094e+03(99.81) | Time/Itr 2.97(3.09) | BPD 1.83(1.83) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.035e-01(3.479e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 2.31e-04
Itr 000040 | Wall 3.124e+03(76.20) | Time/Itr 3.32(3.06) | BPD 1.83(1.83) | Loss 1.85(1.85) | FE 128(128) | Grad Norm 3.069e-01(3.364e-01) | TT 4.00(4.00) | kinetic_energy: 1.66e+00 | jacobian_norm2: 2.34e-04
validating...
Epoch 0025 | Time 10.8956, Bit/dim 1.7597, Steps 92.0000, TT 4.00, Transport Cost 9.62e-03
Itr 000000 | Wall 3.128e+03(3127.57) | Time/Itr 3.49(3.08) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 3.057e-01(3.355e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 2.28e-04
Itr 000010 | Wall 3.160e+03(287.23) | Time/Itr 3.08(3.11) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 2.912e-01(3.250e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 2.28e-04
Itr 000020 | Wall 3.186e+03(151.71) | Time/Itr 2.52(2.99) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 2.827e-01(3.163e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 2.18e-04
Itr 000030 | Wall 3.216e+03(103.75) | Time/Itr 2.94(3.00) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 2.862e-01(3.084e-01) | TT 4.00(4.00) | kinetic_energy: 1.65e+00 | jacobian_norm2: 2.23e-04
Itr 000040 | Wall 3.248e+03(79.22) | Time/Itr 2.65(3.04) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 2.766e-01(3.000e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 2.27e-04
validating...
Epoch 0026 | Time 10.8390, Bit/dim 1.7594, Steps 92.0000, TT 4.00, Transport Cost 9.23e-03
Itr 000000 | Wall 3.252e+03(3251.56) | Time/Itr 3.49(3.06) | BPD 1.83(1.83) | Loss 1.84(1.85) | FE 128(128) | Grad Norm 2.739e-01(2.992e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 2.28e-04
Itr 000010 | Wall 3.283e+03(298.46) | Time/Itr 2.75(3.08) | BPD 1.83(1.83) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.740e-01(2.921e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 2.17e-04
Itr 000020 | Wall 3.314e+03(157.80) | Time/Itr 2.94(3.08) | BPD 1.82(1.83) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.639e-01(2.846e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 2.16e-04
Itr 000030 | Wall 3.344e+03(107.88) | Time/Itr 3.46(3.07) | BPD 1.82(1.83) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.405e-01(2.789e-01) | TT 4.00(4.00) | kinetic_energy: 1.64e+00 | jacobian_norm2: 2.23e-04
Itr 000040 | Wall 3.376e+03(82.33) | Time/Itr 3.39(3.09) | BPD 1.82(1.83) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.528e-01(2.725e-01) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 2.16e-04
validating...
Epoch 0027 | Time 10.8010, Bit/dim 1.7592, Steps 90.6667, TT 4.00, Transport Cost 8.94e-03
Itr 000000 | Wall 3.379e+03(3379.18) | Time/Itr 3.46(3.10) | BPD 1.82(1.83) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.490e-01(2.718e-01) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 2.11e-04
Itr 000010 | Wall 3.411e+03(310.11) | Time/Itr 3.29(3.13) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.453e-01(2.661e-01) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 2.14e-04
Itr 000020 | Wall 3.441e+03(163.87) | Time/Itr 3.43(3.10) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.453e-01(2.604e-01) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 2.16e-04
Itr 000030 | Wall 3.470e+03(111.95) | Time/Itr 3.17(3.05) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.394e-01(2.555e-01) | TT 4.00(4.00) | kinetic_energy: 1.63e+00 | jacobian_norm2: 2.08e-04
Itr 000040 | Wall 3.501e+03(85.39) | Time/Itr 3.52(3.05) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.249e-01(2.492e-01) | TT 4.00(4.00) | kinetic_energy: 1.62e+00 | jacobian_norm2: 2.08e-04
validating...
Epoch 0028 | Time 10.2150, Bit/dim 1.7590, Steps 86.4167, TT 4.00, Transport Cost 8.64e-03
Itr 000000 | Wall 3.504e+03(3504.36) | Time/Itr 3.44(3.06) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.502e-01(2.492e-01) | TT 4.00(4.00) | kinetic_energy: 1.62e+00 | jacobian_norm2: 2.09e-04
Itr 000010 | Wall 3.534e+03(321.27) | Time/Itr 3.42(3.04) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.283e-01(2.444e-01) | TT 4.00(4.00) | kinetic_energy: 1.62e+00 | jacobian_norm2: 2.08e-04
Itr 000020 | Wall 3.564e+03(169.70) | Time/Itr 2.74(3.02) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.201e-01(2.395e-01) | TT 4.00(4.00) | kinetic_energy: 1.62e+00 | jacobian_norm2: 2.07e-04
Itr 000030 | Wall 3.595e+03(115.96) | Time/Itr 2.64(3.04) | BPD 1.82(1.82) | Loss 1.84(1.84) | FE 128(128) | Grad Norm 2.199e-01(2.361e-01) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 2.07e-04
Itr 000040 | Wall 3.625e+03(88.41) | Time/Itr 3.19(3.03) | BPD 1.82(1.82) | Loss 1.83(1.84) | FE 128(128) | Grad Norm 2.235e-01(2.305e-01) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 2.10e-04
validating...
Epoch 0029 | Time 10.8539, Bit/dim 1.7587, Steps 92.0000, TT 4.00, Transport Cost 8.41e-03
Itr 000000 | Wall 3.628e+03(3628.14) | Time/Itr 3.47(3.04) | BPD 1.82(1.82) | Loss 1.83(1.84) | FE 128(128) | Grad Norm 2.199e-01(2.301e-01) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 2.01e-04
Itr 000010 | Wall 3.659e+03(332.62) | Time/Itr 3.39(3.05) | BPD 1.82(1.82) | Loss 1.83(1.84) | FE 128(128) | Grad Norm 2.106e-01(2.263e-01) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 2.02e-04
Itr 000020 | Wall 3.688e+03(175.63) | Time/Itr 1.60(3.02) | BPD 1.82(1.82) | Loss 1.83(1.84) | FE 128(128) | Grad Norm 2.050e-01(2.218e-01) | TT 4.00(4.00) | kinetic_energy: 1.61e+00 | jacobian_norm2: 2.05e-04
Itr 000030 | Wall 3.719e+03(119.97) | Time/Itr 3.23(3.03) | BPD 1.82(1.82) | Loss 1.83(1.84) | FE 128(128) | Grad Norm 2.109e-01(2.186e-01) | TT 4.00(4.00) | kinetic_energy: 1.60e+00 | jacobian_norm2: 2.13e-04
Itr 000040 | Wall 3.750e+03(91.45) | Time/Itr 3.43(3.04) | BPD 1.82(1.82) | Loss 1.83(1.83) | FE 128(128) | Grad Norm 2.081e-01(2.145e-01) | TT 4.00(4.00) | kinetic_energy: 1.60e+00 | jacobian_norm2: 2.10e-04
validating...
Epoch 0030 | Time 10.8648, Bit/dim 1.7585, Steps 92.0000, TT 4.00, Transport Cost 8.20e-03
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 1
downscale_factor = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        if block == 1:
            im_dim = 6
            im_size = 16
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]

        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32()) / 255.0
            tensor[i] += torch.from_numpy(nump_array)

        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32()) / 255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d" % torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d" % torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    # device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ", epoch)
        itr = 0
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)

                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)

                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd

                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_" + str(block) + ".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)

                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)

                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )

                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]

                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_" + str(block) + ".pth"),
                                        os.path.join(args.save, "best_" + str(block) + ".pth"))

            # visualize samples and density
            # if write_log:
            # with torch.no_grad():
            # fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            # utils.makedirs(os.path.dirname(fig_filename))
            # generated_samples, _, _ = model(fixed_z, reverse=True)
            # generated_samples = generated_samples.view(-1, *data_shape)
            # nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
            # save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=512, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=10, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=30, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 372312
Iters per train epoch: 11
Iters per test: 18
/HPS/CNF/work/ffjord-rnode/train.py
import argparse
import os, sys
import warnings
import pandas as pd
import time
import numpy as np
import yaml, csv
import shutil

import torch
import torch.backends.cudnn as cudnn
import torch.distributed as distributed
import torch.nn as nn
import torch.optim as optim

import torchvision.datasets as dset
import torchvision.transforms as tforms
from torchvision.utils import save_image

import lib.layers as layers
import lib.utils as utils
import lib.odenvp as odenvp
from lib.datasets import CelebAHQ, Imagenet64

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import append_regularization_keys_header, append_regularization_csv_dict

import dist_utils
from dist_utils import env_world_size, env_rank
from torch.utils.data.distributed import DistributedSampler
from celeb_dataset import CelebDataset
from torch.nn import MSELoss

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'adaptive_heun', 'bosh3']


def get_parser():
    parser = argparse.ArgumentParser("Continuous Normalizing Flow")
    parser.add_argument("--datadir", default="./data/")
    parser.add_argument("--nworkers", type=int, default=8)
    parser.add_argument("--data", choices=["mnist", "svhn", "cifar10", 'lsun_church', 'celebahq', 'imagenet64'],
                        type=str, default="mnist")
    parser.add_argument("--dims", type=str, default="64,64,64")
    parser.add_argument("--strides", type=str, default="1,1,1,1")
    parser.add_argument("--num_blocks", type=int, default=2, help='Number of stacked CNFs.')

    parser.add_argument(
        "--layer_type", type=str, default="concat",
        choices=["ignore", "concat"]
    )
    parser.add_argument("--divergence_fn", type=str, default="approximate", choices=["brute_force", "approximate"])
    parser.add_argument(
        "--nonlinearity", type=str, default="softplus", choices=["tanh", "relu", "softplus", "elu"]
    )
    parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
    parser.add_argument('--optimizer', type=str, default='adam', choices=['adam', 'sgd'])
    parser.add_argument('--atol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--rtol', type=float, default=1e-5, help='only for adaptive solvers')
    parser.add_argument('--step_size', type=float, default=0.25, help='only for fixed step size solvers')
    parser.add_argument('--first_step', type=float, default=0.166667, help='only for adaptive solvers')

    parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
    parser.add_argument('--test_atol', type=float, default=None)
    parser.add_argument('--test_rtol', type=float, default=None)
    parser.add_argument('--test_step_size', type=float, default=None)
    parser.add_argument('--test_first_step', type=float, default=None)

    parser.add_argument("--imagesize", type=int, default=None)
    parser.add_argument("--alpha", type=float, default=1e-6)
    parser.add_argument('--time_length', type=float, default=1.0)
    parser.add_argument('--train_T', type=eval, default=False)

    parser.add_argument("--num_epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=200)
    parser.add_argument(
        "--batch_size_schedule", type=str, default="",
        help="Increases the batchsize at every given epoch, dash separated."
    )
    parser.add_argument("--test_batch_size", type=int, default=200)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--warmup_iters", type=float, default=1000)
    parser.add_argument("--weight_decay", type=float, default=0.)

    parser.add_argument("--add_noise", type=eval, default=True, choices=[True, False])
    parser.add_argument('--nbits', type=int, default=8)
    parser.add_argument('--div_samples', type=int, default=1)
    parser.add_argument('--squeeze_first', type=eval, default=False, choices=[True, False])
    parser.add_argument('--zero_last', type=eval, default=True, choices=[True, False])
    parser.add_argument('--seed', type=int, default=42)

    # Regularizations
    parser.add_argument('--kinetic-energy', type=float, default=None, help="int_t ||f||_2^2")
    parser.add_argument('--jacobian-norm2', type=float, default=None, help="int_t ||df/dx||_F^2")
    parser.add_argument('--total-deriv', type=float, default=None, help="int_t ||df/dt||^2")
    parser.add_argument('--directional-penalty', type=float, default=None, help="int_t ||(df/dx)^T f||^2")

    parser.add_argument(
        "--max_grad_norm", type=float, default=np.inf,
        help="Max norm of graidents"
    )

    parser.add_argument("--resume", type=str, default=None, help='path to saved check point')
    parser.add_argument("--save", type=str, default="experiments/celebahq")
    parser.add_argument("--val_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=10)
    parser.add_argument('--validate', type=eval, default=False, choices=[True, False])

    parser.add_argument('--distributed', action='store_true', help='Run distributed training. Default True')
    parser.add_argument('--dist-url', default='env://', type=str,
                        help='url used to set up distributed training')
    parser.add_argument('--dist-backend', default='gloo', type=str, help='distributed backend')
    parser.add_argument('--local_rank', default=0, type=int,
                        help='Used for multi-process training. Can either be manually set ' +
                             'or automatically set by using \'python -m multiproc\'.')

    # parser.add_argument('--skip-auto-shutdown', action='store_true',
    #                    help='Shutdown instance at the end of training or failure')
    # parser.add_argument('--auto-shutdown-success-delay-mins', default=10, type=int,
    #                    help='how long to wait until shutting down on success')
    # parser.add_argument('--auto-shutdown-failure-delay-mins', default=60, type=int,
    #                    help='how long to wait before shutting down on error')

    return parser


cudnn.benchmark = True
block = 1
downscale_factor = 2
args = get_parser().parse_args()
torch.manual_seed(args.seed)
nvals = 2 ** args.nbits

# Only want master rank logging
is_master = (not args.distributed) or (dist_utils.env_rank() == 0)
is_rank0 = args.local_rank == 0
write_log = is_rank0 and is_master


def add_noise(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))
    if args.add_noise:
        noise = x.new().resize_as_(x).uniform_()
    else:
        noise = 1 / 2
    return x.add_(noise).div_(2 ** nbits)


def shift(x, nbits=8):
    if nbits < 8:
        x = x // (2 ** (8 - nbits))

    return x.add_(1 / 2).div_(2 ** nbits)


def unshift(x, nbits=8):
    return x.add_(-1 / (2 ** (nbits + 1)))


def update_lr(optimizer, itr):
    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)
    lr = args.lr * iter_frac
    for param_group in optimizer.param_groups:
        param_group["lr"] = lr


# def get_get_size_for_model(stage, in_channels, im_size, downscale_factor=2):
#     in_width = im_size
#     in_height = im_size
#     counter = 0
#     while True:
#         out_channels = in_channels * (downscale_factor ** 2)
#
#         out_height = in_height // downscale_factor
#         out_width = in_width // downscale_factor


def get_dataset(args, device):
    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size)])

    if args.data == "mnist":
        im_dim = 1
        im_size = 28 if args.imagesize is None else args.imagesize
        train_set = dset.MNIST(root=args.datadir, train=True, transform=trans(im_size), download=True)
        test_set = dset.MNIST(root=args.datadir, train=False, transform=trans(im_size), download=True)
    elif args.data == "svhn":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.SVHN(root=args.datadir, split="train", transform=trans(im_size), download=True)
        test_set = dset.SVHN(root=args.datadir, split="test", transform=trans(im_size), download=True)
    elif args.data == "cifar10":
        im_dim = 3
        im_size = 32 if args.imagesize is None else args.imagesize
        train_set = dset.CIFAR10(
            root=args.datadir, train=True, transform=tforms.Compose([
                tforms.Resize(im_size),
                tforms.RandomHorizontalFlip(),
            ]), download=True
        )
        test_set = dset.CIFAR10(root=args.datadir, train=False, transform=None, download=True)
    elif args.data == 'celebahq':

        if block == 2:
            im_dim = 12
            im_size = 8
        if block == 1:
            im_dim = 6
            im_size = 16
        train_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/training_sets/" + str(block))
        test_set = CelebDataset(root="/HPS/CNF/work/ffjord-rnode/data/CelebAMask-HQ/test_sets/" + str(block))
    elif args.data == 'imagenet64':
        im_dim = 3
        if args.imagesize != 64:
            args.imagesize = 64
        im_size = 64
        train_set = Imagenet64(train=True, root=args.datadir)
        test_set = Imagenet64(train=False, root=args.datadir)
    elif args.data == 'lsun_church':
        im_dim = 3
        im_size = 64 if args.imagesize is None else args.imagesize
        train_set = dset.LSUN(
            'data', ['church_outdoor_train'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
        test_set = dset.LSUN(
            'data', ['church_outdoor_val'], transform=tforms.Compose([
                tforms.Resize(96),
                tforms.RandomCrop(64),
                tforms.Resize(im_size),
            ])
        )
    data_shape = (im_dim, im_size, im_size)

    def fast_collate(batch):
        imgs = [img[0] for img in batch]
        targets = [target[1] for target in batch]
        im_dim = imgs[0].shape[0]
        w = imgs[0].shape[1]
        h = imgs[0].shape[2]

        tensor = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)
        for i, img in enumerate(imgs):
            nump_array = np.asarray(img, dtype=np.float32()) / 255.0
            tensor[i] += torch.from_numpy(nump_array)

        im_dim = targets[0].shape[0]
        w = targets[0].shape[1]
        h = targets[0].shape[2]
        target_tensors = torch.zeros((len(imgs), im_dim, w, h), dtype=torch.float32)

        for target in targets:
            nump_array = np.asarray(target, dtype=np.float32()) / 255.0
            target_tensors += torch.from_numpy(nump_array)
        return tensor, target_tensors

    train_sampler = (DistributedSampler(train_set,
                                        num_replicas=env_world_size(), rank=env_rank()) if args.distributed
                     else None)
    train_loader = torch.utils.data.DataLoader(
        dataset=train_set, batch_size=args.batch_size,  # shuffle=True,
        num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)

    test_sampler = (DistributedSampler(test_set,
                                       num_replicas=env_world_size(), rank=env_rank(),
                                       shuffle=False) if args.distributed
                    else None)
    test_loader = torch.utils.data.DataLoader(
        dataset=test_set, batch_size=args.test_batch_size,  # shuffle=False,
        num_workers=8, pin_memory=True, sampler=test_sampler, collate_fn=fast_collate
    )

    return train_loader, test_loader, data_shape


def compute_bits_per_dim(x, model):
    zero = torch.zeros(x.shape[0], 1).to(x)

    z, delta_logp, reg_states, out = model(x, zero)  # run model forward

    reg_states = tuple(torch.mean(rs) for rs in reg_states)

    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)
    logpx = logpz - delta_logp

    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches
    bits_per_dim = -(logpx_per_dim - np.log(nvals)) / np.log(2)

    return bits_per_dim, (x, z), reg_states, out


def create_model(args, data_shape, regularization_fns):
    hidden_dims = tuple(map(int, args.dims.split(",")))
    strides = tuple(map(int, args.strides.split(",")))
    model = odenvp.ODENVP(
        (args.batch_size, *data_shape),
        n_blocks=args.num_blocks,
        intermediate_dims=hidden_dims,
        div_samples=args.div_samples,
        strides=strides,
        squeeze_first=args.squeeze_first,
        nonlinearity=args.nonlinearity,
        layer_type=args.layer_type,
        zero_last=args.zero_last,
        alpha=args.alpha,
        cnf_kwargs={"T": args.time_length, "train_T": args.train_T, "regularization_fns": regularization_fns},
    )
    print("model created ...")
    return model


def main():
    if write_log:
        utils.makedirs(args.save)
        logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

        logger.info(args)

        args_file_path = os.path.join(args.save, 'args.yaml')
        with open(args_file_path, 'w') as f:
            yaml.dump(vars(args), f, default_flow_style=False)

    if args.distributed:
        if write_log: logger.info('Distributed initializing process group')
        torch.cuda.set_device("cuda:%d" % torch.cuda.current_device())
        distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,
                                       world_size=dist_utils.env_world_size(), rank=env_rank())
        assert (dist_utils.env_world_size() == distributed.get_world_size())
        if write_log: logger.info("Distributed: success (%d/%d)" % (args.local_rank, distributed.get_world_size()))

    # get deivce
    device = torch.device("cuda:%d" % torch.cuda.current_device() if torch.cuda.is_available() else "cpu")
    # device = "cpu"
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    # load dataset
    train_loader, test_loader, data_shape = get_dataset(args, device)

    trainlog = os.path.join(args.save, 'training.csv')
    testlog = os.path.join(args.save, 'test.csv')

    traincolumns = ['itr', 'wall', 'itr_time', 'loss', 'bpd', 'fe', 'total_time', 'grad_norm']
    testcolumns = ['wall', 'epoch', 'eval_time', 'bpd', 'fe', 'total_time', 'transport_cost']

    # build model
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = create_model(args, data_shape, regularization_fns).cuda()
    # model = model.cuda()
    args.distributed = False
    if args.distributed: model = dist_utils.DDP(model,
                                                device_ids=[args.local_rank],
                                                output_device=args.local_rank)

    traincolumns = append_regularization_keys_header(traincolumns, regularization_fns)

    if not args.resume and write_log:
        with open(trainlog, 'w') as f:
            csvlogger = csv.DictWriter(f, traincolumns)
            csvlogger.writeheader()
        with open(testlog, 'w') as f:
            csvlogger = csv.DictWriter(f, testcolumns)
            csvlogger.writeheader()

    set_cnf_options(args, model)

    if write_log: logger.info(model)
    if write_log: logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    if write_log: logger.info('Iters per train epoch: {}'.format(len(train_loader)))
    if write_log: logger.info('Iters per test: {}'.format(len(test_loader)))

    # optimizer
    if args.optimizer == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    elif args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, momentum=0.9,
                              nesterov=False)

    # restore parameters
    if args.resume is not None:
        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage.cuda(args.local_rank))
        model.load_state_dict(checkpt["state_dict"])
        if "optim_state_dict" in checkpt.keys():
            optimizer.load_state_dict(checkpt["optim_state_dict"])
            # Manually move optimizer state to device.
            for state in optimizer.state.values():
                for k, v in state.items():
                    if torch.is_tensor(v):
                        state[k] = cvt(v)

    # For visualization.
    if write_log: fixed_z = cvt(torch.randn(min(args.test_batch_size, 100), *data_shape))

    if write_log:
        time_meter = utils.RunningAverageMeter(0.97)
        bpd_meter = utils.RunningAverageMeter(0.97)
        loss_meter = utils.RunningAverageMeter(0.97)
        steps_meter = utils.RunningAverageMeter(0.97)
        grad_meter = utils.RunningAverageMeter(0.97)
        tt_meter = utils.RunningAverageMeter(0.97)

    if not args.resume:
        best_loss = float("inf")
        itr = 0
        wall_clock = 0.
        begin_epoch = 1
    else:
        chkdir = os.path.dirname(args.resume)
        tedf = pd.read_csv(os.path.join(chkdir, 'test.csv'))
        trdf = pd.read_csv(os.path.join(chkdir, 'training.csv'))
        wall_clock = trdf['wall'].to_numpy()[-1]
        itr = trdf['itr'].to_numpy()[-1]
        best_loss = tedf['bpd'].min()
        begin_epoch = int(tedf['epoch'].to_numpy()[-1] + 1)  # not exactly correct

    if args.distributed:
        if write_log: logger.info('Syncing machines before training')
        dist_utils.sum_tensor(torch.tensor([1.0]).float().cuda())

    mse_loss = MSELoss()
    length_of_trainloader = len(train_loader.dataset)
    print("number of batches:", length_of_trainloader // train_loader.batch_size)
    print("best loss", best_loss)
    for epoch in range(begin_epoch, args.num_epochs + 1):
        print("epoch number : ", epoch)
        itr = 0
        if not args.validate:
            model.train()

            with open(trainlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, traincolumns)

                number_of_batches = length_of_trainloader // train_loader.batch_size
                for i in range(number_of_batches):
                    x, label = next(iter(train_loader))
                    x = x.to(device)
                    label = label.to(device)

                    # x, label = x.cuda(), label.cuda()
                    # print(type(x))
                    start = time.time()
                    update_lr(optimizer, itr)
                    optimizer.zero_grad()

                    # cast data and move to device
                    x = add_noise(cvt(x), nbits=args.nbits)
                    # x = x.clamp_(min=0, max=1)
                    # compute loss
                    bpd, (x, z), reg_states, out = compute_bits_per_dim(x, model)
                    if np.isnan(bpd.data.item()):
                        raise ValueError('model returned nan during training')
                    elif np.isinf(bpd.data.item()):
                        raise ValueError('model returned inf during training')

                    loss = bpd

                    loss += mse_loss(out, label)
                    if regularization_coeffs:
                        reg_loss = sum(
                            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if
                            coeff != 0
                        )
                        loss = loss + reg_loss

                    total_time = count_total_time(model)

                    loss.backward()
                    nfe_opt = count_nfe(model)
                    if write_log: steps_meter.update(nfe_opt)
                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

                    optimizer.step()

                    itr_time = time.time() - start
                    wall_clock += itr_time

                    batch_size = x.size(0)
                    metrics = torch.tensor([1., batch_size,
                                            loss.item(),
                                            bpd.item(),
                                            nfe_opt,
                                            grad_norm,
                                            *reg_states]).float().cuda()

                    rv = tuple(torch.tensor(0.).cuda() for r in reg_states)

                    total_gpus, batch_total, r_loss, r_bpd, r_nfe, r_grad_norm, *rv = dist_utils.sum_tensor(
                        metrics).cpu().numpy()

                    if write_log:
                        time_meter.update(itr_time)
                        bpd_meter.update(r_bpd / total_gpus)
                        loss_meter.update(r_loss / total_gpus)
                        grad_meter.update(r_grad_norm / total_gpus)
                        tt_meter.update(total_time)

                        fmt = '{:.4f}'
                        logdict = {'itr': itr,
                                   'wall': fmt.format(wall_clock),
                                   'itr_time': fmt.format(itr_time),
                                   'loss': fmt.format(r_loss / total_gpus),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'total_time': fmt.format(total_time),
                                   'fe': r_nfe / total_gpus,
                                   'grad_norm': fmt.format(r_grad_norm / total_gpus),
                                   }
                        if regularization_coeffs:
                            rv = tuple(v_ / total_gpus for v_ in rv)
                            logdict = append_regularization_csv_dict(logdict,
                                                                     regularization_fns, rv)
                        csvlogger.writerow(logdict)

                        if itr % args.log_freq == 0:
                            log_message = (
                                "Itr {:06d} | Wall {:.3e}({:.2f}) | "
                                "Time/Itr {:.2f}({:.2f}) | BPD {:.2f}({:.2f}) | "
                                "Loss {:.2f}({:.2f}) | "
                                "FE {:.0f}({:.0f}) | Grad Norm {:.3e}({:.3e}) | "
                                "TT {:.2f}({:.2f})".format(
                                    itr, wall_clock, wall_clock / (itr + 1),
                                    time_meter.val, time_meter.avg,
                                    bpd_meter.val, bpd_meter.avg,
                                    loss_meter.val, loss_meter.avg,
                                    steps_meter.val, steps_meter.avg,
                                    grad_meter.val, grad_meter.avg,
                                    tt_meter.val, tt_meter.avg
                                )
                            )
                            if regularization_coeffs:
                                log_message = append_regularization_to_log(log_message,
                                                                           regularization_fns, rv)
                            logger.info(log_message)

                    itr += 1

        # compute test loss
        model.eval()
        if args.local_rank == 0:
            utils.makedirs(args.save)
            torch.save({
                "args": args,
                "state_dict": model.state_dict() if torch.cuda.is_available() else model.state_dict(),
                "optim_state_dict": optimizer.state_dict(),
                "fixed_z": fixed_z.cpu()
            }, os.path.join(args.save, "checkpt_" + str(block) + ".pth"))
        if epoch % args.val_freq == 0 or args.validate:
            with open(testlog, 'a') as f:
                if write_log: csvlogger = csv.DictWriter(f, testcolumns)
                with torch.no_grad():
                    start = time.time()
                    if write_log: logger.info("validating...")

                    lossmean = 0.
                    meandist = 0.
                    steps = 0
                    tt = 0.
                    for i, (x, y) in enumerate(test_loader):
                        sh = x.shape
                        x = shift(cvt(x), nbits=args.nbits)

                        loss, (x, z), _, out = compute_bits_per_dim(x, model)
                        batch_size, in_channels, in_height, in_width = x.size()
                        out_channels = in_channels * (downscale_factor ** 2)

                        out_height = in_height // downscale_factor
                        out_width = in_width // downscale_factor
                        input_view = x.contiguous().view(
                            batch_size, in_channels, out_height, downscale_factor, out_width, downscale_factor
                        )

                        output = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()
                        output = output.view(batch_size, out_channels, out_height, out_width)
                        d = output.size(1) // 2
                        output = output[:, d:]

                        dist = (output.view(output.size(0), -1) - z).pow(2).mean(dim=-1).mean()
                        meandist = i / (i + 1) * dist + meandist / (i + 1)
                        lossmean = i / (i + 1) * lossmean + loss / (i + 1)

                        tt = i / (i + 1) * tt + count_total_time(model) / (i + 1)
                        steps = i / (i + 1) * steps + count_nfe(model) / (i + 1)

                    loss = lossmean.item()
                    metrics = torch.tensor([1., loss, meandist, steps]).float()

                    total_gpus, r_bpd, r_mdist, r_steps = dist_utils.sum_tensor(metrics).cpu().numpy()
                    eval_time = time.time() - start

                    if write_log:
                        fmt = '{:.4f}'
                        logdict = {'epoch': epoch,
                                   'eval_time': fmt.format(eval_time),
                                   'bpd': fmt.format(r_bpd / total_gpus),
                                   'wall': fmt.format(wall_clock),
                                   'total_time': fmt.format(tt),
                                   'transport_cost': fmt.format(r_mdist / total_gpus),
                                   'fe': '{:.2f}'.format(r_steps / total_gpus)}

                        csvlogger.writerow(logdict)

                        logger.info(
                            "Epoch {:04d} | Time {:.4f}, Bit/dim {:.4f}, Steps {:.4f}, TT {:.2f}, Transport Cost {:.2e}".format(
                                epoch, eval_time, r_bpd / total_gpus, r_steps / total_gpus, tt, r_mdist / total_gpus))

                    loss = r_bpd / total_gpus

                    if loss < best_loss and args.local_rank == 0:
                        best_loss = loss
                        shutil.copyfile(os.path.join(args.save, "checkpt_" + str(block) + ".pth"),
                                        os.path.join(args.save, "best_" + str(block) + ".pth"))

            # visualize samples and density
            # if write_log:
            # with torch.no_grad():
            # fig_filename = os.path.join(args.save, "figs", "{:04d}.jpg".format(epoch))
            # utils.makedirs(os.path.dirname(fig_filename))
            # generated_samples, _, _ = model(fixed_z, reverse=True)
            # generated_samples = generated_samples.view(-1, *data_shape)
            # nb = int(np.ceil(np.sqrt(float(fixed_z.size(0)))))
            # save_image(unshift(generated_samples, nbits=args.nbits), fig_filename, nrow=nb)
            if args.validate:
                break


if __name__ == '__main__':
    # try:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=UserWarning)
        main()
        # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')
    # except Exception as e:
    #     exc_type, exc_value, exc_traceback = sys.exc_info()
    #     import traceback
    #
    #     traceback.print_tb(exc_traceback, file=sys.stdout)
    # in case of exception, wait 2 hours before shutting down
    # if not args.skip_auto_shutdown: os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')

Namespace(add_noise=True, alpha=0.05, atol=1e-05, batch_size=128, batch_size_schedule='', data='celebahq', datadir='../data/', dims='64,64,64', directional_penalty=None, dist_backend='gloo', dist_url='env://', distributed=True, div_samples=1, divergence_fn='approximate', first_step=0.166667, imagesize=None, jacobian_norm2=0.01, kinetic_energy=0.01, layer_type='concat', local_rank=0, log_freq=10, lr=0.001, max_grad_norm=inf, nbits=5, nonlinearity='softplus', num_blocks=2, num_epochs=30, nworkers=8, optimizer='adam', resume=None, rtol=1e-05, save='../experiments/celebahq/example/', seed=42, solver='rk4', squeeze_first=False, step_size=0.25, strides='1,1,1,1', test_atol=1e-05, test_batch_size=128, test_first_step=None, test_rtol=1e-05, test_solver='dopri5', test_step_size=None, time_length=1.0, total_deriv=None, train_T=False, val_freq=1, validate=False, warmup_iters=1000, weight_decay=0.0, zero_last=True)
Distributed initializing process group
Added key: store_based_barrier_key:1 to store for rank: 0
Distributed: success (0/4)
ODENVP(
  (transforms): ModuleList(
    (0): StackedCNFLayers(
      (chain): ModuleList(
        (0): LogitTransform()
        (1): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (2): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (3): SqueezeLayer()
        (4): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
        (5): CNF(
          (odefunc): RegularizedODEfunc(
            (odefunc): ODEfunc(
              (diffeq): ODEnet(
                (layers): ModuleList(
                  (0): ConcatConv2d(
                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (1): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (2): ConcatConv2d(
                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                  (3): ConcatConv2d(
                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (activation_fns): ModuleList(
                  (0): Softplus(beta=1, threshold=20)
                  (1): Softplus(beta=1, threshold=20)
                  (2): Softplus(beta=1, threshold=20)
                )
              )
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 372312
Iters per train epoch: 42
Iters per test: 18
Itr 000000 | Wall 3.216e+00(3.22) | Time/Itr 3.22(3.22) | BPD 13.17(13.17) | Loss 13.17(13.17) | FE 128(128) | Grad Norm 9.713e+01(9.713e+01) | TT 4.00(4.00) | kinetic_energy: 9.02e-04 | jacobian_norm2: 0.00e+00
Itr 000010 | Wall 2.878e+01(2.62) | Time/Itr 2.65(3.05) | BPD 12.54(13.10) | Loss 12.54(13.10) | FE 128(128) | Grad Norm 9.605e+01(9.700e+01) | TT 4.00(4.00) | kinetic_energy: 1.67e-03 | jacobian_norm2: 5.44e-09
Itr 000020 | Wall 5.555e+01(2.65) | Time/Itr 2.45(2.95) | BPD 10.71(12.70) | Loss 10.71(12.70) | FE 128(128) | Grad Norm 9.283e+01(9.633e+01) | TT 4.00(4.00) | kinetic_energy: 1.49e-02 | jacobian_norm2: 1.18e-07
Itr 000030 | Wall 8.264e+01(2.67) | Time/Itr 2.76(2.89) | BPD 7.68(11.74) | Loss 7.68(11.74) | FE 128(128) | Grad Norm 8.422e+01(9.431e+01) | TT 4.00(4.00) | kinetic_energy: 8.75e-02 | jacobian_norm2: 1.28e-06
Itr 000040 | Wall 1.079e+02(2.63) | Time/Itr 2.69(2.79) | BPD 3.97(10.12) | Loss 3.97(10.12) | FE 128(128) | Grad Norm 5.811e+01(8.833e+01) | TT 4.00(4.00) | kinetic_energy: 3.68e-01 | jacobian_norm2: 1.20e-05
Itr 000050 | Wall 1.339e+02(2.63) | Time/Itr 2.81(2.74) | BPD 2.31(8.16) | Loss 2.32(8.16) | FE 128(128) | Grad Norm 9.326e+00(7.200e+01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 7.78e-05
Itr 000060 | Wall 1.605e+02(2.63) | Time/Itr 2.88(2.72) | BPD 2.46(6.69) | Loss 2.47(6.69) | FE 128(128) | Grad Norm 2.354e+01(6.048e+01) | TT 4.00(4.00) | kinetic_energy: 1.30e+00 | jacobian_norm2: 8.68e-05
Itr 000070 | Wall 1.872e+02(2.64) | Time/Itr 2.69(2.71) | BPD 2.24(5.52) | Loss 2.25(5.53) | FE 128(128) | Grad Norm 9.441e+00(4.680e+01) | TT 4.00(4.00) | kinetic_energy: 8.67e-01 | jacobian_norm2: 3.54e-05
Itr 000080 | Wall 2.140e+02(2.64) | Time/Itr 2.79(2.70) | BPD 2.21(4.66) | Loss 2.22(4.67) | FE 128(128) | Grad Norm 8.401e+00(3.741e+01) | TT 4.00(4.00) | kinetic_energy: 8.81e-01 | jacobian_norm2: 3.17e-05
Itr 000090 | Wall 2.409e+02(2.65) | Time/Itr 2.83(2.70) | BPD 2.17(4.01) | Loss 2.18(4.02) | FE 128(128) | Grad Norm 3.944e+00(2.856e+01) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 3.87e-05
Itr 000100 | Wall 2.683e+02(2.66) | Time/Itr 2.72(2.71) | BPD 2.14(3.52) | Loss 2.15(3.53) | FE 128(128) | Grad Norm 2.222e+00(2.185e+01) | TT 4.00(4.00) | kinetic_energy: 9.97e-01 | jacobian_norm2: 3.15e-05
Itr 000110 | Wall 2.958e+02(2.67) | Time/Itr 2.92(2.72) | BPD 2.12(3.16) | Loss 2.13(3.17) | FE 128(128) | Grad Norm 3.388e+00(1.704e+01) | TT 4.00(4.00) | kinetic_energy: 9.74e-01 | jacobian_norm2: 2.58e-05
Itr 000120 | Wall 3.228e+02(2.67) | Time/Itr 2.53(2.72) | BPD 2.10(2.88) | Loss 2.11(2.89) | FE 128(128) | Grad Norm 1.809e+00(1.313e+01) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 2.41e-05
Itr 000130 | Wall 3.491e+02(2.66) | Time/Itr 2.67(2.69) | BPD 2.07(2.67) | Loss 2.08(2.68) | FE 128(128) | Grad Norm 2.134e+00(1.018e+01) | TT 4.00(4.00) | kinetic_energy: 1.01e+00 | jacobian_norm2: 1.96e-05
Itr 000140 | Wall 3.758e+02(2.67) | Time/Itr 2.67(2.69) | BPD 2.04(2.51) | Loss 2.06(2.52) | FE 128(128) | Grad Norm 1.839e+00(8.057e+00) | TT 4.00(4.00) | kinetic_energy: 1.02e+00 | jacobian_norm2: 1.77e-05
Itr 000150 | Wall 4.021e+02(2.66) | Time/Itr 2.74(2.67) | BPD 2.02(2.38) | Loss 2.03(2.39) | FE 128(128) | Grad Norm 1.675e+00(6.385e+00) | TT 4.00(4.00) | kinetic_energy: 1.03e+00 | jacobian_norm2: 1.52e-05
Itr 000160 | Wall 4.293e+02(2.67) | Time/Itr 2.68(2.68) | BPD 1.99(2.28) | Loss 2.00(2.29) | FE 128(128) | Grad Norm 1.663e+00(5.159e+00) | TT 4.00(4.00) | kinetic_energy: 1.04e+00 | jacobian_norm2: 1.38e-05
validating...
Epoch 0001 | Time 19.0625, Bit/dim 1.8033, Steps 80.0000, TT 4.00, Transport Cost 1.34e-01
Itr 000000 | Wall 4.395e+02(439.53) | Time/Itr 2.86(2.67) | BPD 1.98(2.25) | Loss 1.99(2.26) | FE 128(128) | Grad Norm 1.542e+00(4.748e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.37e-05
Itr 000010 | Wall 4.667e+02(42.43) | Time/Itr 2.73(2.68) | BPD 1.98(2.18) | Loss 1.99(2.19) | FE 128(128) | Grad Norm 1.538e+00(3.906e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.36e-05
Itr 000020 | Wall 4.941e+02(23.53) | Time/Itr 2.64(2.70) | BPD 1.98(2.13) | Loss 1.99(2.14) | FE 128(128) | Grad Norm 1.546e+00(3.287e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.34e-05
Itr 000030 | Wall 5.205e+02(16.79) | Time/Itr 2.53(2.68) | BPD 1.98(2.09) | Loss 1.99(2.10) | FE 128(128) | Grad Norm 1.561e+00(2.832e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.34e-05
Itr 000040 | Wall 5.473e+02(13.35) | Time/Itr 2.76(2.68) | BPD 1.97(2.06) | Loss 1.98(2.07) | FE 128(128) | Grad Norm 1.533e+00(2.494e+00) | TT 4.00(4.00) | kinetic_energy: 1.05e+00 | jacobian_norm2: 1.33e-05
Itr 000050 | Wall 5.741e+02(11.26) | Time/Itr 2.87(2.68) | BPD 1.96(2.03) | Loss 1.97(2.04) | FE 128(128) | Grad Norm 1.487e+00(2.235e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.36e-05
Itr 000060 | Wall 6.005e+02(9.84) | Time/Itr 2.80(2.67) | BPD 1.95(2.01) | Loss 1.96(2.02) | FE 128(128) | Grad Norm 1.445e+00(2.032e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.41e-05
Itr 000070 | Wall 6.277e+02(8.84) | Time/Itr 2.78(2.69) | BPD 1.94(2.00) | Loss 1.95(2.01) | FE 128(128) | Grad Norm 1.409e+00(1.872e+00) | TT 4.00(4.00) | kinetic_energy: 1.06e+00 | jacobian_norm2: 1.49e-05
Itr 000080 | Wall 6.535e+02(8.07) | Time/Itr 2.27(2.66) | BPD 1.93(1.98) | Loss 1.94(1.99) | FE 128(128) | Grad Norm 1.342e+00(1.742e+00) | TT 4.00(4.00) | kinetic_energy: 1.07e+00 | jacobian_norm2: 1.67e-05
Itr 000090 | Wall 6.801e+02(7.47) | Time/Itr 2.79(2.66) | BPD 1.92(1.97) | Loss 1.93(1.98) | FE 128(128) | Grad Norm 1.285e+00(1.629e+00) | TT 4.00(4.00) | kinetic_energy: 1.08e+00 | jacobian_norm2: 1.88e-05
Itr 000100 | Wall 7.066e+02(7.00) | Time/Itr 2.81(2.66) | BPD 1.90(1.95) | Loss 1.91(1.96) | FE 128(128) | Grad Norm 1.231e+00(1.530e+00) | TT 4.00(4.00) | kinetic_energy: 1.09e+00 | jacobian_norm2: 2.32e-05
Itr 000110 | Wall 7.337e+02(6.61) | Time/Itr 2.82(2.67) | BPD 1.89(1.94) | Loss 1.90(1.95) | FE 128(128) | Grad Norm 1.139e+00(1.438e+00) | TT 4.00(4.00) | kinetic_energy: 1.10e+00 | jacobian_norm2: 3.28e-05
Itr 000120 | Wall 7.591e+02(6.27) | Time/Itr 2.65(2.64) | BPD 1.87(1.92) | Loss 1.88(1.93) | FE 128(128) | Grad Norm 1.064e+00(1.350e+00) | TT 4.00(4.00) | kinetic_energy: 1.11e+00 | jacobian_norm2: 4.26e-05
Itr 000130 | Wall 7.859e+02(6.00) | Time/Itr 2.72(2.65) | BPD 1.85(1.90) | Loss 1.86(1.91) | FE 128(128) | Grad Norm 9.658e-01(1.262e+00) | TT 4.00(4.00) | kinetic_energy: 1.12e+00 | jacobian_norm2: 6.50e-05
Itr 000140 | Wall 8.135e+02(5.77) | Time/Itr 2.78(2.68) | BPD 1.83(1.89) | Loss 1.84(1.90) | FE 128(128) | Grad Norm 8.598e-01(1.170e+00) | TT 4.00(4.00) | kinetic_energy: 1.13e+00 | jacobian_norm2: 9.43e-05
Itr 000150 | Wall 8.396e+02(5.56) | Time/Itr 2.77(2.66) | BPD 1.82(1.87) | Loss 1.83(1.88) | FE 128(128) | Grad Norm 7.379e-01(1.073e+00) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 1.49e-04
Itr 000160 | Wall 8.664e+02(5.38) | Time/Itr 2.78(2.66) | BPD 1.80(1.85) | Loss 1.81(1.86) | FE 128(128) | Grad Norm 6.018e-01(9.650e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.01e-04
validating...
Epoch 0002 | Time 19.9080, Bit/dim 1.7588, Steps 86.0000, TT 4.00, Transport Cost 1.29e-02
Itr 000000 | Wall 8.774e+02(877.45) | Time/Itr 2.82(2.68) | BPD 1.80(1.85) | Loss 1.81(1.86) | FE 128(128) | Grad Norm 5.380e-01(9.189e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.58e-04
Itr 000010 | Wall 9.048e+02(82.26) | Time/Itr 2.83(2.69) | BPD 1.80(1.83) | Loss 1.81(1.85) | FE 128(128) | Grad Norm 5.296e-01(8.188e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.55e-04
Itr 000020 | Wall 9.315e+02(44.36) | Time/Itr 2.48(2.69) | BPD 1.80(1.82) | Loss 1.81(1.84) | FE 128(128) | Grad Norm 5.065e-01(7.405e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.51e-04
Itr 000030 | Wall 9.593e+02(30.95) | Time/Itr 2.64(2.71) | BPD 1.79(1.82) | Loss 1.81(1.83) | FE 128(128) | Grad Norm 5.130e-01(6.802e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.60e-04
Itr 000040 | Wall 9.856e+02(24.04) | Time/Itr 2.68(2.69) | BPD 1.79(1.81) | Loss 1.80(1.82) | FE 128(128) | Grad Norm 4.653e-01(6.268e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.79e-04
Itr 000050 | Wall 1.013e+03(19.86) | Time/Itr 2.77(2.70) | BPD 1.79(1.81) | Loss 1.80(1.82) | FE 128(128) | Grad Norm 4.323e-01(5.812e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.95e-04
Itr 000060 | Wall 1.040e+03(17.05) | Time/Itr 2.79(2.70) | BPD 1.79(1.80) | Loss 1.80(1.81) | FE 128(128) | Grad Norm 3.861e-01(5.375e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 2.87e-04
Itr 000070 | Wall 1.066e+03(15.01) | Time/Itr 2.77(2.67) | BPD 1.79(1.80) | Loss 1.80(1.81) | FE 128(128) | Grad Norm 3.763e-01(4.944e-01) | TT 4.00(4.00) | kinetic_energy: 1.16e+00 | jacobian_norm2: 3.10e-04
Itr 000080 | Wall 1.094e+03(13.50) | Time/Itr 2.74(2.70) | BPD 1.79(1.80) | Loss 1.80(1.81) | FE 128(128) | Grad Norm 3.087e-01(4.513e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 3.33e-04
Itr 000090 | Wall 1.121e+03(12.32) | Time/Itr 2.73(2.71) | BPD 1.79(1.79) | Loss 1.80(1.80) | FE 128(128) | Grad Norm 2.638e-01(4.087e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 3.53e-04
Itr 000100 | Wall 1.147e+03(11.36) | Time/Itr 2.65(2.69) | BPD 1.78(1.79) | Loss 1.80(1.80) | FE 128(128) | Grad Norm 2.477e-01(3.698e-01) | TT 4.00(4.00) | kinetic_energy: 1.15e+00 | jacobian_norm2: 3.52e-04
Itr 000110 | Wall 1.175e+03(10.58) | Time/Itr 3.08(2.71) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.377e-01(3.336e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.27e-04
Itr 000120 | Wall 1.201e+03(9.92) | Time/Itr 2.63(2.68) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.496e-01(3.022e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.43e-04
Itr 000130 | Wall 1.228e+03(9.37) | Time/Itr 2.72(2.69) | BPD 1.78(1.79) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 2.108e-01(2.723e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.44e-04
Itr 000140 | Wall 1.255e+03(8.90) | Time/Itr 2.70(2.69) | BPD 1.78(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.620e-01(2.489e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.52e-04
Itr 000150 | Wall 1.281e+03(8.49) | Time/Itr 2.80(2.68) | BPD 1.78(1.78) | Loss 1.79(1.80) | FE 128(128) | Grad Norm 1.443e-01(2.266e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.71e-04
Itr 000160 | Wall 1.308e+03(8.12) | Time/Itr 2.68(2.68) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.528e-01(2.085e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.72e-04
validating...
Epoch 0003 | Time 20.0060, Bit/dim 1.7534, Steps 86.0000, TT 4.00, Transport Cost 4.86e-03
Itr 000000 | Wall 1.319e+03(1319.30) | Time/Itr 2.80(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.433e-01(2.011e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.80e-04
Itr 000010 | Wall 1.346e+03(122.32) | Time/Itr 2.65(2.68) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.350e-01(1.845e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.56e-04
Itr 000020 | Wall 1.373e+03(65.37) | Time/Itr 2.71(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.440e-01(1.734e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.71e-04
Itr 000030 | Wall 1.400e+03(45.16) | Time/Itr 2.66(2.70) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.469e-01(1.642e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.54e-04
Itr 000040 | Wall 1.427e+03(34.80) | Time/Itr 2.80(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.387e-01(1.577e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.88e-04
Itr 000050 | Wall 1.454e+03(28.51) | Time/Itr 2.80(2.70) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.466e-01(1.514e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.71e-04
Itr 000060 | Wall 1.481e+03(24.28) | Time/Itr 2.68(2.71) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.306e-01(1.467e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.96e-04
Itr 000070 | Wall 1.508e+03(21.24) | Time/Itr 2.59(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.265e-01(1.443e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 3.90e-04
Itr 000080 | Wall 1.534e+03(18.94) | Time/Itr 2.90(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.166e-01(1.402e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.17e-04
Itr 000090 | Wall 1.561e+03(17.16) | Time/Itr 2.72(2.68) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.186e-01(1.360e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.16e-04
Itr 000100 | Wall 1.588e+03(15.73) | Time/Itr 2.82(2.70) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.252e-01(1.322e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.40e-04
Itr 000110 | Wall 1.615e+03(14.55) | Time/Itr 2.81(2.69) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.082e-01(1.292e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.48e-04
Itr 000120 | Wall 1.642e+03(13.57) | Time/Itr 2.76(2.70) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.249e-01(1.282e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.66e-04
Itr 000130 | Wall 1.670e+03(12.75) | Time/Itr 2.64(2.71) | BPD 1.78(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.258e-01(1.281e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 4.99e-04
Itr 000140 | Wall 1.697e+03(12.03) | Time/Itr 2.83(2.71) | BPD 1.77(1.78) | Loss 1.79(1.79) | FE 128(128) | Grad Norm 1.141e-01(1.269e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 5.44e-04
Itr 000150 | Wall 1.723e+03(11.41) | Time/Itr 2.79(2.69) | BPD 1.77(1.78) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.236e-01(1.282e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 5.61e-04
Itr 000160 | Wall 1.751e+03(10.88) | Time/Itr 3.08(2.72) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.630e-01(1.287e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 6.38e-04
validating...
Epoch 0004 | Time 21.7010, Bit/dim 1.7493, Steps 92.0000, TT 4.00, Transport Cost 3.65e-03
Itr 000000 | Wall 1.762e+03(1761.76) | Time/Itr 2.86(2.71) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.019e-01(1.288e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 6.79e-04
Itr 000010 | Wall 1.789e+03(162.63) | Time/Itr 2.76(2.72) | BPD 1.77(1.77) | Loss 1.78(1.79) | FE 128(128) | Grad Norm 1.015e-01(1.221e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 6.66e-04
Itr 000020 | Wall 1.815e+03(86.44) | Time/Itr 2.72(2.70) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.026e-01(1.171e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 6.66e-04
Itr 000030 | Wall 1.842e+03(59.43) | Time/Itr 2.80(2.70) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.094e-01(1.134e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 7.04e-04
Itr 000040 | Wall 1.870e+03(45.60) | Time/Itr 2.65(2.71) | BPD 1.77(1.77) | Loss 1.78(1.78) | FE 128(128) | Grad Norm 1.042e-01(1.113e-01) | TT 4.00(4.00) | kinetic_energy: 1.14e+00 | jacobian_norm2: 6.93e-04
